{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "from encoder_paths import *\n",
    "import json\n",
    "from pprint import pprint\n",
    "TRAIN_FILE_PATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/training.json\"\n",
    "VALIDATION_FILE_PATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/testing.json\"\n",
    "with open(TRAIN_FILE_PATH) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(VALIDATION_FILE_PATH) as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "pprint(len(train_data))\n",
    "pprint(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3190b86190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "import torch\n",
    "\n",
    "numpy.random.seed(69)\n",
    "random.seed(69)\n",
    "torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fear', 'anger', 'neutral', 'disgust', 'joy', 'surprise', 'sadness'}\n"
     ]
    }
   ],
   "source": [
    "all_emotions = set()\n",
    "for conversation in train_data:\n",
    "    conversation = conversation[\"conversation\"]\n",
    "    for utterance in conversation:\n",
    "        all_emotions.add(utterance[\"emotion\"])\n",
    "pprint(all_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionIndexer:\n",
    "    def __init__(self):\n",
    "        self.emotion_to_index = {\n",
    "            'joy': 0,\n",
    "            'sadness': 1,\n",
    "            'anger': 2,\n",
    "            'neutral': 3,\n",
    "            'surprise': 4,\n",
    "            'disgust': 5,\n",
    "            'fear': 6,\n",
    "        }\n",
    "\n",
    "        self.index_to_emotion = {index: emotion for emotion, index in self.emotion_to_index.items()}\n",
    "\n",
    "    def emotion_to_idx(self, emotion):\n",
    "        return self.emotion_to_index.get(emotion, None)\n",
    "\n",
    "    def idx_to_emotion(self, index):\n",
    "        return self.index_to_emotion.get(index, None)\n",
    "\n",
    "# Example usage\n",
    "indexer = EmotionIndexer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        with open(audio_embeddings_path, \"rb\") as f:\n",
    "            self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        audio_embedding = audio_embedding.squeeze()\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    \n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        with open(video_embeddings_path, \"rb\") as f:\n",
    "            self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        # video_name = video_name.split(\".\")[0]\n",
    "        video_embedding = self.video_embeddings[video_name].reshape((16,-1))\n",
    "        video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, json_file, audio_encoder, video_encoder, text_encoder, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = self.load_data(json_file)\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def load_data(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.data[idx]['conversation']\n",
    "        # emotion_labels = [utterance['emotion'] for utterance in conversation]\n",
    "        audio_paths = [utterance['video_name'].replace('mp4', 'wav') for utterance in conversation]\n",
    "        video_paths = [utterance['video_name'] for utterance in conversation]\n",
    "        texts = [utterance['video_name'] for utterance in conversation]\n",
    "\n",
    "        audio_embeddings = [self.audio_encoder.lmao(audio_path) for audio_path in audio_paths]\n",
    "        video_embeddings = [self.video_encoder.lmao(video_path) for video_path in video_paths]\n",
    "        text_embeddings = [self.text_encoder.lmao(text) for text in texts]\n",
    "\n",
    "        cause_pairs = self.data[idx]['emotion-cause_pairs']\n",
    "        useful_utterances = set([int(cause_pair[1]) for cause_pair in cause_pairs])\n",
    "        cause_labels = []\n",
    "        for utterance in conversation:\n",
    "            if utterance['utterance_ID'] in useful_utterances:\n",
    "                cause_labels.append(1)\n",
    "            else:\n",
    "                cause_labels.append(0)\n",
    "        \n",
    "        # Pad or truncate conversations to the maximum sequence length\n",
    "        if len(conversation) < self.max_seq_len:\n",
    "            pad_length = self.max_seq_len - len(conversation)\n",
    "            audio_embeddings += [torch.zeros_like(audio_embeddings[0])] * pad_length\n",
    "            video_embeddings += [torch.zeros_like(video_embeddings[0])] * pad_length\n",
    "            text_embeddings += [torch.zeros_like(text_embeddings[0])] * pad_length\n",
    "            cause_labels += [-1] * pad_length\n",
    "            pad_mask = [1] * len(conversation) + [0] * pad_length\n",
    "        else:\n",
    "            audio_embeddings = audio_embeddings[:self.max_seq_len]\n",
    "            video_embeddings = video_embeddings[:self.max_seq_len]\n",
    "            text_embeddings = text_embeddings[:self.max_seq_len]\n",
    "            cause_labels = cause_labels[:self.max_seq_len]\n",
    "            pad_mask = [1] * self.max_seq_len\n",
    "\n",
    "        audio_embeddings = torch.stack(audio_embeddings)\n",
    "        video_embeddings = torch.stack(video_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        cause_labels = torch.from_numpy(np.array(cause_labels))\n",
    "        pad_mask = torch.from_numpy(np.array(pad_mask))\n",
    "        \n",
    "        return {\n",
    "            'audio': audio_embeddings,\n",
    "            'video': video_embeddings,\n",
    "            'text': text_embeddings,\n",
    "            'cause_labels': cause_labels,\n",
    "            'pad_mask': pad_mask\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "# You need to define your audio, video, and text encoders accordingly\n",
    "\n",
    "# Define your data paths\n",
    "# DATA_DIR = \"/tmp/semeval24_task3\"\n",
    "\n",
    "# AUDIO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/audio_embedding_6373.npy\"\n",
    "# VIDEO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\"\n",
    "# TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "max_seq_len = 35  # Adjust this according to your needs\n",
    "\n",
    "train_dataset = ConversationDataset(TRAIN_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "validation_dataset = ConversationDataset(VALIDATION_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=True)\n",
    "# Example of iterating through batches\n",
    "# for batch in dataloader:\n",
    "#     audio = batch['audio']  # Shape: (batch_size, max_seq_len, audio_embedding_size)\n",
    "#     video = batch['video']  # Shape: (batch_size, max_seq_len, video_embedding_size)\n",
    "#     text = batch['text']    # Shape: (batch_size, max_seq_len, text_embedding_size)\n",
    "#     cause_labels = batch['cause_labels']  # List of emotion labels for each utterance in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout, num_emotions):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "\n",
    "#         self.transformer_encoder = TransformerEncoder(\n",
    "#             TransformerEncoderLayer(hidden_size, num_heads, hidden_size, dropout),\n",
    "#             num_layers\n",
    "#         )\n",
    "        \n",
    "#         self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "#     def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "\n",
    "#         # Concatenate or combine the audio, video, and text encodings here\n",
    "#         # You can use any method like concatenation, addition, or other fusion techniques\n",
    "#         # Combine the encodings (you can customize this part)\n",
    "#         audio_encoding = audio_encoding.float()\n",
    "#         video_encoding = video_encoding.float()\n",
    "#         text_encoding = text_encoding.float().squeeze()\n",
    "#         combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "#         combined_encoding = self.first_linear(combined_encoding)\n",
    "        \n",
    "        \n",
    "#         combined_encoding = combined_encoding.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, input_size)\n",
    "        \n",
    "        \n",
    "#         transformer_output = self.transformer_encoder(combined_encoding)\n",
    "\n",
    "#         # Take the output of the Transformer encoder for the last position as the summary\n",
    "#         emotion_logits = self.linear(transformer_output.permute(1, 0, 2))\n",
    "\n",
    "#         return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        # self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "        # Replace Transformer with BiLSTM\n",
    "        self.bilstm = nn.LSTM(input_size, input_size // 2, num_layers, \n",
    "                              dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.final_linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "        # Concatenate or combine the audio, video, and text encodings\n",
    "        audio_encoding = audio_encoding.float()\n",
    "        video_encoding = video_encoding.float()\n",
    "        text_encoding = text_encoding.float().squeeze()\n",
    "        \n",
    "        audio_encoding = self.audio_dropout(audio_encoding)\n",
    "        video_encoding = self.video_dropout(video_encoding)\n",
    "        text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "        combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "        # Take the output of the BiLSTM\n",
    "        emotion_logits = self.linear(lstm_output)\n",
    "        emotion_logits = self.relu(emotion_logits)\n",
    "        emotion_logits = self.final_linear(emotion_logits)\n",
    "        # Apply a softmax layer\n",
    "        emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "        return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "#         self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "#         self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "#         self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#         self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "#         # Replace Transformer with BiLSTM\n",
    "#         self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "#                               dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "#         self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "#     def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "#         # Concatenate or combine the audio, video, and text encodings\n",
    "#         audio_encoding = audio_encoding.float()\n",
    "#         video_encoding = video_encoding.float()\n",
    "#         text_encoding = text_encoding.float().squeeze()\n",
    "        \n",
    "#         audio_encoding = self.audio_dropout(audio_encoding)\n",
    "#         video_encoding = self.video_dropout(video_encoding)\n",
    "#         text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "#         combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "#         combined_encoding = self.first_linear(combined_encoding)\n",
    "#         combined_encoding = self.relu(combined_encoding)\n",
    "#         combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        \n",
    "#         # Pass through BiLSTM\n",
    "#         lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "#         # Take the output of the BiLSTM\n",
    "#         emotion_logits = self.linear(lstm_output)\n",
    "#         # Apply a softmax layer\n",
    "#         emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "#         return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/semeval24_task3/audio_embeddings/audio_embeddings_microsoft_wavlm-base-plus-sd.pkl\n",
      "/tmp/semeval24_task3/text_embeddings/text_embeddings_roberta_base_emotion.pkl\n",
      "/tmp/semeval24_task3/video_embeddings/final_embeddings.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.95it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.91it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/40] Training Loss: 0.004426745894076799\n",
      "Epoch [0/40] Validation Loss: 0.004305580216977332\n",
      "Epoch [0/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       759\n",
      "           1       0.00      0.00      0.00       681\n",
      "\n",
      "    accuracy                           0.53      1440\n",
      "   macro avg       0.26      0.50      0.35      1440\n",
      "weighted avg       0.28      0.53      0.36      1440\n",
      "\n",
      "Epoch [0/40] Accuracy: 0.5211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.01it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] Training Loss: 0.004396225355051061\n",
      "Epoch [1/40] Validation Loss: 0.0042608482970131764\n",
      "Epoch [1/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.79      0.67       759\n",
      "           1       0.61      0.36      0.45       681\n",
      "\n",
      "    accuracy                           0.59      1440\n",
      "   macro avg       0.59      0.58      0.56      1440\n",
      "weighted avg       0.59      0.59      0.57      1440\n",
      "\n",
      "Epoch [1/40] Accuracy: 0.5473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.58it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40] Training Loss: 0.00431829717850507\n",
      "Epoch [2/40] Validation Loss: 0.004196642711758613\n",
      "Epoch [2/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.72      0.65       759\n",
      "           1       0.59      0.45      0.51       681\n",
      "\n",
      "    accuracy                           0.59      1440\n",
      "   macro avg       0.59      0.59      0.58      1440\n",
      "weighted avg       0.59      0.59      0.59      1440\n",
      "\n",
      "Epoch [2/40] Accuracy: 0.5918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/40] Training Loss: 0.004265415207467963\n",
      "Epoch [3/40] Validation Loss: 0.004198545010553466\n",
      "Epoch [3/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.49      0.55       759\n",
      "           1       0.54      0.68      0.60       681\n",
      "\n",
      "    accuracy                           0.58      1440\n",
      "   macro avg       0.59      0.58      0.58      1440\n",
      "weighted avg       0.59      0.58      0.58      1440\n",
      "\n",
      "Epoch [3/40] Accuracy: 0.5989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.19it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 28.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/40] Training Loss: 0.004227837168217998\n",
      "Epoch [4/40] Validation Loss: 0.00416704879866706\n",
      "Epoch [4/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.57      0.59       759\n",
      "           1       0.56      0.61      0.59       681\n",
      "\n",
      "    accuracy                           0.59      1440\n",
      "   macro avg       0.59      0.59      0.59      1440\n",
      "weighted avg       0.59      0.59      0.59      1440\n",
      "\n",
      "Epoch [4/40] Accuracy: 0.6096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.77it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/40] Training Loss: 0.004200210466752137\n",
      "Epoch [5/40] Validation Loss: 0.004144492497046789\n",
      "Epoch [5/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.68      0.64       759\n",
      "           1       0.59      0.51      0.55       681\n",
      "\n",
      "    accuracy                           0.60      1440\n",
      "   macro avg       0.60      0.59      0.59      1440\n",
      "weighted avg       0.60      0.60      0.60      1440\n",
      "\n",
      "Epoch [5/40] Accuracy: 0.6169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.75it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/40] Training Loss: 0.004179369963804033\n",
      "Epoch [6/40] Validation Loss: 0.004121405300166872\n",
      "Epoch [6/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.70      0.65       759\n",
      "           1       0.60      0.49      0.54       681\n",
      "\n",
      "    accuracy                           0.60      1440\n",
      "   macro avg       0.60      0.60      0.60      1440\n",
      "weighted avg       0.60      0.60      0.60      1440\n",
      "\n",
      "Epoch [6/40] Accuracy: 0.6235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.82it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/40] Training Loss: 0.004154616133784394\n",
      "Epoch [7/40] Validation Loss: 0.004085895129375987\n",
      "Epoch [7/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.62      0.63       759\n",
      "           1       0.59      0.62      0.61       681\n",
      "\n",
      "    accuracy                           0.62      1440\n",
      "   macro avg       0.62      0.62      0.62      1440\n",
      "weighted avg       0.62      0.62      0.62      1440\n",
      "\n",
      "Epoch [7/40] Accuracy: 0.6285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.61it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/40] Training Loss: 0.004115956572928798\n",
      "Epoch [8/40] Validation Loss: 0.00408528277443515\n",
      "Epoch [8/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.70      0.66       759\n",
      "           1       0.61      0.51      0.55       681\n",
      "\n",
      "    accuracy                           0.61      1440\n",
      "   macro avg       0.61      0.61      0.61      1440\n",
      "weighted avg       0.61      0.61      0.61      1440\n",
      "\n",
      "Epoch [8/40] Accuracy: 0.6362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.77it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/40] Training Loss: 0.004088742934245078\n",
      "Epoch [9/40] Validation Loss: 0.004057255842619472\n",
      "Epoch [9/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.62      0.64       759\n",
      "           1       0.60      0.64      0.62       681\n",
      "\n",
      "    accuracy                           0.63      1440\n",
      "   macro avg       0.63      0.63      0.63      1440\n",
      "weighted avg       0.63      0.63      0.63      1440\n",
      "\n",
      "Epoch [9/40] Accuracy: 0.6460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.81it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/40] Training Loss: 0.0040467780685080265\n",
      "Epoch [10/40] Validation Loss: 0.00403335653245449\n",
      "Epoch [10/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.58      0.62       759\n",
      "           1       0.59      0.68      0.63       681\n",
      "\n",
      "    accuracy                           0.62      1440\n",
      "   macro avg       0.63      0.63      0.62      1440\n",
      "weighted avg       0.63      0.62      0.62      1440\n",
      "\n",
      "Epoch [10/40] Accuracy: 0.6492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.04it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/40] Training Loss: 0.004007200975901231\n",
      "Epoch [11/40] Validation Loss: 0.004000326784120665\n",
      "Epoch [11/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.65      0.65       759\n",
      "           1       0.62      0.63      0.63       681\n",
      "\n",
      "    accuracy                           0.64      1440\n",
      "   macro avg       0.64      0.64      0.64      1440\n",
      "weighted avg       0.64      0.64      0.64      1440\n",
      "\n",
      "Epoch [11/40] Accuracy: 0.6545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.03it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 28.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/40] Training Loss: 0.003982924952061856\n",
      "Epoch [12/40] Validation Loss: 0.003956558886501524\n",
      "Epoch [12/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.67       759\n",
      "           1       0.63      0.60      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.64      0.64      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [12/40] Accuracy: 0.6620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.94it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/40] Training Loss: 0.003932579073258755\n",
      "Epoch [13/40] Validation Loss: 0.003946219839983516\n",
      "Epoch [13/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.67       759\n",
      "           1       0.63      0.65      0.64       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [13/40] Accuracy: 0.6729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.60it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/40] Training Loss: 0.003916620273427214\n",
      "Epoch [14/40] Validation Loss: 0.003932883217930794\n",
      "Epoch [14/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.61      0.65       759\n",
      "           1       0.62      0.70      0.66       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [14/40] Accuracy: 0.6726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.96it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/40] Training Loss: 0.0038630178687470485\n",
      "Epoch [15/40] Validation Loss: 0.003978284572561582\n",
      "Epoch [15/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.55      0.62       759\n",
      "           1       0.60      0.75      0.67       681\n",
      "\n",
      "    accuracy                           0.64      1440\n",
      "   macro avg       0.65      0.65      0.64      1440\n",
      "weighted avg       0.66      0.64      0.64      1440\n",
      "\n",
      "Epoch [15/40] Accuracy: 0.6877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.79it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/40] Training Loss: 0.0038510722982461588\n",
      "Epoch [16/40] Validation Loss: 0.003989286803536945\n",
      "Epoch [16/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.50      0.59       759\n",
      "           1       0.59      0.80      0.68       681\n",
      "\n",
      "    accuracy                           0.64      1440\n",
      "   macro avg       0.66      0.65      0.63      1440\n",
      "weighted avg       0.66      0.64      0.63      1440\n",
      "\n",
      "Epoch [16/40] Accuracy: 0.6902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.87it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/40] Training Loss: 0.003813076781780225\n",
      "Epoch [17/40] Validation Loss: 0.0038844406604766845\n",
      "Epoch [17/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.68      0.68       759\n",
      "           1       0.64      0.65      0.65       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [17/40] Accuracy: 0.6996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.80it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/40] Training Loss: 0.003786401155889793\n",
      "Epoch [18/40] Validation Loss: 0.003889555732409159\n",
      "Epoch [18/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.72      0.69       759\n",
      "           1       0.66      0.59      0.62       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [18/40] Accuracy: 0.7046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.78it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/40] Training Loss: 0.0037568745251721435\n",
      "Epoch [19/40] Validation Loss: 0.003858761323822869\n",
      "Epoch [19/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.63      0.66       759\n",
      "           1       0.63      0.70      0.66       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.67      0.66      0.66      1440\n",
      "\n",
      "Epoch [19/40] Accuracy: 0.7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.61it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/40] Training Loss: 0.003721628726257931\n",
      "Epoch [20/40] Validation Loss: 0.003875546157360077\n",
      "Epoch [20/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.71      0.69       759\n",
      "           1       0.65      0.60      0.62       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [20/40] Accuracy: 0.7134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.70it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/40] Training Loss: 0.003713920410263371\n",
      "Epoch [21/40] Validation Loss: 0.003867802189456092\n",
      "Epoch [21/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.57      0.64       759\n",
      "           1       0.61      0.75      0.68       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.67      0.66      0.66      1440\n",
      "weighted avg       0.67      0.66      0.66      1440\n",
      "\n",
      "Epoch [21/40] Accuracy: 0.7163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.80it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/40] Training Loss: 0.0036710917641351315\n",
      "Epoch [22/40] Validation Loss: 0.0038430941187673146\n",
      "Epoch [22/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       759\n",
      "           1       0.64      0.65      0.65       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [22/40] Accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.83it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/40] Training Loss: 0.0036402509951299804\n",
      "Epoch [23/40] Validation Loss: 0.003844314192732175\n",
      "Epoch [23/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.70       759\n",
      "           1       0.66      0.63      0.65       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [23/40] Accuracy: 0.7313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.79it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/40] Training Loss: 0.0036460778030799872\n",
      "Epoch [24/40] Validation Loss: 0.0038611435227923923\n",
      "Epoch [24/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.60      0.66       759\n",
      "           1       0.63      0.74      0.68       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [24/40] Accuracy: 0.7293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.92it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/40] Training Loss: 0.0036239902005523322\n",
      "Epoch [25/40] Validation Loss: 0.003840582693616549\n",
      "Epoch [25/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.65      0.67       759\n",
      "           1       0.64      0.68      0.66       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.67      0.67      0.66      1440\n",
      "weighted avg       0.67      0.66      0.66      1440\n",
      "\n",
      "Epoch [25/40] Accuracy: 0.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.65it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/40] Training Loss: 0.003604227152253638\n",
      "Epoch [26/40] Validation Loss: 0.0038413347055514654\n",
      "Epoch [26/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.63      0.66       759\n",
      "           1       0.63      0.71      0.67       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [26/40] Accuracy: 0.7393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.88it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/40] Training Loss: 0.0035760182985502646\n",
      "Epoch [27/40] Validation Loss: 0.003845470564232932\n",
      "Epoch [27/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.61      0.66       759\n",
      "           1       0.63      0.73      0.68       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.68      0.67      0.67      1440\n",
      "weighted avg       0.68      0.67      0.67      1440\n",
      "\n",
      "Epoch [27/40] Accuracy: 0.7459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.82it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/40] Training Loss: 0.003560076578956038\n",
      "Epoch [28/40] Validation Loss: 0.003819638200932079\n",
      "Epoch [28/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       759\n",
      "           1       0.64      0.67      0.66       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [28/40] Accuracy: 0.7444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.80it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/40] Training Loss: 0.0035544291183918957\n",
      "Epoch [29/40] Validation Loss: 0.003867849831779798\n",
      "Epoch [29/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       759\n",
      "           1       0.62      0.75      0.68       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.67      0.67      0.66      1440\n",
      "weighted avg       0.67      0.66      0.66      1440\n",
      "\n",
      "Epoch [29/40] Accuracy: 0.7437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.76it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/40] Training Loss: 0.0035094984089254462\n",
      "Epoch [30/40] Validation Loss: 0.0038811109132236904\n",
      "Epoch [30/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.58      0.65       759\n",
      "           1       0.63      0.78      0.69       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.68      0.68      0.67      1440\n",
      "weighted avg       0.69      0.67      0.67      1440\n",
      "\n",
      "Epoch [30/40] Accuracy: 0.7555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.71it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/40] Training Loss: 0.0035137739960847203\n",
      "Epoch [31/40] Validation Loss: 0.0038964255816406673\n",
      "Epoch [31/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.56      0.64       759\n",
      "           1       0.62      0.79      0.69       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.68      0.68      0.67      1440\n",
      "weighted avg       0.69      0.67      0.67      1440\n",
      "\n",
      "Epoch [31/40] Accuracy: 0.7547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.70it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/40] Training Loss: 0.003495307313328023\n",
      "Epoch [32/40] Validation Loss: 0.003903319231337971\n",
      "Epoch [32/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.70       759\n",
      "           1       0.66      0.54      0.59       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.64      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [32/40] Accuracy: 0.7578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.77it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/40] Training Loss: 0.00346962545137118\n",
      "Epoch [33/40] Validation Loss: 0.003838244370288319\n",
      "Epoch [33/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.68       759\n",
      "           1       0.65      0.69      0.67       681\n",
      "\n",
      "    accuracy                           0.68      1440\n",
      "   macro avg       0.68      0.68      0.68      1440\n",
      "weighted avg       0.68      0.68      0.68      1440\n",
      "\n",
      "Epoch [33/40] Accuracy: 0.7622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.75it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/40] Training Loss: 0.0034727444201622195\n",
      "Epoch [34/40] Validation Loss: 0.0038300053526957828\n",
      "Epoch [34/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       759\n",
      "           1       0.66      0.64      0.65       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [34/40] Accuracy: 0.7599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.95it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/40] Training Loss: 0.0034514303982223584\n",
      "Epoch [35/40] Validation Loss: 0.0038351755175325608\n",
      "Epoch [35/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.65      0.68       759\n",
      "           1       0.64      0.70      0.67       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.68      0.67      1440\n",
      "weighted avg       0.68      0.67      0.67      1440\n",
      "\n",
      "Epoch [35/40] Accuracy: 0.7653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.74it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/40] Training Loss: 0.00341665865175144\n",
      "Epoch [36/40] Validation Loss: 0.0038485397895177205\n",
      "Epoch [36/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.65      0.68       759\n",
      "           1       0.64      0.70      0.67       681\n",
      "\n",
      "    accuracy                           0.68      1440\n",
      "   macro avg       0.68      0.68      0.68      1440\n",
      "weighted avg       0.68      0.68      0.68      1440\n",
      "\n",
      "Epoch [36/40] Accuracy: 0.7694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.88it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/40] Training Loss: 0.0033954001234536768\n",
      "Epoch [37/40] Validation Loss: 0.0038499570141235988\n",
      "Epoch [37/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.64      0.68       759\n",
      "           1       0.64      0.72      0.68       681\n",
      "\n",
      "    accuracy                           0.68      1440\n",
      "   macro avg       0.68      0.68      0.68      1440\n",
      "weighted avg       0.68      0.68      0.68      1440\n",
      "\n",
      "Epoch [37/40] Accuracy: 0.7777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.92it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/40] Training Loss: 0.003404490015308829\n",
      "Epoch [38/40] Validation Loss: 0.0038540390216641957\n",
      "Epoch [38/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.70       759\n",
      "           1       0.66      0.64      0.65       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [38/40] Accuracy: 0.7719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.54it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/40] Training Loss: 0.003408442734595306\n",
      "Epoch [39/40] Validation Loss: 0.003964760485622618\n",
      "Epoch [39/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.80      0.70       759\n",
      "           1       0.68      0.47      0.56       681\n",
      "\n",
      "    accuracy                           0.64      1440\n",
      "   macro avg       0.65      0.64      0.63      1440\n",
      "weighted avg       0.65      0.64      0.63      1440\n",
      "\n",
      "Epoch [39/40] Accuracy: 0.7707\n",
      "Training complete!\n",
      "=======================================\n",
      "BEST MODEL\n",
      "Best epoch: 28\n",
      "Best validation loss: 0.003819638200932079\n",
      "Best classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       759\n",
      "           1       0.64      0.67      0.66       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Define your model\n",
    "model = EmotionClassifier(input_size=768*3, hidden_size=2000, num_emotions=2, num_layers=3, dropout=0.3)\n",
    "model.to(\"cuda:1\")\n",
    "\n",
    "num_epochs = 40\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = AdamW(model.parameters(), lr=0.5*1e-5)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Define training parameters\n",
    "\n",
    "# Training loop\n",
    "best_model_file = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "best_classification_report = None\n",
    "\n",
    "print(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "print(TEXT_EMBEDDINGS_FILEPATH)\n",
    "print(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "for epoch in (range(num_epochs)):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):  # Assuming you have a DataLoader for your dataset\n",
    "        # Extract data from the batch\n",
    "        audio = batch['audio'].to('cuda:1')\n",
    "        video = batch['video'].to('cuda:1')\n",
    "        text = batch['text'].to('cuda:1')\n",
    "        emotion_indices = batch['cause_labels'].to('cuda:1')\n",
    "        pad_mask = batch['pad_mask'].to('cuda:1')\n",
    "\n",
    "        # Forward pass\n",
    "        emotion_logits = model(audio, video, text)\n",
    "\n",
    "        # Reshape emotion_logits\n",
    "        emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "        # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "        emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "        # Calculate a mask to exclude padded positions from the loss\n",
    "        pad_mask = pad_mask.view(-1)     \n",
    "\n",
    "        # Calculate the loss, excluding padded positions\n",
    "        loss = criterion(emotion_logits, emotion_indices)\n",
    "        # masked_loss = torch.sum(loss * pad_mask) / torch.sum(pad_mask)\n",
    "        masked_loss = loss\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        masked_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += masked_loss.item()\n",
    "        total_tokens += torch.sum(pad_mask).item()\n",
    "        \n",
    "        predicted_emotions = torch.argmax(emotion_logits, dim=1)\n",
    "        correct_predictions = ((predicted_emotions == emotion_indices) * pad_mask).sum().item()\n",
    "\n",
    "        total_correct += correct_predictions\n",
    "        total_predictions += torch.sum(pad_mask).item()  # Batch size\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    total_val_tokens = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_predictions = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    padded_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(validation_dataloader):\n",
    "            audio = val_batch['audio'].to('cuda:1')\n",
    "            video = val_batch['video'].to('cuda:1')\n",
    "            text = val_batch['text'].to('cuda:1')\n",
    "            emotion_indices = val_batch['cause_labels'].to('cuda:1')\n",
    "            pad_mask = val_batch['pad_mask'].to('cuda:1')\n",
    "\n",
    "            emotion_logits = model(audio, video, text)\n",
    "\n",
    "            # Reshape emotion_logits\n",
    "            emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "            # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "            emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "            pad_mask = pad_mask.view(-1)   \n",
    "\n",
    "            # Calculate the loss, excluding padded positions\n",
    "            val_loss = criterion(emotion_logits, emotion_indices)\n",
    "            masked_loss = torch.sum(val_loss * pad_mask) / torch.sum(pad_mask)\n",
    "            \n",
    "            total_val_loss += masked_loss.item()\n",
    "            total_val_tokens += torch.sum(pad_mask).item()\n",
    "            \n",
    "            predicted_emotions_val = torch.argmax(emotion_logits, dim=1)\n",
    "            correct_predictions_val = ((predicted_emotions_val == emotion_indices) * pad_mask).sum().item()\n",
    "            total_val_correct += correct_predictions_val\n",
    "            total_val_predictions += torch.sum(pad_mask).item()\n",
    "\n",
    "            # Store true and predicted labels for F1 score calculation\n",
    "            true_labels.extend(emotion_indices.cpu().numpy())\n",
    "            predicted_labels.extend(predicted_emotions_val.cpu().numpy())\n",
    "            padded_labels.extend(pad_mask.cpu().numpy())\n",
    "\n",
    "    final_true_labels = [label for label, pad in zip(true_labels, padded_labels) if pad == 1]\n",
    "    final_predicted_labels = [label for label, pad in zip(predicted_labels, padded_labels) if pad == 1]\n",
    "    classification_rep = classification_report(final_true_labels, final_predicted_labels)\n",
    "\n",
    "    # Calculate and print the average loss for this epoch\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    avg_val_loss = total_val_loss / total_val_tokens\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Training Loss: {avg_loss}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Validation Loss: {avg_val_loss}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Classification Report:\\n{classification_rep}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Accuracy: {total_correct / total_predictions:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        best_classification_report = classification_rep\n",
    "        best_model_file = f\"/tmp/semeval24_task3/baseline_models/cause_models/best_cause_model.pt\"\n",
    "        torch.save(model.state_dict(), best_model_file)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"/tmp/semeval24_task3/baseline_models/cause_models/cause_model_{epoch:02}.pt\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(\"=======================================\")\n",
    "print(\"BEST MODEL\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Best validation loss: {best_val_loss}\")\n",
    "print(f\"Best classification report:\\n{best_classification_report}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
