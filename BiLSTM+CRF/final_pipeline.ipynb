{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c616ee6-0273-43a9-baad-a07c3bac3672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# DATA_DIR = \"/tmp/semeval24_task3/\"\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "TRAIN_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/training.json\"\n",
    "TEST_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/testing.json\"\n",
    "# TEST_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Evaluation_Data/Subtask_2_test.json\"\n",
    "from encoder_paths import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ce159fc-758c-4bb2-860a-ea15885d1a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(TEST_DATA_FILEPATH) as f:\n",
    "    test_data = json.load(f)\n",
    "with open(TRAIN_DATA_FILEPATH) as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15715037-fba8-4cee-939c-9af7c8f25346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        with open(audio_embeddings_path, \"rb\") as f:\n",
    "            self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        if audio_name == \"dia2020utt6\":\n",
    "            return torch.zeros(768)\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        audio_embedding = audio_embedding.squeeze()\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    \n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        with open(video_embeddings_path, \"rb\") as f:\n",
    "            self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        # video_name = video_name.split(\".\")[0]\n",
    "        video_embedding = self.video_embeddings[video_name].reshape((16,-1))\n",
    "        video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "734b7058-fa30-471e-8c71-9b88af2fd1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2059, 1024, 1472, 5282, 1647, 369, 326]\n"
     ]
    }
   ],
   "source": [
    "class EmotionIndexer:\n",
    "    def __init__(self):\n",
    "        self.emotion_to_index = {\n",
    "            'joy': 0,\n",
    "            'sadness': 1,\n",
    "            'anger': 2,\n",
    "            'neutral': 3,\n",
    "            'surprise': 4,\n",
    "            'disgust': 5,\n",
    "            'fear': 6,\n",
    "            'pad': 7,\n",
    "        }\n",
    "        self.emotion_freq = [0]*7\n",
    "        self.weights = None\n",
    "\n",
    "        self.index_to_emotion = {index: emotion for emotion, index in self.emotion_to_index.items()}\n",
    "\n",
    "    def emotion_to_idx(self, emotion):\n",
    "        return self.emotion_to_index.get(emotion, None)\n",
    "\n",
    "    def idx_to_emotion(self, index):\n",
    "        return self.index_to_emotion.get(index, None)\n",
    "\n",
    "    def compute_weights(self, data):\n",
    "        for conversation in data:\n",
    "            conversation = conversation['conversation']\n",
    "            for utterance in conversation:\n",
    "                emotion = utterance['emotion']\n",
    "                self.emotion_freq[self.emotion_to_index[emotion]] += 1\n",
    "        print(self.emotion_freq)\n",
    "        self.weights = [1/freq for freq in self.emotion_freq]\n",
    "\n",
    "# Example usage\n",
    "indexer = EmotionIndexer()\n",
    "indexer.compute_weights(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60972bfd-56a1-46cd-9587-2a06b4357891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1961a5a6-f3ce-4f37-9f18-f454ba29aa37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, json_file, audio_encoder, video_encoder, text_encoder, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = self.load_data(json_file)\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def load_data(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.data[idx]['conversation']\n",
    "        emotion_labels = []\n",
    "        audio_paths = [utterance['video_name'].replace('mp4', 'wav') for utterance in conversation]\n",
    "        video_paths = [utterance['video_name'] for utterance in conversation]\n",
    "        texts = [utterance['video_name'] for utterance in conversation]\n",
    "\n",
    "        audio_embeddings = [self.audio_encoder.lmao(audio_path) for audio_path in audio_paths]\n",
    "        video_embeddings = [self.video_encoder.lmao(video_path) for video_path in video_paths]\n",
    "        text_embeddings = [self.text_encoder.lmao(text) for text in texts]\n",
    "        \n",
    "        cause_pairs = []\n",
    "        useful_utterances = set([int(cause_pair[1]) for cause_pair in cause_pairs])\n",
    "        cause_labels = []\n",
    "        for utterance in conversation:\n",
    "            if utterance['utterance_ID'] in useful_utterances:\n",
    "                cause_labels.append(1)\n",
    "            else:\n",
    "                cause_labels.append(0)\n",
    "\n",
    "        # Pad or truncate conversations to the maximum sequence length\n",
    "        if len(conversation) < self.max_seq_len:\n",
    "            pad_length = self.max_seq_len - len(conversation)\n",
    "            audio_embeddings += [torch.zeros_like(audio_embeddings[0])] * pad_length\n",
    "            video_embeddings += [torch.zeros_like(video_embeddings[0])] * pad_length\n",
    "            text_embeddings += [torch.zeros_like(text_embeddings[0])] * pad_length\n",
    "            emotion_labels += ['pad'] * pad_length\n",
    "            cause_labels += [-1] * pad_length\n",
    "            pad_mask = [1] * len(conversation) + [0] * pad_length\n",
    "        else:\n",
    "            audio_embeddings = audio_embeddings[:self.max_seq_len]\n",
    "            video_embeddings = video_embeddings[:self.max_seq_len]\n",
    "            text_embeddings = text_embeddings[:self.max_seq_len]\n",
    "            emotion_labels = emotion_labels[:self.max_seq_len]\n",
    "            cause_labels = cause_labels[:self.max_seq_len]\n",
    "            pad_mask = [1] * self.max_seq_len\n",
    "\n",
    "        emotion_indices = [indexer.emotion_to_idx(emotion) for emotion in emotion_labels]\n",
    "        \n",
    "        audio_embeddings = torch.stack(audio_embeddings)\n",
    "        video_embeddings = torch.stack(video_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        emotion_indices = torch.from_numpy(np.array(emotion_indices))\n",
    "        pad_mask = torch.from_numpy(np.array(pad_mask))\n",
    "        cause_labels = torch.from_numpy(np.array(cause_labels))\n",
    "        \n",
    "        return {\n",
    "            'audio': audio_embeddings,\n",
    "            'video': video_embeddings,\n",
    "            'text': text_embeddings,\n",
    "            # 'conversation_id': \n",
    "        }\n",
    "# Example usage\n",
    "# You need to define your audio, video, and text encoders accordingly\n",
    "\n",
    "# Define your data paths\n",
    "# AUDIO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/audio_embedding_6373.npy\"\n",
    "# VIDEO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\"\n",
    "# TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "max_seq_len = 35  # Adjust this according to your needs\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "test_dataset = ConversationDataset(TEST_DATA_FILEPATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Example of iterating through batches\n",
    "for batch in test_dataloader:\n",
    "    audio = batch['audio']  # Shape: (batch_size, max_seq_len, audio_embedding_size)\n",
    "    video = batch['video']  # Shape: (batch_size, max_seq_len, video_embedding_size)\n",
    "    text = batch['text']    # Shape: (batch_size, max_seq_len, text_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76c585d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        # self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "        # Replace Transformer with BiLSTM\n",
    "        self.bilstm = nn.LSTM(input_size, input_size // 2, num_layers, \n",
    "                              dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.final_linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "        # Concatenate or combine the audio, video, and text encodings\n",
    "        audio_encoding = audio_encoding.float()\n",
    "        video_encoding = video_encoding.float()\n",
    "        text_encoding = text_encoding.float()\n",
    "        \n",
    "        audio_encoding = self.audio_dropout(audio_encoding)\n",
    "        video_encoding = self.video_dropout(video_encoding)\n",
    "        text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "        combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "        # Take the output of the BiLSTM\n",
    "        emotion_logits = self.linear(lstm_output)\n",
    "        emotion_logits = self.relu(emotion_logits)\n",
    "        emotion_logits = self.final_linear(emotion_logits)\n",
    "        # Apply a softmax layer\n",
    "        emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "        return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5f9fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_positional_embeddings(dimension, count):\n",
    "    embeddings = [list(np.zeros(dimension))]\n",
    "    embeddings.extend([\n",
    "        list(np.random.normal(loc=0.0, scale=0.1, size=dimension)) for _ in range(count)\n",
    "    ])\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8327df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionCauseDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        utterance_embedding_size,\n",
    "        device,\n",
    "        hidden_dimension=4096,\n",
    "        positional_embeddings_dimension=200,\n",
    "        dropout=0.2,\n",
    "        *args, **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "        positional_embeddings = generate_positional_embeddings(positional_embeddings_dimension, 200)\n",
    "        self.positional_embeddings = torch.from_numpy(positional_embeddings).to(device).float()\n",
    "        \n",
    "        self.non_neutral_dropout = nn.Dropout(dropout)\n",
    "        self.candidate_cause_dropout = nn.Dropout(dropout)\n",
    "        self.distance_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(utterance_embedding_size*2 + positional_embeddings_dimension, hidden_dimension)\n",
    "        self.linear1_activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dimension, 1)\n",
    "\n",
    "    def forward(self, non_neutral_utterances, candidate_cause_utterances, distances):\n",
    "        positional_embedding = self.positional_embeddings[distances].float()\n",
    "        \n",
    "        non_neutral_utterances = self.non_neutral_dropout(non_neutral_utterances)\n",
    "        candidate_cause_utterances = self.candidate_cause_dropout(candidate_cause_utterances)\n",
    "        positional_embedding = self.distance_dropout(positional_embedding)\n",
    "\n",
    "        embeddings = torch.concat((non_neutral_utterances, candidate_cause_utterances, positional_embedding), axis=1).float()\n",
    "\n",
    "        return self.linear2(\n",
    "            self.linear1_activation(\n",
    "                self.linear1(embeddings)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3da338a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from TorchCRF import CRF\n",
    "\n",
    "class EmotionClassifierCRF(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "        super(EmotionClassifierCRF, self).__init__()\n",
    "        \n",
    "        self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "        # Replace Transformer with BiLSTM\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "                              dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "        self.crf_model = CRF(num_emotions)\n",
    "        \n",
    "\n",
    "    def generate_emissions(self, audio_encoding, video_encoding, text_encoding):\n",
    "        # Concatenate or combine the audio, video, and text encodings\n",
    "        audio_encoding = audio_encoding.float()\n",
    "        video_encoding = video_encoding.float()\n",
    "        text_encoding = text_encoding.float()\n",
    "        \n",
    "        audio_encoding = self.audio_dropout(audio_encoding)\n",
    "        video_encoding = self.video_dropout(video_encoding)\n",
    "        text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "        combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "        combined_encoding = self.first_linear(combined_encoding)\n",
    "        combined_encoding = self.relu(combined_encoding)\n",
    "        combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "        # Take the output of the BiLSTM\n",
    "        emotion_logits = self.linear(lstm_output)\n",
    "        # Apply a softmax layer\n",
    "        # emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "        return emotion_logits\n",
    "\n",
    "    def loss(self, audio_encoding, video_encoding, text_encoding, emotion_labels, padding):\n",
    "\n",
    "        emissions = self.generate_emissions(audio_encoding, video_encoding, text_encoding)\n",
    "        emotion_labels = emotion_labels.unsqueeze(1)\n",
    "        x, y, _ = emissions.shape\n",
    "        padding = torch.ones((x, y), dtype=torch.bool).to('cuda')\n",
    "        emotion_labels = emotion_labels.squeeze(1)\n",
    "        loss = -self.crf_model(emissions, emotion_labels, padding)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, audio_encoding, video_encoding, text_encoding):\n",
    "        emissions = self.generate_emissions(audio_encoding, video_encoding, text_encoding)\n",
    "        x, y, _ = emissions.shape\n",
    "        padding = torch.ones((x, y), dtype=torch.bool).to('cuda')\n",
    "        label = self.crf_model.viterbi_decode(emissions, padding)\n",
    "        return label\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim).to('cuda'), torch.randn(2, 1, self.hidden_dim).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2922c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/semeval24_task3/audio_embeddings/audio_embeddings_microsoft_wavlm-base-plus-sd.pkl\n",
      "/tmp/semeval24_task3/text_embeddings/text_embeddings_roberta_base_emotion.pkl\n",
      "/tmp/semeval24_task3/video_embeddings/final_embeddings.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [00:03<00:00, 41.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "emotion_classifier = EmotionClassifierCRF(input_size=768*3, hidden_size=2000, num_emotions=7, dropout=0.3, num_layers=4)\n",
    "emotion_classifier.load_state_dict(torch.load('/tmp/semeval24_task3/baseline_models/emotion_models/emotion_model_59.pt'))\n",
    "\n",
    "cause_classifier = EmotionClassifier(input_size=768*3, hidden_size=2000, num_emotions=2, dropout=0.3, num_layers=3)\n",
    "cause_classifier.load_state_dict(torch.load('/tmp/semeval24_task3/baseline_models/cause_models/best_cause_model.pt'))\n",
    "\n",
    "emotion_cause_detector = EmotionCauseDetector(utterance_embedding_size=768*3, device='cuda:1', hidden_dimension=2000)\n",
    "emotion_cause_detector.load_state_dict(torch.load('/tmp/semeval24_task3/baseline_models/pairing_models/paring_model_best_model.pt'))\n",
    "\n",
    "emotion_classifier.to('cuda:1')\n",
    "cause_classifier.to('cuda:1')\n",
    "emotion_cause_detector.to('cuda:1')\n",
    "positional_embeddings = np.load('/tmp/semeval24_task3/baseline_models/pairing_models/pairing_model_pos_embeds_best_model.npy')\n",
    "emotion_cause_detector.positional_embeddings = torch.from_numpy(positional_embeddings).to('cuda:1').float()\n",
    "print(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "print(TEXT_EMBEDDINGS_FILEPATH)\n",
    "print(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "emotion_cause_pairs = defaultdict(list)\n",
    "\n",
    "for conversation_idx, conversation in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    conversation_id = conversation['conversation_ID']\n",
    "    \n",
    "    audio_paths = [utterance['video_name'].replace('mp4', 'wav') for utterance in conversation['conversation']]\n",
    "    video_paths = [utterance['video_name'] for utterance in conversation['conversation']]\n",
    "    texts = [utterance['video_name'] for utterance in conversation['conversation']]\n",
    "    \n",
    "    audio_embeddings = [audio_encoder.lmao(audio_path) for audio_path in audio_paths]\n",
    "    video_embeddings = [video_encoder.lmao(video_path) for video_path in video_paths]\n",
    "    text_embeddings = [text_encoder.lmao(text) for text in texts]\n",
    "\n",
    "    audio_embeddings = torch.stack(audio_embeddings)\n",
    "    video_embeddings = torch.stack(video_embeddings)\n",
    "    text_embeddings = torch.stack(text_embeddings)\n",
    "    \n",
    "    audio_embeddings = audio_embeddings.unsqueeze(0).to('cuda:1')\n",
    "    video_embeddings = video_embeddings.unsqueeze(0).to('cuda:1')\n",
    "    text_embeddings = text_embeddings.unsqueeze(0).to('cuda:1')\n",
    "    \n",
    "    # emotion_logits = emotion_classifier(audio_embeddings, video_embeddings, text_embeddings)\n",
    "    # emotion_logits = emotion_logits.squeeze(0)\n",
    "    # predicted_emotions = torch.argmax(emotion_logits, dim=1)\n",
    "    predicted_emotions = emotion_classifier.predict(audio_embeddings, video_embeddings, text_embeddings)[0]\n",
    "    candidate_utterances = [(idx, emotion) for idx, emotion in enumerate(predicted_emotions) if emotion != 3]\n",
    "    \n",
    "    cause_logits = cause_classifier(audio_embeddings, video_embeddings, text_embeddings)\n",
    "    cause_logits = cause_logits.squeeze(0)\n",
    "    predicted_causes = torch.argmax(cause_logits, dim=1)\n",
    "    \n",
    "    candidate_causes = [idx for idx, cause in enumerate(predicted_causes) if cause == 1]\n",
    "    \n",
    "    for candidate_utterance in candidate_utterances:\n",
    "        for candidate_cause in candidate_causes:\n",
    "            utterance_embedding = torch.cat((audio_embeddings[0][candidate_utterance[0]], video_embeddings[0][candidate_utterance[0]], text_embeddings[0][candidate_utterance[0]]), dim=0)\n",
    "            cause_embedding = torch.cat((audio_embeddings[0][candidate_cause], video_embeddings[0][candidate_cause], text_embeddings[0][candidate_cause]), dim=0)\n",
    "            distance = abs(candidate_utterance[0] - candidate_cause)\n",
    "            \n",
    "            utterance_embedding = utterance_embedding.unsqueeze(0).to('cuda:1')\n",
    "            cause_embedding = cause_embedding.unsqueeze(0).to('cuda:1')\n",
    "            distance = torch.tensor([distance]).to('cuda:1')\n",
    "            prediction = emotion_cause_detector(utterance_embedding, cause_embedding, distance)\n",
    "            prediction = torch.sigmoid(prediction)\n",
    "            prediction = prediction.cpu().item()\n",
    "            if prediction >= 0.5:\n",
    "                emotion_cause_pairs[conversation_id].append((f\"{candidate_utterance[0]+1}_{indexer.idx_to_emotion(candidate_utterance[1])}\", f\"{candidate_cause+1}\"))\n",
    "    test_data[conversation_idx]['emotion-cause_pairs'] = emotion_cause_pairs[conversation_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8b8924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Subtask_2_pred.json', 'w') as f:\n",
    "    json.dump(test_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55b7ad89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: Subtask_2_pred.json (deflated 90%)\n"
     ]
    }
   ],
   "source": [
    "!zip bilstm+crf.zip Subtask_2_pred.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/suyash.mathur/semeval24/task3/BiLSTM+CRF\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a02de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
