{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "from encoder_paths import *\n",
    "import json\n",
    "from pprint import pprint\n",
    "TRAIN_FILE_PATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/training.json\"\n",
    "VALIDATION_FILE_PATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/testing.json\"\n",
    "with open(TRAIN_FILE_PATH) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(VALIDATION_FILE_PATH) as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "pprint(len(train_data))\n",
    "pprint(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f048422e190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "import torch\n",
    "\n",
    "numpy.random.seed(69)\n",
    "random.seed(69)\n",
    "torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'surprise', 'joy', 'fear', 'sadness', 'disgust', 'anger', 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "all_emotions = set()\n",
    "for conversation in train_data:\n",
    "    conversation = conversation[\"conversation\"]\n",
    "    for utterance in conversation:\n",
    "        all_emotions.add(utterance[\"emotion\"])\n",
    "pprint(all_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionIndexer:\n",
    "    def __init__(self):\n",
    "        self.emotion_to_index = {\n",
    "            'joy': 0,\n",
    "            'sadness': 1,\n",
    "            'anger': 2,\n",
    "            'neutral': 3,\n",
    "            'surprise': 4,\n",
    "            'disgust': 5,\n",
    "            'fear': 6,\n",
    "        }\n",
    "\n",
    "        self.index_to_emotion = {index: emotion for emotion, index in self.emotion_to_index.items()}\n",
    "\n",
    "    def emotion_to_idx(self, emotion):\n",
    "        return self.emotion_to_index.get(emotion, None)\n",
    "\n",
    "    def idx_to_emotion(self, index):\n",
    "        return self.index_to_emotion.get(index, None)\n",
    "\n",
    "# Example usage\n",
    "indexer = EmotionIndexer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        with open(audio_embeddings_path, \"rb\") as f:\n",
    "            self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        audio_embedding = audio_embedding.squeeze()\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    \n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        with open(video_embeddings_path, \"rb\") as f:\n",
    "            self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        # video_name = video_name.split(\".\")[0]\n",
    "        video_embedding = self.video_embeddings[video_name].reshape((16,-1))\n",
    "        video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, json_file, audio_encoder, video_encoder, text_encoder, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = self.load_data(json_file)\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def load_data(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.data[idx]['conversation']\n",
    "        # emotion_labels = [utterance['emotion'] for utterance in conversation]\n",
    "        audio_paths = [utterance['video_name'].replace('mp4', 'wav') for utterance in conversation]\n",
    "        video_paths = [utterance['video_name'] for utterance in conversation]\n",
    "        texts = [utterance['video_name'] for utterance in conversation]\n",
    "\n",
    "        audio_embeddings = [self.audio_encoder.lmao(audio_path) for audio_path in audio_paths]\n",
    "        video_embeddings = [self.video_encoder.lmao(video_path) for video_path in video_paths]\n",
    "        text_embeddings = [self.text_encoder.lmao(text) for text in texts]\n",
    "\n",
    "        cause_pairs = self.data[idx]['emotion-cause_pairs']\n",
    "        useful_utterances = set([int(cause_pair[1]) for cause_pair in cause_pairs])\n",
    "        cause_labels = []\n",
    "        for utterance in conversation:\n",
    "            if utterance['utterance_ID'] in useful_utterances:\n",
    "                cause_labels.append(1)\n",
    "            else:\n",
    "                cause_labels.append(0)\n",
    "        \n",
    "        # Pad or truncate conversations to the maximum sequence length\n",
    "        if len(conversation) < self.max_seq_len:\n",
    "            pad_length = self.max_seq_len - len(conversation)\n",
    "            audio_embeddings += [torch.zeros_like(audio_embeddings[0])] * pad_length\n",
    "            video_embeddings += [torch.zeros_like(video_embeddings[0])] * pad_length\n",
    "            text_embeddings += [torch.zeros_like(text_embeddings[0])] * pad_length\n",
    "            cause_labels += [-1] * pad_length\n",
    "            pad_mask = [1] * len(conversation) + [0] * pad_length\n",
    "        else:\n",
    "            audio_embeddings = audio_embeddings[:self.max_seq_len]\n",
    "            video_embeddings = video_embeddings[:self.max_seq_len]\n",
    "            text_embeddings = text_embeddings[:self.max_seq_len]\n",
    "            cause_labels = cause_labels[:self.max_seq_len]\n",
    "            pad_mask = [1] * self.max_seq_len\n",
    "\n",
    "        audio_embeddings = torch.stack(audio_embeddings)\n",
    "        video_embeddings = torch.stack(video_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        cause_labels = torch.from_numpy(np.array(cause_labels))\n",
    "        pad_mask = torch.from_numpy(np.array(pad_mask))\n",
    "        \n",
    "        return {\n",
    "            'audio': audio_embeddings,\n",
    "            'video': video_embeddings,\n",
    "            'text': text_embeddings,\n",
    "            'cause_labels': cause_labels,\n",
    "            'pad_mask': pad_mask\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "# You need to define your audio, video, and text encoders accordingly\n",
    "\n",
    "# Define your data paths\n",
    "# DATA_DIR = \"/tmp/semeval24_task3\"\n",
    "\n",
    "# AUDIO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/audio_embedding_6373.npy\"\n",
    "# VIDEO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\"\n",
    "# TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "max_seq_len = 35  # Adjust this according to your needs\n",
    "\n",
    "train_dataset = ConversationDataset(TRAIN_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "validation_dataset = ConversationDataset(VALIDATION_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=True)\n",
    "# Example of iterating through batches\n",
    "# for batch in dataloader:\n",
    "#     audio = batch['audio']  # Shape: (batch_size, max_seq_len, audio_embedding_size)\n",
    "#     video = batch['video']  # Shape: (batch_size, max_seq_len, video_embedding_size)\n",
    "#     text = batch['text']    # Shape: (batch_size, max_seq_len, text_embedding_size)\n",
    "#     cause_labels = batch['cause_labels']  # List of emotion labels for each utterance in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout, num_emotions):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "\n",
    "#         self.transformer_encoder = TransformerEncoder(\n",
    "#             TransformerEncoderLayer(hidden_size, num_heads, hidden_size, dropout),\n",
    "#             num_layers\n",
    "#         )\n",
    "        \n",
    "#         self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "#     def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "\n",
    "#         # Concatenate or combine the audio, video, and text encodings here\n",
    "#         # You can use any method like concatenation, addition, or other fusion techniques\n",
    "#         # Combine the encodings (you can customize this part)\n",
    "#         audio_encoding = audio_encoding.float()\n",
    "#         video_encoding = video_encoding.float()\n",
    "#         text_encoding = text_encoding.float().squeeze()\n",
    "#         combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "#         combined_encoding = self.first_linear(combined_encoding)\n",
    "        \n",
    "        \n",
    "#         combined_encoding = combined_encoding.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, input_size)\n",
    "        \n",
    "        \n",
    "#         transformer_output = self.transformer_encoder(combined_encoding)\n",
    "\n",
    "#         # Take the output of the Transformer encoder for the last position as the summary\n",
    "#         emotion_logits = self.linear(transformer_output.permute(1, 0, 2))\n",
    "\n",
    "#         return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        # self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "        # Replace Transformer with BiLSTM\n",
    "        self.bilstm = nn.LSTM(input_size, input_size // 2, num_layers, \n",
    "                              dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.final_linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "        # Concatenate or combine the audio, video, and text encodings\n",
    "        audio_encoding = audio_encoding.float()\n",
    "        video_encoding = video_encoding.float()\n",
    "        text_encoding = text_encoding.float().squeeze()\n",
    "        \n",
    "        audio_encoding = self.audio_dropout(audio_encoding)\n",
    "        video_encoding = self.video_dropout(video_encoding)\n",
    "        text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "        combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "        # Take the output of the BiLSTM\n",
    "        emotion_logits = self.linear(lstm_output)\n",
    "        emotion_logits = self.relu(emotion_logits)\n",
    "        emotion_logits = self.final_linear(emotion_logits)\n",
    "        # Apply a softmax layer\n",
    "        emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "        return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "#         self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "#         self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "#         self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#         self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "#         # Replace Transformer with BiLSTM\n",
    "#         self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "#                               dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "#         self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "#     def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "#         # Concatenate or combine the audio, video, and text encodings\n",
    "#         audio_encoding = audio_encoding.float()\n",
    "#         video_encoding = video_encoding.float()\n",
    "#         text_encoding = text_encoding.float().squeeze()\n",
    "        \n",
    "#         audio_encoding = self.audio_dropout(audio_encoding)\n",
    "#         video_encoding = self.video_dropout(video_encoding)\n",
    "#         text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "#         combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "#         combined_encoding = self.first_linear(combined_encoding)\n",
    "#         combined_encoding = self.relu(combined_encoding)\n",
    "#         combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        \n",
    "#         # Pass through BiLSTM\n",
    "#         lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "#         # Take the output of the BiLSTM\n",
    "#         emotion_logits = self.linear(lstm_output)\n",
    "#         # Apply a softmax layer\n",
    "#         emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "#         return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/semeval24_task3/audio_embeddings/audio_embeddings_facebook_wav2vec2-large-960h.pkl\n",
      "/tmp/semeval24_task3/text_embeddings/text_embeddings_roberta_large.pkl\n",
      "/tmp/semeval24_task3/video_embeddings/final_embeddings.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.10it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.19it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/40] Training Loss: 0.004432162052011713\n",
      "Epoch [0/40] Validation Loss: 0.004313827181855838\n",
      "Epoch [0/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       759\n",
      "           1       0.00      0.00      0.00       681\n",
      "\n",
      "    accuracy                           0.53      1440\n",
      "   macro avg       0.26      0.50      0.35      1440\n",
      "weighted avg       0.28      0.53      0.36      1440\n",
      "\n",
      "Epoch [0/40] Accuracy: 0.5178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.26it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] Training Loss: 0.004421530167098625\n",
      "Epoch [1/40] Validation Loss: 0.004305437869495816\n",
      "Epoch [1/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.96      0.69       759\n",
      "           1       0.66      0.08      0.15       681\n",
      "\n",
      "    accuracy                           0.55      1440\n",
      "   macro avg       0.60      0.52      0.42      1440\n",
      "weighted avg       0.60      0.55      0.43      1440\n",
      "\n",
      "Epoch [1/40] Accuracy: 0.5311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.42it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40] Training Loss: 0.004410019878370537\n",
      "Epoch [2/40] Validation Loss: 0.004299492099218898\n",
      "Epoch [2/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63       759\n",
      "           1       0.52      0.32      0.39       681\n",
      "\n",
      "    accuracy                           0.54      1440\n",
      "   macro avg       0.53      0.53      0.51      1440\n",
      "weighted avg       0.53      0.54      0.52      1440\n",
      "\n",
      "Epoch [2/40] Accuracy: 0.5387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.32it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 28.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/40] Training Loss: 0.004395265201011386\n",
      "Epoch [3/40] Validation Loss: 0.004279197007417679\n",
      "Epoch [3/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.82      0.66       759\n",
      "           1       0.56      0.26      0.35       681\n",
      "\n",
      "    accuracy                           0.55      1440\n",
      "   macro avg       0.56      0.54      0.51      1440\n",
      "weighted avg       0.56      0.55      0.51      1440\n",
      "\n",
      "Epoch [3/40] Accuracy: 0.5562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.70it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 31.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/40] Training Loss: 0.0043686538437961485\n",
      "Epoch [4/40] Validation Loss: 0.004254042150245773\n",
      "Epoch [4/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.68      0.63       759\n",
      "           1       0.57      0.47      0.51       681\n",
      "\n",
      "    accuracy                           0.58      1440\n",
      "   macro avg       0.58      0.57      0.57      1440\n",
      "weighted avg       0.58      0.58      0.57      1440\n",
      "\n",
      "Epoch [4/40] Accuracy: 0.5678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 13.15it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 28.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/40] Training Loss: 0.004325378845552181\n",
      "Epoch [5/40] Validation Loss: 0.004233534592721197\n",
      "Epoch [5/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.87      0.69       759\n",
      "           1       0.64      0.25      0.35       681\n",
      "\n",
      "    accuracy                           0.58      1440\n",
      "   macro avg       0.60      0.56      0.52      1440\n",
      "weighted avg       0.60      0.58      0.53      1440\n",
      "\n",
      "Epoch [5/40] Accuracy: 0.5890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.39it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/40] Training Loss: 0.004281669691167096\n",
      "Epoch [6/40] Validation Loss: 0.0041812701771656675\n",
      "Epoch [6/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.73      0.66       759\n",
      "           1       0.61      0.47      0.53       681\n",
      "\n",
      "    accuracy                           0.60      1440\n",
      "   macro avg       0.61      0.60      0.59      1440\n",
      "weighted avg       0.61      0.60      0.60      1440\n",
      "\n",
      "Epoch [6/40] Accuracy: 0.5989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.43it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/40] Training Loss: 0.00423403800400091\n",
      "Epoch [7/40] Validation Loss: 0.004192857734031148\n",
      "Epoch [7/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.45      0.53       759\n",
      "           1       0.54      0.72      0.62       681\n",
      "\n",
      "    accuracy                           0.58      1440\n",
      "   macro avg       0.59      0.59      0.57      1440\n",
      "weighted avg       0.59      0.58      0.57      1440\n",
      "\n",
      "Epoch [7/40] Accuracy: 0.6101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 13.27it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 31.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/40] Training Loss: 0.004191168365303113\n",
      "Epoch [8/40] Validation Loss: 0.004178258900841077\n",
      "Epoch [8/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.41      0.50       759\n",
      "           1       0.54      0.77      0.63       681\n",
      "\n",
      "    accuracy                           0.58      1440\n",
      "   macro avg       0.60      0.59      0.57      1440\n",
      "weighted avg       0.60      0.58      0.57      1440\n",
      "\n",
      "Epoch [8/40] Accuracy: 0.6205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.76it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 30.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/40] Training Loss: 0.004145043933222237\n",
      "Epoch [9/40] Validation Loss: 0.004137834658225378\n",
      "Epoch [9/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.47      0.56       759\n",
      "           1       0.56      0.74      0.64       681\n",
      "\n",
      "    accuracy                           0.60      1440\n",
      "   macro avg       0.61      0.61      0.60      1440\n",
      "weighted avg       0.62      0.60      0.59      1440\n",
      "\n",
      "Epoch [9/40] Accuracy: 0.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 13.15it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 32.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/40] Training Loss: 0.004097416440257592\n",
      "Epoch [10/40] Validation Loss: 0.004060859688454204\n",
      "Epoch [10/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.63      0.63       759\n",
      "           1       0.59      0.60      0.60       681\n",
      "\n",
      "    accuracy                           0.62      1440\n",
      "   macro avg       0.61      0.61      0.61      1440\n",
      "weighted avg       0.62      0.62      0.62      1440\n",
      "\n",
      "Epoch [10/40] Accuracy: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.91it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 31.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/40] Training Loss: 0.004052048349470544\n",
      "Epoch [11/40] Validation Loss: 0.0040413457486364575\n",
      "Epoch [11/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.56      0.61       759\n",
      "           1       0.58      0.68      0.62       681\n",
      "\n",
      "    accuracy                           0.62      1440\n",
      "   macro avg       0.62      0.62      0.62      1440\n",
      "weighted avg       0.62      0.62      0.61      1440\n",
      "\n",
      "Epoch [11/40] Accuracy: 0.6558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.65it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/40] Training Loss: 0.0040304938681323945\n",
      "Epoch [12/40] Validation Loss: 0.003985426450769106\n",
      "Epoch [12/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66       759\n",
      "           1       0.62      0.58      0.60       681\n",
      "\n",
      "    accuracy                           0.63      1440\n",
      "   macro avg       0.63      0.63      0.63      1440\n",
      "weighted avg       0.63      0.63      0.63      1440\n",
      "\n",
      "Epoch [12/40] Accuracy: 0.6542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.65it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 29.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/40] Training Loss: 0.003974441506512422\n",
      "Epoch [13/40] Validation Loss: 0.0039923477503988475\n",
      "Epoch [13/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       759\n",
      "           1       0.61      0.60      0.61       681\n",
      "\n",
      "    accuracy                           0.63      1440\n",
      "   macro avg       0.63      0.63      0.63      1440\n",
      "weighted avg       0.63      0.63      0.63      1440\n",
      "\n",
      "Epoch [13/40] Accuracy: 0.6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.96it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 31.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/40] Training Loss: 0.003922506641975539\n",
      "Epoch [14/40] Validation Loss: 0.003962202577127351\n",
      "Epoch [14/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.64       759\n",
      "           1       0.60      0.64      0.62       681\n",
      "\n",
      "    accuracy                           0.63      1440\n",
      "   macro avg       0.63      0.63      0.63      1440\n",
      "weighted avg       0.63      0.63      0.63      1440\n",
      "\n",
      "Epoch [14/40] Accuracy: 0.6807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 13.16it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 32.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/40] Training Loss: 0.0038799748200287285\n",
      "Epoch [15/40] Validation Loss: 0.003951410576701164\n",
      "Epoch [15/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.64      0.65       759\n",
      "           1       0.61      0.63      0.62       681\n",
      "\n",
      "    accuracy                           0.64      1440\n",
      "   macro avg       0.64      0.64      0.64      1440\n",
      "weighted avg       0.64      0.64      0.64      1440\n",
      "\n",
      "Epoch [15/40] Accuracy: 0.6868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.87it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 31.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/40] Training Loss: 0.003846643926161774\n",
      "Epoch [16/40] Validation Loss: 0.0039505101326439114\n",
      "Epoch [16/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.61      0.65       759\n",
      "           1       0.62      0.70      0.65       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.66      0.65      0.65      1440\n",
      "\n",
      "Epoch [16/40] Accuracy: 0.6911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.91it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 31.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/40] Training Loss: 0.003817333925709034\n",
      "Epoch [17/40] Validation Loss: 0.0039039902802970672\n",
      "Epoch [17/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.60      0.64       759\n",
      "           1       0.61      0.68      0.64       681\n",
      "\n",
      "    accuracy                           0.64      1440\n",
      "   macro avg       0.64      0.64      0.64      1440\n",
      "weighted avg       0.64      0.64      0.64      1440\n",
      "\n",
      "Epoch [17/40] Accuracy: 0.7011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.40it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 28.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/40] Training Loss: 0.003789861866460566\n",
      "Epoch [18/40] Validation Loss: 0.0039044116106298236\n",
      "Epoch [18/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68       759\n",
      "           1       0.64      0.61      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [18/40] Accuracy: 0.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.95it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 31.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/40] Training Loss: 0.0037728100195337433\n",
      "Epoch [19/40] Validation Loss: 0.003909130560027229\n",
      "Epoch [19/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       759\n",
      "           1       0.63      0.62      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [19/40] Accuracy: 0.7053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.93it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/40] Training Loss: 0.003723768972980535\n",
      "Epoch [20/40] Validation Loss: 0.003918072829643886\n",
      "Epoch [20/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.60      0.64       759\n",
      "           1       0.61      0.70      0.65       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [20/40] Accuracy: 0.7191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.49it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 29.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/40] Training Loss: 0.003721099840869398\n",
      "Epoch [21/40] Validation Loss: 0.0038884758949279787\n",
      "Epoch [21/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68       759\n",
      "           1       0.65      0.60      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [21/40] Accuracy: 0.7206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.51it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 17.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/40] Training Loss: 0.0036923224306016516\n",
      "Epoch [22/40] Validation Loss: 0.0038914344376987883\n",
      "Epoch [22/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       759\n",
      "           1       0.63      0.63      0.63       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [22/40] Accuracy: 0.7194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.64it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/40] Training Loss: 0.0036727931613218944\n",
      "Epoch [23/40] Validation Loss: 0.003893142234947946\n",
      "Epoch [23/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.71      0.68       759\n",
      "           1       0.64      0.59      0.61       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [23/40] Accuracy: 0.7303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.39it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 30.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/40] Training Loss: 0.003655463621058246\n",
      "Epoch [24/40] Validation Loss: 0.003918081108066771\n",
      "Epoch [24/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.73      0.69       759\n",
      "           1       0.66      0.57      0.61       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.65      0.65      1440\n",
      "weighted avg       0.66      0.66      0.65      1440\n",
      "\n",
      "Epoch [24/40] Accuracy: 0.7265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.56it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/40] Training Loss: 0.003639743464932143\n",
      "Epoch [25/40] Validation Loss: 0.0039380136877298355\n",
      "Epoch [25/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.59      0.64       759\n",
      "           1       0.61      0.72      0.66       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.66      0.66      0.65      1440\n",
      "weighted avg       0.66      0.65      0.65      1440\n",
      "\n",
      "Epoch [25/40] Accuracy: 0.7336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 11.64it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/40] Training Loss: 0.0036409232003493637\n",
      "Epoch [26/40] Validation Loss: 0.003903531738453441\n",
      "Epoch [26/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.74      0.69       759\n",
      "           1       0.66      0.57      0.61       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.65      0.65      1440\n",
      "weighted avg       0.66      0.66      0.65      1440\n",
      "\n",
      "Epoch [26/40] Accuracy: 0.7287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.78it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/40] Training Loss: 0.003599119395377337\n",
      "Epoch [27/40] Validation Loss: 0.003936663187212414\n",
      "Epoch [27/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.62      0.65       759\n",
      "           1       0.62      0.68      0.65       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [27/40] Accuracy: 0.7394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.40it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 28.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/40] Training Loss: 0.0035759854545753004\n",
      "Epoch [28/40] Validation Loss: 0.0039062396105792787\n",
      "Epoch [28/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       759\n",
      "           1       0.63      0.61      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [28/40] Accuracy: 0.7441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 13.20it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/40] Training Loss: 0.003593160291445922\n",
      "Epoch [29/40] Validation Loss: 0.0038902780248059165\n",
      "Epoch [29/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.72      0.69       759\n",
      "           1       0.65      0.59      0.62       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.66      0.66      0.65      1440\n",
      "\n",
      "Epoch [29/40] Accuracy: 0.7366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.47it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 30.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/40] Training Loss: 0.0035578151261711544\n",
      "Epoch [30/40] Validation Loss: 0.003937760285205311\n",
      "Epoch [30/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.61      0.65       759\n",
      "           1       0.61      0.69      0.65       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [30/40] Accuracy: 0.7451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.45it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/40] Training Loss: 0.0035378196838699015\n",
      "Epoch [31/40] Validation Loss: 0.003917498058742947\n",
      "Epoch [31/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.62      0.65       759\n",
      "           1       0.62      0.70      0.66       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.66      0.66      0.65      1440\n",
      "weighted avg       0.66      0.65      0.65      1440\n",
      "\n",
      "Epoch [31/40] Accuracy: 0.7513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.97it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 30.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/40] Training Loss: 0.0035328638970592513\n",
      "Epoch [32/40] Validation Loss: 0.003911727542678515\n",
      "Epoch [32/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.73      0.69       759\n",
      "           1       0.66      0.58      0.62       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.65      0.65      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [32/40] Accuracy: 0.7471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.83it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 30.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/40] Training Loss: 0.003505770076974408\n",
      "Epoch [33/40] Validation Loss: 0.0039278373950057555\n",
      "Epoch [33/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       759\n",
      "           1       0.64      0.62      0.63       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [33/40] Accuracy: 0.7572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.08it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/40] Training Loss: 0.0035066669729603797\n",
      "Epoch [34/40] Validation Loss: 0.003901997125811047\n",
      "Epoch [34/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68       759\n",
      "           1       0.65      0.60      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [34/40] Accuracy: 0.7587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.14it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/40] Training Loss: 0.0034678478978598134\n",
      "Epoch [35/40] Validation Loss: 0.003916717775993877\n",
      "Epoch [35/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       759\n",
      "           1       0.64      0.63      0.63       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [35/40] Accuracy: 0.7630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.28it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 31.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/40] Training Loss: 0.0034764722553671007\n",
      "Epoch [36/40] Validation Loss: 0.003967098063892788\n",
      "Epoch [36/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       759\n",
      "           1       0.63      0.63      0.63       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [36/40] Accuracy: 0.7579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.63it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/40] Training Loss: 0.00344978559828615\n",
      "Epoch [37/40] Validation Loss: 0.0039352566003799435\n",
      "Epoch [37/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.62      0.66       759\n",
      "           1       0.62      0.70      0.66       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [37/40] Accuracy: 0.7650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.25it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 30.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/40] Training Loss: 0.0034564683414639017\n",
      "Epoch [38/40] Validation Loss: 0.003917074493236012\n",
      "Epoch [38/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.66       759\n",
      "           1       0.62      0.68      0.65       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.66      0.65      1440\n",
      "weighted avg       0.66      0.65      0.65      1440\n",
      "\n",
      "Epoch [38/40] Accuracy: 0.7628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:06<00:00, 12.77it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 30.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/40] Training Loss: 0.0034487429525731027\n",
      "Epoch [39/40] Validation Loss: 0.0038887867910994424\n",
      "Epoch [39/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       759\n",
      "           1       0.63      0.67      0.65       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.66      0.65      0.65      1440\n",
      "\n",
      "Epoch [39/40] Accuracy: 0.7659\n",
      "Training complete!\n",
      "=======================================\n",
      "BEST MODEL\n",
      "Best epoch: 21\n",
      "Best validation loss: 0.0038884758949279787\n",
      "Best classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68       759\n",
      "           1       0.65      0.60      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Define your model\n",
    "model = EmotionClassifier(input_size=768+1024*2, hidden_size=2000, num_emotions=2, num_layers=2, dropout=0.3)\n",
    "model.to(\"cuda:1\")\n",
    "\n",
    "num_epochs = 40\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = AdamW(model.parameters(), lr=0.5*1e-5)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Define training parameters\n",
    "\n",
    "# Training loop\n",
    "best_model_file = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "best_classification_report = None\n",
    "\n",
    "print(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "print(TEXT_EMBEDDINGS_FILEPATH)\n",
    "print(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "for epoch in (range(num_epochs)):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):  # Assuming you have a DataLoader for your dataset\n",
    "        # Extract data from the batch\n",
    "        audio = batch['audio'].to('cuda:1')\n",
    "        video = batch['video'].to('cuda:1')\n",
    "        text = batch['text'].to('cuda:1')\n",
    "        emotion_indices = batch['cause_labels'].to('cuda:1')\n",
    "        pad_mask = batch['pad_mask'].to('cuda:1')\n",
    "\n",
    "        # Forward pass\n",
    "        emotion_logits = model(audio, video, text)\n",
    "\n",
    "        # Reshape emotion_logits\n",
    "        emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "        # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "        emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "        # Calculate a mask to exclude padded positions from the loss\n",
    "        pad_mask = pad_mask.view(-1)     \n",
    "\n",
    "        # Calculate the loss, excluding padded positions\n",
    "        loss = criterion(emotion_logits, emotion_indices)\n",
    "        # masked_loss = torch.sum(loss * pad_mask) / torch.sum(pad_mask)\n",
    "        masked_loss = loss\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        masked_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += masked_loss.item()\n",
    "        total_tokens += torch.sum(pad_mask).item()\n",
    "        \n",
    "        predicted_emotions = torch.argmax(emotion_logits, dim=1)\n",
    "        correct_predictions = ((predicted_emotions == emotion_indices) * pad_mask).sum().item()\n",
    "\n",
    "        total_correct += correct_predictions\n",
    "        total_predictions += torch.sum(pad_mask).item()  # Batch size\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    total_val_tokens = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_predictions = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    padded_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(validation_dataloader):\n",
    "            audio = val_batch['audio'].to('cuda:1')\n",
    "            video = val_batch['video'].to('cuda:1')\n",
    "            text = val_batch['text'].to('cuda:1')\n",
    "            emotion_indices = val_batch['cause_labels'].to('cuda:1')\n",
    "            pad_mask = val_batch['pad_mask'].to('cuda:1')\n",
    "\n",
    "            emotion_logits = model(audio, video, text)\n",
    "\n",
    "            # Reshape emotion_logits\n",
    "            emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "            # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "            emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "            pad_mask = pad_mask.view(-1)   \n",
    "\n",
    "            # Calculate the loss, excluding padded positions\n",
    "            val_loss = criterion(emotion_logits, emotion_indices)\n",
    "            masked_loss = torch.sum(val_loss * pad_mask) / torch.sum(pad_mask)\n",
    "            \n",
    "            total_val_loss += masked_loss.item()\n",
    "            total_val_tokens += torch.sum(pad_mask).item()\n",
    "            \n",
    "            predicted_emotions_val = torch.argmax(emotion_logits, dim=1)\n",
    "            correct_predictions_val = ((predicted_emotions_val == emotion_indices) * pad_mask).sum().item()\n",
    "            total_val_correct += correct_predictions_val\n",
    "            total_val_predictions += torch.sum(pad_mask).item()\n",
    "\n",
    "            # Store true and predicted labels for F1 score calculation\n",
    "            true_labels.extend(emotion_indices.cpu().numpy())\n",
    "            predicted_labels.extend(predicted_emotions_val.cpu().numpy())\n",
    "            padded_labels.extend(pad_mask.cpu().numpy())\n",
    "\n",
    "    final_true_labels = [label for label, pad in zip(true_labels, padded_labels) if pad == 1]\n",
    "    final_predicted_labels = [label for label, pad in zip(predicted_labels, padded_labels) if pad == 1]\n",
    "    classification_rep = classification_report(final_true_labels, final_predicted_labels)\n",
    "\n",
    "    # Calculate and print the average loss for this epoch\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    avg_val_loss = total_val_loss / total_val_tokens\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Training Loss: {avg_loss}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Validation Loss: {avg_val_loss}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Classification Report:\\n{classification_rep}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Accuracy: {total_correct / total_predictions:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        best_classification_report = classification_rep\n",
    "        best_model_file = f\"/tmp/semeval24_task3/baseline_models/cause_models/best_cause_model.pt\"\n",
    "        torch.save(model.state_dict(), best_model_file)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"/tmp/semeval24_task3/baseline_models/cause_models/cause_model_{epoch:02}.pt\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(\"=======================================\")\n",
    "print(\"BEST MODEL\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Best validation loss: {best_val_loss}\")\n",
    "print(f\"Best classification report:\\n{best_classification_report}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
