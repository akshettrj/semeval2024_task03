{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba951e4-a7d5-4f21-8fb8-7918de7bd983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "from encoder_paths import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2a0059-825e-4bc6-affd-6b1eb34e4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# import random\n",
    "# import torch\n",
    "\n",
    "# numpy.random.seed(69)\n",
    "# random.seed(69)\n",
    "# torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2806718a-4a8a-473c-9ea8-99d1d03768b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "BASE_DIR = \"/tmp/akshett.jindal\"\n",
    "HUGGINGFACE_CACHE_DIR = os.path.join(BASE_DIR, \".huggingface_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b524940c-cccb-46c7-b285-a5210aa69bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# DATA_DIR = \"/tmp/semeval24_task3\"\n",
    "\n",
    "# TRAIN_DATA_FILEPATH = os.path.join(DATA_DIR, \"final_clean_data\", \"train\", \"Subtask_2.json\")\n",
    "# VAL_DATA_FILEPATH = os.path.join(DATA_DIR, \"final_clean_data\", \"val\", \"Subtask_2.json\")\n",
    "\n",
    "TRAIN_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/training.json\"\n",
    "VAL_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/testing.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86071165-de8e-4a02-91c2-66165d6d65c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        with open(audio_embeddings_path, \"rb\") as f:\n",
    "            self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        audio_embedding = audio_embedding.squeeze()\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    \n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        with open(video_embeddings_path, \"rb\") as f:\n",
    "            self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        # video_name = video_name.split(\".\")[0]\n",
    "        video_embedding = self.video_embeddings[video_name].reshape((16,-1))\n",
    "        video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544e6927-b52c-4edf-b7e4-bd9285d35a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmotionCauseDataset(Dataset):\n",
    "    def __init__(self, file_path, audio_encoder, video_encoder, text_encoder, neg_to_pos_ratio):\n",
    "        with open(file_path) as f:\n",
    "            self.file_data = json.load(f)\n",
    "\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.data = []\n",
    "        self.POSITIVE_SAMPLE_COUNT = 0\n",
    "        self.NEGATIVE_SAMPLE_COUNT = 0\n",
    "\n",
    "        for conversation in self.file_data:\n",
    "            positive_samples = []\n",
    "            negative_samples = []\n",
    "\n",
    "            utterances = {\n",
    "                utterance[\"utterance_ID\"]: utterance\n",
    "                for utterance in conversation[\"conversation\"]\n",
    "            }\n",
    "            utterance_ids = set(utterances.keys())\n",
    "\n",
    "            causes = {\n",
    "                utterance_id: []\n",
    "                for utterance_id in utterance_ids\n",
    "            }\n",
    "\n",
    "            for emo_cause_pairs in conversation[\"emotion-cause_pairs\"]:\n",
    "                emotion_utterance_num, emotion = emo_cause_pairs[0].split(\"_\")\n",
    "                emotion_utterance_num = int(emotion_utterance_num)\n",
    "                cause_utterance_num = int(emo_cause_pairs[1])\n",
    "\n",
    "                if emotion != \"neutral\":\n",
    "                    causes[emotion_utterance_num].append(cause_utterance_num)\n",
    "\n",
    "            for emo_utterance_id in utterance_ids:\n",
    "                emo_utterance = utterances[emo_utterance_id]\n",
    "\n",
    "                if utterances[emo_utterance_id][\"emotion\"] == \"neutral\":\n",
    "                    continue\n",
    "\n",
    "                for cause_utterance_id in utterance_ids:\n",
    "                    cause_utterance = utterances[cause_utterance_id]\n",
    "\n",
    "                    is_cause = cause_utterance_id in causes[emo_utterance_id]\n",
    "\n",
    "                    data_point = {\n",
    "                        \"original_utterance\": {\n",
    "                            \"id\": emo_utterance_id,\n",
    "                            \"text\": emo_utterance[\"text\"],\n",
    "                            \"video_name\": emo_utterance[\"video_name\"],\n",
    "                        },\n",
    "                        \"cause_utterance\": {\n",
    "                            \"id\": cause_utterance_id,\n",
    "                            \"text\": cause_utterance[\"text\"],\n",
    "                            \"video_name\": cause_utterance[\"video_name\"],\n",
    "                        },\n",
    "                        \"is_cause\": is_cause,\n",
    "                    }\n",
    "\n",
    "                    if is_cause:\n",
    "                        positive_samples.append(data_point)\n",
    "                        self.POSITIVE_SAMPLE_COUNT += 1\n",
    "                    else:\n",
    "                        negative_samples.append(data_point)\n",
    "                        self.NEGATIVE_SAMPLE_COUNT += 1\n",
    "\n",
    "            random.shuffle(negative_samples)\n",
    "\n",
    "            self.data.extend(positive_samples)\n",
    "            if neg_to_pos_ratio is not None:\n",
    "                self.data.extend(negative_samples[:min(neg_to_pos_ratio * len(positive_samples), len(negative_samples))])\n",
    "            else:\n",
    "                self.data.extend(negative_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "\n",
    "        orig_utt = data[\"original_utterance\"]\n",
    "        cause_utt = data[\"cause_utterance\"]\n",
    "\n",
    "        orig_id = orig_utt[\"id\"]\n",
    "        orig_text = orig_utt[\"text\"]\n",
    "        orig_video = orig_utt[\"video_name\"]\n",
    "        orig_audio = orig_utt[\"video_name\"].replace(\".mp4\", \".wav\")\n",
    "\n",
    "        cause_id = cause_utt[\"id\"]\n",
    "        cause_text = cause_utt[\"text\"]\n",
    "        cause_video = cause_utt[\"video_name\"]\n",
    "        cause_audio = cause_utt[\"video_name\"].replace(\".mp4\", \".wav\")\n",
    "\n",
    "        is_cause = data[\"is_cause\"]\n",
    "\n",
    "        return {\n",
    "            \"distance\": abs(orig_id - cause_id),\n",
    "            \"original_audio\": self.audio_encoder.lmao(orig_audio).float(),\n",
    "            \"original_video\": self.video_encoder.lmao(orig_video).float(),\n",
    "            \"original_text\": self.text_encoder.lmao(orig_video).squeeze().float(),\n",
    "            \"cause_audio\": self.audio_encoder.lmao(cause_audio).float(),\n",
    "            \"cause_video\": self.video_encoder.lmao(cause_video).float(),\n",
    "            \"cause_text\": self.text_encoder.lmao(cause_video).squeeze().float(),\n",
    "            \"is_cause\": 1.0 if is_cause else 0.0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246a8dd9-1de3-4c4b-b1b2-3089cfd6a653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49835, 11047)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# AUDIO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/audio_embedding_6373.npy\"\n",
    "# VIDEO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\"\n",
    "# TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "\n",
    "trn_dataset = EmotionCauseDataset(\n",
    "    TRAIN_DATA_FILEPATH,\n",
    "    audio_encoder,\n",
    "    video_encoder,\n",
    "    text_encoder,\n",
    "    neg_to_pos_ratio=5\n",
    ")\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = EmotionCauseDataset(\n",
    "    VAL_DATA_FILEPATH,\n",
    "    audio_encoder,\n",
    "    video_encoder,\n",
    "    text_encoder,\n",
    "    neg_to_pos_ratio=None,\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "len(trn_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae58a04-a8b4-444c-a76d-e7a3ac6fe11e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_positional_embeddings(dimension, count):\n",
    "    embeddings = [list(np.zeros(dimension))]\n",
    "    embeddings.extend([\n",
    "        list(np.random.normal(loc=0.0, scale=0.1, size=dimension)) for _ in range(count)\n",
    "    ])\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5243cb81-825d-4146-8db0-46ed822dd34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionCauseDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        utterance_embedding_size,\n",
    "        device,\n",
    "        hidden_dimension=4096,\n",
    "        positional_embeddings_dimension=200,\n",
    "        dropout=0.2,\n",
    "        *args, **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "        positional_embeddings = generate_positional_embeddings(positional_embeddings_dimension, 200)\n",
    "        self.positional_embeddings = torch.from_numpy(positional_embeddings).to(device).float()\n",
    "        \n",
    "        self.non_neutral_dropout = nn.Dropout(dropout)\n",
    "        self.candidate_cause_dropout = nn.Dropout(dropout)\n",
    "        self.distance_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(utterance_embedding_size*2 + positional_embeddings_dimension, hidden_dimension)\n",
    "        self.linear1_activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dimension, 1)\n",
    "\n",
    "    def forward(self, non_neutral_utterances, candidate_cause_utterances, distances):\n",
    "        positional_embedding = self.positional_embeddings[distances]\n",
    "        \n",
    "        non_neutral_utterances = self.non_neutral_dropout(non_neutral_utterances)\n",
    "        candidate_cause_utterances = self.candidate_cause_dropout(candidate_cause_utterances)\n",
    "        positional_embedding = self.distance_dropout(positional_embedding)\n",
    "\n",
    "        embeddings = torch.concat((non_neutral_utterances, candidate_cause_utterances, positional_embedding), axis=1)\n",
    "\n",
    "        return self.linear2(\n",
    "            self.linear1_activation(\n",
    "                self.linear1(embeddings)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f41eb7cc-c5c2-4709-bb9f-3e0ba28a8616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "def save_model(epoch_num):\n",
    "    torch.save(model.state_dict(), f\"/tmp/semeval24_task3/baseline_models/pairing_models/paring_model_{epoch_num:02}.pt\")\n",
    "    numpy.save(\n",
    "        f\"/tmp/semeval24_task3/baseline_models/pairing_models/pairing_model_pos_embeds_{epoch_num:02}.npy\",\n",
    "        model.positional_embeddings.cpu().numpy(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63eaef23-10f5-4068-b853-7f2d9a393201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/semeval24_task3/audio_embeddings/audio_embeddings_facebook_wav2vec2-large-960h.pkl\n",
      "/tmp/semeval24_task3/video_embeddings/final_embeddings.pkl\n",
      "/tmp/semeval24_task3/text_embeddings/text_embeddings_roberta_large.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▎         | 1/40 [00:09<06:27,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.64      0.77     10056\n",
      "         1.0       0.17      0.75      0.28       991\n",
      "\n",
      "    accuracy                           0.65     11047\n",
      "   macro avg       0.57      0.70      0.52     11047\n",
      "weighted avg       0.89      0.65      0.72     11047\n",
      "\n",
      "Epoch [01/40] Train Loss: 0.023019805240758758\n",
      "Epoch [01/40] Validation Loss: 0.017721938183817048\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|▌         | 2/40 [00:18<05:56,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [02/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.56      0.72     10056\n",
      "         1.0       0.17      0.90      0.28       991\n",
      "\n",
      "    accuracy                           0.59     11047\n",
      "   macro avg       0.58      0.73      0.50     11047\n",
      "weighted avg       0.91      0.59      0.68     11047\n",
      "\n",
      "Epoch [02/40] Train Loss: 0.020377223390874957\n",
      "Epoch [02/40] Validation Loss: 0.017537865014028445\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [03/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.75      0.85     10056\n",
      "         1.0       0.25      0.84      0.39       991\n",
      "\n",
      "    accuracy                           0.76     11047\n",
      "   macro avg       0.62      0.80      0.62     11047\n",
      "weighted avg       0.91      0.76      0.81     11047\n",
      "\n",
      "Epoch [03/40] Train Loss: 0.018737262210572068\n",
      "Epoch [03/40] Validation Loss: 0.014832073480149373\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 4/40 [00:36<05:25,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [04/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.78      0.87     10056\n",
      "         1.0       0.27      0.84      0.41       991\n",
      "\n",
      "    accuracy                           0.79     11047\n",
      "   macro avg       0.63      0.81      0.64     11047\n",
      "weighted avg       0.92      0.79      0.83     11047\n",
      "\n",
      "Epoch [04/40] Train Loss: 0.01746926741273728\n",
      "Epoch [04/40] Validation Loss: 0.013907855917332486\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  12%|█▎        | 5/40 [00:45<05:16,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [05/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.65      0.79     10056\n",
      "         1.0       0.21      0.92      0.34       991\n",
      "\n",
      "    accuracy                           0.68     11047\n",
      "   macro avg       0.60      0.79      0.56     11047\n",
      "weighted avg       0.92      0.68      0.75     11047\n",
      "\n",
      "Epoch [05/40] Train Loss: 0.016527242160165753\n",
      "Epoch [05/40] Validation Loss: 0.015354909502065032\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  15%|█▌        | 6/40 [00:54<05:04,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [06/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.73      0.84     10056\n",
      "         1.0       0.25      0.90      0.39       991\n",
      "\n",
      "    accuracy                           0.75     11047\n",
      "   macro avg       0.62      0.82      0.62     11047\n",
      "weighted avg       0.92      0.75      0.80     11047\n",
      "\n",
      "Epoch [06/40] Train Loss: 0.01579347915844255\n",
      "Epoch [06/40] Validation Loss: 0.013766407985868704\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  18%|█▊        | 7/40 [01:03<04:57,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [07/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.80      0.88     10056\n",
      "         1.0       0.30      0.87      0.45       991\n",
      "\n",
      "    accuracy                           0.81     11047\n",
      "   macro avg       0.64      0.84      0.67     11047\n",
      "weighted avg       0.92      0.81      0.85     11047\n",
      "\n",
      "Epoch [07/40] Train Loss: 0.015204418182190963\n",
      "Epoch [07/40] Validation Loss: 0.012321149869240877\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [08/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.81      0.89     10056\n",
      "         1.0       0.31      0.87      0.46       991\n",
      "\n",
      "    accuracy                           0.82     11047\n",
      "   macro avg       0.65      0.84      0.67     11047\n",
      "weighted avg       0.92      0.82      0.85     11047\n",
      "\n",
      "Epoch [08/40] Train Loss: 0.014720347600291117\n",
      "Epoch [08/40] Validation Loss: 0.011945050958622826\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  22%|██▎       | 9/40 [01:21<04:37,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [09/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.79      0.88     10056\n",
      "         1.0       0.30      0.88      0.44       991\n",
      "\n",
      "    accuracy                           0.80     11047\n",
      "   macro avg       0.64      0.84      0.66     11047\n",
      "weighted avg       0.92      0.80      0.84     11047\n",
      "\n",
      "Epoch [09/40] Train Loss: 0.01436860574574888\n",
      "Epoch [09/40] Validation Loss: 0.01199325726455859\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 10/40 [01:30<04:26,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.77      0.87     10056\n",
      "         1.0       0.28      0.90      0.43       991\n",
      "\n",
      "    accuracy                           0.78     11047\n",
      "   macro avg       0.63      0.84      0.65     11047\n",
      "weighted avg       0.92      0.78      0.83     11047\n",
      "\n",
      "Epoch [10/40] Train Loss: 0.014087434977784401\n",
      "Epoch [10/40] Validation Loss: 0.01227590945850424\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  28%|██▊       | 11/40 [01:39<04:17,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.88     10056\n",
      "         1.0       0.30      0.88      0.45       991\n",
      "\n",
      "    accuracy                           0.81     11047\n",
      "   macro avg       0.64      0.84      0.67     11047\n",
      "weighted avg       0.92      0.81      0.84     11047\n",
      "\n",
      "Epoch [11/40] Train Loss: 0.013828440276044254\n",
      "Epoch [11/40] Validation Loss: 0.011613748835212377\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.88     10056\n",
      "         1.0       0.30      0.89      0.45       991\n",
      "\n",
      "    accuracy                           0.81     11047\n",
      "   macro avg       0.64      0.84      0.67     11047\n",
      "weighted avg       0.93      0.81      0.84     11047\n",
      "\n",
      "Epoch [12/40] Train Loss: 0.013589610907820153\n",
      "Epoch [12/40] Validation Loss: 0.011530135414704816\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  32%|███▎      | 13/40 [01:57<04:01,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.79      0.88     10056\n",
      "         1.0       0.30      0.89      0.44       991\n",
      "\n",
      "    accuracy                           0.80     11047\n",
      "   macro avg       0.64      0.84      0.66     11047\n",
      "weighted avg       0.92      0.80      0.84     11047\n",
      "\n",
      "Epoch [13/40] Train Loss: 0.013429646918290442\n",
      "Epoch [13/40] Validation Loss: 0.011649796067722757\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.35      0.84      0.49       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.84      0.70     11047\n",
      "weighted avg       0.92      0.85      0.87     11047\n",
      "\n",
      "Epoch [14/40] Train Loss: 0.013315895775298086\n",
      "Epoch [14/40] Validation Loss: 0.010550904160911109\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  38%|███▊      | 15/40 [02:14<03:42,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.81      0.89     10056\n",
      "         1.0       0.31      0.88      0.46       991\n",
      "\n",
      "    accuracy                           0.81     11047\n",
      "   macro avg       0.65      0.84      0.67     11047\n",
      "weighted avg       0.93      0.81      0.85     11047\n",
      "\n",
      "Epoch [15/40] Train Loss: 0.013227813803537438\n",
      "Epoch [15/40] Validation Loss: 0.011142473856228154\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.36      0.84      0.51       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.85      0.71     11047\n",
      "weighted avg       0.93      0.85      0.88     11047\n",
      "\n",
      "Epoch [16/40] Train Loss: 0.013066109219890034\n",
      "Epoch [16/40] Validation Loss: 0.010295246999761323\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  42%|████▎     | 17/40 [02:32<03:23,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.89     10056\n",
      "         1.0       0.31      0.89      0.46       991\n",
      "\n",
      "    accuracy                           0.81     11047\n",
      "   macro avg       0.65      0.85      0.67     11047\n",
      "weighted avg       0.93      0.81      0.85     11047\n",
      "\n",
      "Epoch [17/40] Train Loss: 0.012921296439087692\n",
      "Epoch [17/40] Validation Loss: 0.011136125945849073\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.91     10056\n",
      "         1.0       0.34      0.85      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.85      0.70     11047\n",
      "weighted avg       0.93      0.84      0.87     11047\n",
      "\n",
      "Epoch [18/40] Train Loss: 0.012886253052394301\n",
      "Epoch [18/40] Validation Loss: 0.010423825883440353\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  48%|████▊     | 19/40 [02:50<03:06,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.83      0.90     10056\n",
      "         1.0       0.34      0.86      0.48       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.85      0.69     11047\n",
      "weighted avg       0.93      0.84      0.86     11047\n",
      "\n",
      "Epoch [19/40] Train Loss: 0.012710328418250089\n",
      "Epoch [19/40] Validation Loss: 0.010514792001838982\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.36      0.84      0.51       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.85      0.71     11047\n",
      "weighted avg       0.93      0.85      0.88     11047\n",
      "\n",
      "Epoch [20/40] Train Loss: 0.01264308993739267\n",
      "Epoch [20/40] Validation Loss: 0.010144181939781059\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  52%|█████▎    | 21/40 [03:08<02:48,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.82      0.89     10056\n",
      "         1.0       0.32      0.88      0.47       991\n",
      "\n",
      "    accuracy                           0.82     11047\n",
      "   macro avg       0.65      0.85      0.68     11047\n",
      "weighted avg       0.93      0.82      0.86     11047\n",
      "\n",
      "Epoch [21/40] Train Loss: 0.012643137496183178\n",
      "Epoch [21/40] Validation Loss: 0.010695002558918853\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  55%|█████▌    | 22/40 [03:16<02:38,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.91     10056\n",
      "         1.0       0.35      0.85      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.67      0.85      0.70     11047\n",
      "weighted avg       0.93      0.84      0.87     11047\n",
      "\n",
      "Epoch [22/40] Train Loss: 0.012594211163505138\n",
      "Epoch [22/40] Validation Loss: 0.010203967989164928\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  57%|█████▊    | 23/40 [03:25<02:29,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.77      0.86     10056\n",
      "         1.0       0.28      0.91      0.43       991\n",
      "\n",
      "    accuracy                           0.78     11047\n",
      "   macro avg       0.63      0.84      0.65     11047\n",
      "weighted avg       0.92      0.78      0.82     11047\n",
      "\n",
      "Epoch [23/40] Train Loss: 0.012420908698362704\n",
      "Epoch [23/40] Validation Loss: 0.012134027492923057\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 24/40 [03:34<02:20,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.81      0.89     10056\n",
      "         1.0       0.32      0.89      0.47       991\n",
      "\n",
      "    accuracy                           0.82     11047\n",
      "   macro avg       0.65      0.85      0.68     11047\n",
      "weighted avg       0.93      0.82      0.85     11047\n",
      "\n",
      "Epoch [24/40] Train Loss: 0.012536312672914824\n",
      "Epoch [24/40] Validation Loss: 0.010765203331058607\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  62%|██████▎   | 25/40 [03:43<02:12,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.92     10056\n",
      "         1.0       0.37      0.84      0.51       991\n",
      "\n",
      "    accuracy                           0.86     11047\n",
      "   macro avg       0.67      0.85      0.71     11047\n",
      "weighted avg       0.93      0.86      0.88     11047\n",
      "\n",
      "Epoch [25/40] Train Loss: 0.012337094238018643\n",
      "Epoch [25/40] Validation Loss: 0.009961833487222509\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  65%|██████▌   | 26/40 [03:52<02:03,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.82      0.90     10056\n",
      "         1.0       0.33      0.88      0.48       991\n",
      "\n",
      "    accuracy                           0.83     11047\n",
      "   macro avg       0.66      0.85      0.69     11047\n",
      "weighted avg       0.93      0.83      0.86     11047\n",
      "\n",
      "Epoch [26/40] Train Loss: 0.01225475004423198\n",
      "Epoch [26/40] Validation Loss: 0.010454217664500734\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  68%|██████▊   | 27/40 [04:00<01:55,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.88     10056\n",
      "         1.0       0.30      0.90      0.45       991\n",
      "\n",
      "    accuracy                           0.81     11047\n",
      "   macro avg       0.65      0.85      0.67     11047\n",
      "weighted avg       0.93      0.81      0.84     11047\n",
      "\n",
      "Epoch [27/40] Train Loss: 0.012170719753150493\n",
      "Epoch [27/40] Validation Loss: 0.011216373065381912\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 28/40 [04:09<01:45,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.90     10056\n",
      "         1.0       0.34      0.86      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.85      0.70     11047\n",
      "weighted avg       0.93      0.84      0.87     11047\n",
      "\n",
      "Epoch [28/40] Train Loss: 0.012230384573309109\n",
      "Epoch [28/40] Validation Loss: 0.010183575811055728\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  72%|███████▎  | 29/40 [04:18<01:36,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.36      0.84      0.50       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.85      0.71     11047\n",
      "weighted avg       0.93      0.85      0.87     11047\n",
      "\n",
      "Epoch [29/40] Train Loss: 0.012151601501033562\n",
      "Epoch [29/40] Validation Loss: 0.010061267484035748\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 30/40 [04:27<01:27,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.36      0.84      0.51       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.85      0.71     11047\n",
      "weighted avg       0.93      0.85      0.88     11047\n",
      "\n",
      "Epoch [30/40] Train Loss: 0.012079906590325616\n",
      "Epoch [30/40] Validation Loss: 0.00999752428459374\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  78%|███████▊  | 31/40 [04:35<01:18,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.83      0.90     10056\n",
      "         1.0       0.34      0.87      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.85      0.69     11047\n",
      "weighted avg       0.93      0.84      0.86     11047\n",
      "\n",
      "Epoch [31/40] Train Loss: 0.012042137420873867\n",
      "Epoch [31/40] Validation Loss: 0.010310363571512934\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.79      0.88     10056\n",
      "         1.0       0.30      0.91      0.45       991\n",
      "\n",
      "    accuracy                           0.80     11047\n",
      "   macro avg       0.64      0.85      0.66     11047\n",
      "weighted avg       0.93      0.80      0.84     11047\n",
      "\n",
      "Epoch [32/40] Train Loss: 0.011925714443973464\n",
      "Epoch [32/40] Validation Loss: 0.011367669704421777\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  82%|████████▎ | 33/40 [04:53<01:01,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.79      0.88     10056\n",
      "         1.0       0.30      0.90      0.45       991\n",
      "\n",
      "    accuracy                           0.80     11047\n",
      "   macro avg       0.64      0.85      0.66     11047\n",
      "weighted avg       0.93      0.80      0.84     11047\n",
      "\n",
      "Epoch [33/40] Train Loss: 0.011854307895691045\n",
      "Epoch [33/40] Validation Loss: 0.0113405588541473\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  85%|████████▌ | 34/40 [05:02<00:52,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.82      0.90     10056\n",
      "         1.0       0.33      0.88      0.48       991\n",
      "\n",
      "    accuracy                           0.83     11047\n",
      "   macro avg       0.66      0.85      0.69     11047\n",
      "weighted avg       0.93      0.83      0.86     11047\n",
      "\n",
      "Epoch [34/40] Train Loss: 0.011849114518288878\n",
      "Epoch [34/40] Validation Loss: 0.010443106616719235\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  88%|████████▊ | 35/40 [05:11<00:43,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.83      0.90     10056\n",
      "         1.0       0.34      0.87      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.85      0.70     11047\n",
      "weighted avg       0.93      0.84      0.87     11047\n",
      "\n",
      "Epoch [35/40] Train Loss: 0.01174270848134657\n",
      "Epoch [35/40] Validation Loss: 0.010229980934406352\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 36/40 [05:19<00:35,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.90     10056\n",
      "         1.0       0.34      0.86      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.85      0.70     11047\n",
      "weighted avg       0.93      0.84      0.87     11047\n",
      "\n",
      "Epoch [36/40] Train Loss: 0.01169726155610412\n",
      "Epoch [36/40] Validation Loss: 0.010193402741936886\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.89     10056\n",
      "         1.0       0.31      0.89      0.46       991\n",
      "\n",
      "    accuracy                           0.81     11047\n",
      "   macro avg       0.65      0.85      0.67     11047\n",
      "weighted avg       0.93      0.81      0.85     11047\n",
      "\n",
      "Epoch [37/40] Train Loss: 0.011499413647173929\n",
      "Epoch [37/40] Validation Loss: 0.010989686616532187\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  95%|█████████▌| 38/40 [05:37<00:17,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.83      0.90     10056\n",
      "         1.0       0.34      0.86      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.85      0.70     11047\n",
      "weighted avg       0.93      0.84      0.87     11047\n",
      "\n",
      "Epoch [38/40] Train Loss: 0.011621868836603553\n",
      "Epoch [38/40] Validation Loss: 0.010225147954727204\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.36      0.84      0.51       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.85      0.71     11047\n",
      "weighted avg       0.93      0.85      0.88     11047\n",
      "\n",
      "Epoch [39/40] Train Loss: 0.011540443475663286\n",
      "Epoch [39/40] Validation Loss: 0.009907395855785595\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 40/40 [05:55<00:00,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.91     10056\n",
      "         1.0       0.35      0.85      0.50       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.85      0.70     11047\n",
      "weighted avg       0.93      0.85      0.87     11047\n",
      "\n",
      "Epoch [40/40] Train Loss: 0.01143114999688283\n",
      "Epoch [40/40] Validation Loss: 0.010021769321699705\n",
      "------------------------------------------------------------\n",
      "===================BEST MODEL===================\n",
      "Best Model Epoch: 38\n",
      "Best Model Validation Loss: 0.009907395855785595\n",
      "Best Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.36      0.84      0.51       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.85      0.71     11047\n",
      "weighted avg       0.93      0.85      0.88     11047\n",
      "\n",
      "================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "NUM_EPOCHS = 40\n",
    "AUDIO_EMBEDDING_SIZE = 1024\n",
    "VIDEO_EMBEDDING_SIZE = 768\n",
    "TEXT_EMBEDDING_SIZE = 1024\n",
    "TOTAL_EMBEDDING_SIZE = (\n",
    "    AUDIO_EMBEDDING_SIZE + VIDEO_EMBEDDING_SIZE + TEXT_EMBEDDING_SIZE\n",
    ")\n",
    "# TOTAL_EMBEDDING_SIZE = 11237\n",
    "\n",
    "model = EmotionCauseDetector(\n",
    "    TOTAL_EMBEDDING_SIZE,\n",
    "    device,\n",
    "    hidden_dimension=2000,\n",
    ")\n",
    "_ = model.to(device)\n",
    "\n",
    "weight_ratio = trn_dataset.NEGATIVE_SAMPLE_COUNT / trn_dataset.POSITIVE_SAMPLE_COUNT\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weight_ratio).to(device))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "\n",
    "total_steps = len(trn_dataloader) * NUM_EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "best_model_file = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "best_classification_report = None\n",
    "\n",
    "print(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "print(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "print(TEXT_EMBEDDINGS_FILEPATH)\n",
    "epoch_iter = tqdm(range(NUM_EPOCHS), desc=\"Epoch\", position=0)\n",
    "for epoch in epoch_iter:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(trn_dataloader, desc=\"Train Data Batch\", position=1, leave=False):\n",
    "        distances = batch[\"distance\"].to(device)\n",
    "\n",
    "        orig_audios = batch[\"original_audio\"].to(device)\n",
    "        orig_videos = batch[\"original_video\"].to(device)\n",
    "        orig_texts = batch[\"original_text\"].to(device)\n",
    "\n",
    "        cause_audios = batch[\"cause_audio\"].to(device)\n",
    "        cause_videos = batch[\"cause_video\"].to(device)\n",
    "        cause_texts = batch[\"cause_text\"].to(device)\n",
    "\n",
    "        is_cause = batch[\"is_cause\"].to(device)\n",
    "\n",
    "        orig_embedding = torch.cat((orig_audios, orig_videos, orig_texts), axis=1).float()\n",
    "        cause_embedding = torch.cat((cause_audios, cause_videos, cause_texts), axis=1).float()\n",
    "\n",
    "        output_logits = model(orig_embedding, cause_embedding, distances).squeeze()\n",
    "\n",
    "        loss = criterion(output_logits, is_cause)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    total_val_predictions = 0\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(val_dataloader, desc=\"Val Data Batch\", position=1, leave=False):\n",
    "            distances = val_batch[\"distance\"].to(device)\n",
    "\n",
    "            orig_audios = val_batch[\"original_audio\"].to(device)\n",
    "            orig_videos = val_batch[\"original_video\"].to(device)\n",
    "            orig_texts = val_batch[\"original_text\"].to(device)\n",
    "\n",
    "            cause_audios = val_batch[\"cause_audio\"].to(device)\n",
    "            cause_videos = val_batch[\"cause_video\"].to(device)\n",
    "            cause_texts = val_batch[\"cause_text\"].to(device)\n",
    "\n",
    "            is_cause = val_batch[\"is_cause\"].to(device)\n",
    "\n",
    "            orig_embedding = torch.cat((orig_audios, orig_videos, orig_texts), axis=1).float()\n",
    "            cause_embedding = torch.cat((cause_audios, cause_videos, cause_texts), axis=1).float()\n",
    "\n",
    "            output_logits = model(orig_embedding, cause_embedding, distances).squeeze()\n",
    "\n",
    "            val_loss = criterion(output_logits, is_cause)\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            predicted_is_cause = (output_logits >= 0.5).float()\n",
    "\n",
    "            correct_predictions_val = (is_cause == predicted_is_cause).sum().item()\n",
    "\n",
    "            total_val_correct += correct_predictions_val\n",
    "            total_val_predictions += (predicted_is_cause == 1.0).sum().item()\n",
    "\n",
    "            true_labels.extend(is_cause.cpu().numpy())\n",
    "            predicted_labels.extend(predicted_is_cause.cpu().numpy())\n",
    "\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "    avg_loss = total_loss / len(trn_dataset)\n",
    "    avg_val_loss = total_val_loss / len(val_dataset)\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        best_classification_report = report\n",
    "        # best_model_file = save_model(epoch)\n",
    "        torch.save(model.state_dict(), f\"/tmp/semeval24_task3/baseline_models/pairing_models/paring_model_best_model.pt\")\n",
    "        numpy.save(\n",
    "            f\"/tmp/semeval24_task3/baseline_models/pairing_models/pairing_model_pos_embeds_best_model.npy\",\n",
    "            model.positional_embeddings.cpu().numpy(),\n",
    "        )\n",
    "\n",
    "    print(f\"Epoch [{epoch+1:02}/{NUM_EPOCHS}] Classification Report:\\n{report}\")\n",
    "    print(f\"Epoch [{epoch+1:02}/{NUM_EPOCHS}] Train Loss: {avg_loss}\")\n",
    "    print(f\"Epoch [{epoch+1:02}/{NUM_EPOCHS}] Validation Loss: {avg_val_loss}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    save_model(epoch)\n",
    "\n",
    "print(\"===================BEST MODEL===================\")\n",
    "print(f\"Best Model Epoch: {best_epoch}\")\n",
    "print(f\"Best Model Validation Loss: {best_val_loss}\")\n",
    "print(f\"Best Model Classification Report:\\n{best_classification_report}\")\n",
    "print(\"================================================\")\n",
    "\n",
    "# Current best, Epoch 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28226244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
