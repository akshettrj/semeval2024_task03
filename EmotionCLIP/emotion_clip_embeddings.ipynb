{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066d5d04-2185-416e-aaf0-8846772be663",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/tmp/akshett.jindal\"\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304990ea-2a0a-4631-ac93-06944fa5e602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "MODEL_CHECKPOINTS_DL_LINK = \"https://drive.google.com/file/d/1iWA7KfiR1JjRi-hD6R4LK5cug1FMcblD/view\"\n",
    "MODEL_CHECKPOINTS_PATH = path.join(BASE_DIR, \"cached_models\", \"EmotionCLIP\", \"emotionclip_latest.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24957026-04db-4ee1-a068-f597462b1d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/tmp/akshett.jindal/shared_task_data/task03/train/dia1utt1.mp4',\n",
       "  '/tmp/akshett.jindal/shared_task_data/task03/train/dia1utt2.mp4',\n",
       "  '/tmp/akshett.jindal/shared_task_data/task03/train/dia1utt3.mp4',\n",
       "  '/tmp/akshett.jindal/shared_task_data/task03/train/dia1utt4.mp4',\n",
       "  '/tmp/akshett.jindal/shared_task_data/task03/train/dia1utt5.mp4'],\n",
       " 13619)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import path\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "video_files_glob = path.join(\n",
    "    BASE_DIR,\n",
    "    \"shared_task_data\",\n",
    "    \"task03\",\n",
    "    \"*\",\n",
    "    \"*.mp4\",\n",
    ")\n",
    "\n",
    "mp4_files = sorted(\n",
    "    glob(video_files_glob, recursive=True),\n",
    "    key=lambda fname: tuple([int(num) for num in re.findall(r\"\\d+\", fname)]),\n",
    ")\n",
    "mp4_files[:5], len(mp4_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab162071-4f54-40ad-868b-2b1e722c0238",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'EmotionCLIP' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone \"https://github.com/Xeaver/EmotionCLIP\"\n",
    "!touch EmotionCLIP/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e942055-aff4-489d-929d-f13db9e19396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a20914-866c-433c-8616-23b9072319a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from EmotionCLIP.src.models.base import EmotionCLIP\n",
    "\n",
    "model = EmotionCLIP(\n",
    "    backbone_checkpoint=None,\n",
    "    video_len=8,\n",
    "    backbone_config=\"EmotionCLIP/src/models/model_configs/ViT-B-32.json\"\n",
    ")\n",
    "ckpt = torch.load(MODEL_CHECKPOINTS_PATH, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5292332-9f66-4045-9217-e1963009b6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = MODEL_ID.replace(\"/\", \"_\").replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dee3d9-68fe-48ca-b1bd-1d4caabbb6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "HUGGINGFACE_CACHE_DIR = path.join(BASE_DIR, \".huggingface_cache\")\n",
    "OUTPUT_DIR = path.join(BASE_DIR, \"shared_task\", \"audio_embeddings\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb737e0-a8b0-4cf5-873c-9651e0563124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb640f-4db2-470c-87ba-ce00303a481f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = torch.device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed960ec-f2d1-43aa-8ad6-adefc9785590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = PROCESSOR_CLASS.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    cache_dir=HUGGINGFACE_CACHE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb174f87-4f30-4efc-97d8-546a2a21bc07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MODEL_CLASS.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    cache_dir=HUGGINGFACE_CACHE_DIR,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6033c9c-8ccf-4803-9be1-a27fdd88425a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy\n",
    "from os import path\n",
    "import soundfile\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def data_generator():\n",
    "    for wav_file in tqdm(mp4_files):\n",
    "        with open(wav_file, \"rb\") as f:\n",
    "            audio_data, _ = soundfile.read(f)\n",
    "        audio_id = path.basename(wav_file).replace(\".wav\", \"\")\n",
    "        yield { \"id\": audio_id, \"audio\": numpy.average(audio_data, axis=1) }\n",
    "\n",
    "dataset = Dataset.from_generator(data_generator)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5c61e-abfc-45c5-8cb8-4df5123c5779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx+n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece134a8-3f96-4c87-b01c-bb1abda8990d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy\n",
    "from os import path\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "BUFFER = []\n",
    "BUFFER_MAX = 500 // BATCH_SIZE\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch_num, d in tqdm(enumerate(batch(dataset, n=BATCH_SIZE)), total=len(dataset) // BATCH_SIZE):\n",
    "\n",
    "        inputs = processor(\n",
    "            raw_speech=d[\"audio\"],\n",
    "            padding=BATCH_SIZE > 1,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        for k in inputs.keys():\n",
    "            inputs[k] = inputs[k].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        for k in outputs.keys():\n",
    "            outputs[k] = numpy.array(outputs[k].cpu())\n",
    "\n",
    "        last_hidden_states = numpy.mean(outputs[\"last_hidden_state\"], axis=1)\n",
    "        extract_features = numpy.mean(outputs[\"extract_features\"], axis=1)\n",
    "        if batch_num == 0:\n",
    "            print(last_hidden_states.shape, extract_features.shape)\n",
    "\n",
    "        BUFFER.append({\n",
    "            \"ids\": d[\"id\"],\n",
    "            \"last_hidden_state\": last_hidden_states,\n",
    "            \"extract_features\": extract_features,\n",
    "        })\n",
    "\n",
    "        if len(BUFFER) == BUFFER_MAX:\n",
    "            batch_of = path.join(OUTPUT_DIR, f\"batch_{batch_num}.pkl\")\n",
    "            with open(batch_of, \"wb\") as f:\n",
    "                pickle.dump(BUFFER, f)\n",
    "            del BUFFER\n",
    "            BUFFER = []\n",
    "\n",
    "if len(BUFFER) > 0:\n",
    "    batch_of = path.join(OUTPUT_DIR, f\"batch_{batch_num}.pkl\")\n",
    "    with open(batch_of, \"wb\") as f:\n",
    "        pickle.dump(BUFFER, f)\n",
    "    del BUFFER\n",
    "    BUFFER = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
