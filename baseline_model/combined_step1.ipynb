{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8ed30e-65a3-4e55-bc32-2694490898f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home2/suyash.mathur/semeval24/task3/baseline_model/combined_step1.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/baseline_model/combined_step1.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mencoder_paths\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/baseline_model/combined_step1.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/baseline_model/combined_step1.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpprint\u001b[39;00m \u001b[39mimport\u001b[39;00m pprint\n",
      "File \u001b[0;32m~/semeval24/task3/baseline_model/encoder_paths.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m AUDIO_EMBEDDINGS_FILEPATH \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/tmp/semeval24_task3/audio_embeddings/wav2vec2-large_embeddings.pkl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m VIDEO_EMBEDDINGS_FILEPATH \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m TEXT_EMBEDDINGS_FILEPATH \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DATA_DIR, \u001b[39m\"\u001b[39m\u001b[39mtext_embeddings\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtext_embeddings_bert_base.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "from encoder_paths import *\n",
    "import json\n",
    "from pprint import pprint\n",
    "TRAIN_FILE_PATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/training.json\"\n",
    "VALIDATION_FILE_PATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Evaluation_Data/Subtask_2_test.json\"\n",
    "with open(TRAIN_FILE_PATH) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(VALIDATION_FILE_PATH) as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "pprint(len(train_data))\n",
    "pprint(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2059, 1024, 1472, 5282, 1647, 369, 326]\n",
      "[0.00048567265662943174, 0.0009765625, 0.0006793478260869565, 0.0001893222264293828, 0.0006071645415907711, 0.0027100271002710027, 0.003067484662576687]\n"
     ]
    }
   ],
   "source": [
    "class EmotionIndexer:\n",
    "    def __init__(self):\n",
    "        self.emotion_to_index = {\n",
    "            'joy': 0,\n",
    "            'sadness': 1,\n",
    "            'anger': 2,\n",
    "            'neutral': 3,\n",
    "            'surprise': 4,\n",
    "            'disgust': 5,\n",
    "            'fear': 6,\n",
    "            'pad': 7,\n",
    "        }\n",
    "        self.emotion_freq = [0]*7\n",
    "        self.weights = None\n",
    "\n",
    "        self.index_to_emotion = {index: emotion for emotion, index in self.emotion_to_index.items()}\n",
    "\n",
    "    def emotion_to_idx(self, emotion):\n",
    "        return self.emotion_to_index.get(emotion, None)\n",
    "\n",
    "    def idx_to_emotion(self, index):\n",
    "        return self.index_to_emotion.get(index, None)\n",
    "    \n",
    "    def compute_weights(self, data):\n",
    "        for conversation in data:\n",
    "            conversation = conversation['conversation']\n",
    "            for utterance in conversation:\n",
    "                emotion = utterance['emotion']\n",
    "                self.emotion_freq[self.emotion_to_index[emotion]] += 1\n",
    "        print(self.emotion_freq)\n",
    "        self.weights = [1/freq for freq in self.emotion_freq]\n",
    "\n",
    "# Example usage\n",
    "indexer = EmotionIndexer()\n",
    "indexer.compute_weights(train_data)\n",
    "print(indexer.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "VID_ID_MAPPING = np.load(\"/home2/suyash.mathur/semeval24/task3/MECPE/data/video_id_mapping.npy\", allow_pickle=True).item()\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        self.audio_embeddings = np.load(audio_embeddings_path)\n",
    "        # with open(audio_embeddings_path, \"rb\") as f:\n",
    "            # self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        audio_name = VID_ID_MAPPING[audio_name]\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    \n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        self.video_embeddings = np.load(video_embeddings_path)\n",
    "        # with open(video_embeddings_path, \"rb\") as f:\n",
    "        #     self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        video_name = video_name.split(\".\")[0]\n",
    "        video_name = VID_ID_MAPPING[video_name]\n",
    "        video_embedding = self.video_embeddings[video_name]\n",
    "        # video_embedding = video_embedding.reshape((16,-1))\n",
    "        # video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, json_file, audio_encoder, video_encoder, text_encoder, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = self.load_data(json_file)\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def load_data(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.data[idx]['conversation']\n",
    "        emotion_labels = [utterance['emotion'] for utterance in conversation]\n",
    "        audio_paths = [utterance['video_name'].replace('mp4', 'wav') for utterance in conversation]\n",
    "        video_paths = [utterance['video_name'] for utterance in conversation]\n",
    "        texts = [utterance['video_name'] for utterance in conversation]\n",
    "\n",
    "        audio_embeddings = [self.audio_encoder.lmao(audio_path) for audio_path in audio_paths]\n",
    "        video_embeddings = [self.video_encoder.lmao(video_path) for video_path in video_paths]\n",
    "        text_embeddings = [self.text_encoder.lmao(text) for text in texts]\n",
    "\n",
    "        cause_pairs = self.data[idx]['emotion-cause_pairs']\n",
    "        useful_utterances = set([int(cause_pair[1]) for cause_pair in cause_pairs])\n",
    "        cause_labels = []\n",
    "        for utterance in conversation:\n",
    "            if utterance['utterance_ID'] in useful_utterances:\n",
    "                cause_labels.append(1)\n",
    "            else:\n",
    "                cause_labels.append(0)\n",
    "        \n",
    "        \n",
    "        # Pad or truncate conversations to the maximum sequence length\n",
    "        if len(conversation) < self.max_seq_len:\n",
    "            pad_length = self.max_seq_len - len(conversation)\n",
    "            audio_embeddings += [torch.zeros_like(audio_embeddings[0])] * pad_length\n",
    "            video_embeddings += [torch.zeros_like(video_embeddings[0])] * pad_length\n",
    "            text_embeddings += [torch.zeros_like(text_embeddings[0])] * pad_length\n",
    "            cause_labels += [-1] * pad_length\n",
    "            emotion_labels += ['pad'] * pad_length\n",
    "            pad_mask = [1] * len(conversation) + [0] * pad_length\n",
    "        else:\n",
    "            audio_embeddings = audio_embeddings[:self.max_seq_len]\n",
    "            video_embeddings = video_embeddings[:self.max_seq_len]\n",
    "            text_embeddings = text_embeddings[:self.max_seq_len]\n",
    "            emotion_labels = emotion_labels[:self.max_seq_len]\n",
    "            cause_labels = cause_labels[:self.max_seq_len]\n",
    "            pad_mask = [1] * self.max_seq_len\n",
    "\n",
    "        emotion_indices = [indexer.emotion_to_idx(emotion) for emotion in emotion_labels]\n",
    "        \n",
    "        audio_embeddings = torch.stack(audio_embeddings)\n",
    "        video_embeddings = torch.stack(video_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        emotion_indices = torch.from_numpy(np.array(emotion_indices))\n",
    "        pad_mask = torch.from_numpy(np.array(pad_mask))\n",
    "        cause_labels = torch.from_numpy(np.array(cause_labels))\n",
    "        \n",
    "        return {\n",
    "            'audio': audio_embeddings,\n",
    "            'video': video_embeddings,\n",
    "            'text': text_embeddings,\n",
    "            'emotion_labels': emotion_indices,\n",
    "            'pad_mask': pad_mask,\n",
    "            'cause_labels': cause_labels,\n",
    "        }\n",
    "# Example usage\n",
    "# You need to define your audio, video, and text encoders accordingly\n",
    "\n",
    "# Define your data paths\n",
    "# DATA_DIR = \"/tmp/semeval24_task3\"\n",
    "\n",
    "# AUDIO_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"audio_embeddings\", \"audio_embeddings_new.pkl\")\n",
    "# VIDEO_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"video_embeddings\", \"final_embeddings.pkl\")\n",
    "# TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "AUDIO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/\"\n",
    "VIDEO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\"\n",
    "TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "max_seq_len = 40  # Adjust this according to your needs\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = ConversationDataset(TRAIN_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "validation_dataset = ConversationDataset(VALIDATION_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "# Example of iterating through batches\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "    # audio = batch['audio']  # Shape: (batch_size, max_seq_len, audio_embedding_size)\n",
    "    # video = batch['video']  # Shape: (batch_size, max_seq_len, video_embedding_size)\n",
    "    # text = batch['text']    # Shape: (batch_size, max_seq_len, text_embedding_size)\n",
    "    # emotions = batch['emotion_labels']  # List of emotion labels for each utterance in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BiLSTM_basic(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim=768, hidden_dim=300, output_size=13):\n",
    "        super(BiLSTM_basic, self).__init__()\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        # if embeddings is None:\n",
    "        #     self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # else:\n",
    "        # self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "        # 2. LSTM Layer\n",
    "        #embedding dimension must be equal to bert embeddings\n",
    "        #use of 'batch_first=true'?\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=1, batch_first=False)\n",
    "        \n",
    "        # 3. Optional dropout layer\n",
    "        self.dropout_layer = nn.Dropout(p=0.3)\n",
    "\n",
    "        # 4. Dense Layer ?? \n",
    "        self.hidden2tag = nn.Linear(2*hidden_dim, output_size)\n",
    "\n",
    "        self.relu=nn.ReLU\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    \n",
    "    def generate_emissions(self, batch_text):\n",
    "        hidden_layer = self.init_hidden(len(batch_text))\n",
    "\n",
    "        embeddings = enco(batch_text)\n",
    "\n",
    "        # x_packed = pack_padded_sequence(embeddings, batch_first = True)\n",
    "        \n",
    "        # packed_seqs = pack_padded_sequence(embeddings, batch_length)\n",
    "        print(embeddings.shape)\n",
    "        lstm_output, _ = self.lstm(embeddings, hidden_layer)\n",
    "        print(lstm_output.shape)\n",
    "        # lstm_output, _ = pad_packed_sequence(lstm_output)\n",
    "\n",
    "        # self.relu(lstm_output)\n",
    "        # lstm_output, op_lengths = pad_packed_sequence(lstm_output, batch_first = True)\n",
    "\n",
    "        lstm_output = self.dropout_layer(lstm_output)\n",
    "        print(lstm_output.shape)\n",
    "\n",
    "        emissions = self.hidden2tag(lstm_output)\n",
    "        # emissions = torch.squeeze(emissions)\n",
    "        # emissions = emissions.unsqueeze(0)\n",
    "\n",
    "        return emissions\n",
    "        \n",
    "    def loss(self, batch_text, batch_label):\n",
    "        # print(len(batch_text))\n",
    "\n",
    "        # hidden_layer = self.init_hidden(len(batch_text))\n",
    "\n",
    "        # embeddings = enco(batch_text)\n",
    "\n",
    "        # # x_packed = pack_padded_sequence(embeddings, batch_first = True)\n",
    "        \n",
    "        # # packed_seqs = pack_padded_sequence(embeddings, batch_length)\n",
    "        # lstm_output, _ = self.lstm(embeddings, hidden_layer)\n",
    "        # print(lstm_output.shape)\n",
    "        # # lstm_output, _ = pad_packed_sequence(lstm_output)\n",
    "\n",
    "        # # self.relu(lstm_output)\n",
    "        # # lstm_output, op_lengths = pad_packed_sequence(lstm_output, batch_first = True)\n",
    "\n",
    "        # lstm_output = self.dropout_layer(lstm_output)\n",
    "        # print(lstm_output.shape)\n",
    "\n",
    "        emissions = self.generate_emissions(batch_text)\n",
    "        batch_label = batch_label.unsqueeze(1)\n",
    "        # print(logits.shape)\n",
    "        loss = -self.crf_model(emissions, batch_label)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, batch_text):\n",
    "        emissions = self.generate_emissions(batch_text)\n",
    "        # print(logits.shape)\n",
    "        label = self.crf_model.viterbi_decode(emissions)\n",
    "        return label\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.randn(2, 1, self.hidden_dim).to(device), torch.randn(2, 1, self.hidden_dim).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from TorchCRF import CRF\n",
    "\n",
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "#         self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "#         self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "#         self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#         self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "#         # Replace Transformer with BiLSTM\n",
    "#         self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "#                               dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "#         self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "#         self.crf_model = CRF(num_emotions)\n",
    "        \n",
    "\n",
    "#     def generate_emissions(self, audio_encoding, video_encoding, text_encoding):\n",
    "#         # Concatenate or combine the audio, video, and text encodings\n",
    "#         audio_encoding = audio_encoding.float()\n",
    "#         video_encoding = video_encoding.float()\n",
    "#         text_encoding = text_encoding.float()\n",
    "        \n",
    "#         audio_encoding = self.audio_dropout(audio_encoding)\n",
    "#         video_encoding = self.video_dropout(video_encoding)\n",
    "#         text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "#         combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "#         combined_encoding = self.first_linear(combined_encoding)\n",
    "#         combined_encoding = self.relu(combined_encoding)\n",
    "#         combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        \n",
    "#         # Pass through BiLSTM\n",
    "#         lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "#         # Take the output of the BiLSTM\n",
    "#         emotion_logits = self.linear(lstm_output)\n",
    "#         # Apply a softmax layer\n",
    "#         # emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "#         return emotion_logits\n",
    "\n",
    "#     def loss(self, audio_encoding, video_encoding, text_encoding, emotion_labels, padding):\n",
    "\n",
    "#         emissions = self.generate_emissions(audio_encoding, video_encoding, text_encoding)\n",
    "#         emotion_labels = emotion_labels.unsqueeze(1)\n",
    "#         x, y, _ = emissions.shape\n",
    "#         padding = torch.ones((x, y), dtype=torch.bool).to('cuda')\n",
    "#         emotion_labels = emotion_labels.squeeze(1)\n",
    "#         loss = -self.crf_model(emissions, emotion_labels, padding)\n",
    "\n",
    "#         return loss\n",
    "    \n",
    "#     def predict(self, audio_encoding, video_encoding, text_encoding):\n",
    "#         emissions = self.generate_emissions(audio_encoding, video_encoding, text_encoding)\n",
    "#         x, y, _ = emissions.shape\n",
    "#         padding = torch.ones((x, y), dtype=torch.bool).to('cuda')\n",
    "#         label = self.crf_model.viterbi_decode(emissions, padding)\n",
    "#         return label\n",
    "    \n",
    "#     def init_hidden(self):\n",
    "#         return (torch.randn(2, 1, self.hidden_dim).to('cuda'), torch.randn(2, 1, self.hidden_dim).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c6494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "        # Replace Transformer with BiLSTM\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "                              dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "        # Concatenate or combine the audio, video, and text encodings\n",
    "        audio_encoding = audio_encoding.float()\n",
    "        video_encoding = video_encoding.float()\n",
    "        text_encoding = text_encoding.float().squeeze()\n",
    "        \n",
    "        audio_encoding = self.audio_dropout(audio_encoding)\n",
    "        video_encoding = self.video_dropout(video_encoding)\n",
    "        text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "        combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "        combined_encoding = self.first_linear(combined_encoding)\n",
    "        combined_encoding = self.relu(combined_encoding)\n",
    "        combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "        # Take the output of the BiLSTM\n",
    "        emotion_logits = self.linear(lstm_output)\n",
    "        # Apply a softmax layer\n",
    "        emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "        return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:18<00:00,  3.34it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.24it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "Epoch [1/30] Training Loss: 0.01346567102092878\n",
      "Epoch [1/30] Accuracy: 0.16782987108958042\n",
      "Epoch [1/30] Accuracy: 0.5083340175712292\n",
      "VALIDATION METRICS\n",
      "Epoch [1/30] Validation Loss: 0.012885637084643046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.00      0.01       242\n",
      "           1       0.09      1.00      0.16       123\n",
      "           2       0.00      0.00      0.00       143\n",
      "           3       0.00      0.00      0.00       647\n",
      "           4       0.00      0.00      0.00       193\n",
      "           5       0.00      0.00      0.00        45\n",
      "           6       0.00      0.00      0.00        47\n",
      "\n",
      "    accuracy                           0.09      1440\n",
      "   macro avg       0.05      0.14      0.02      1440\n",
      "weighted avg       0.05      0.09      0.01      1440\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.12      0.20       759\n",
      "           1       0.49      0.93      0.64       681\n",
      "\n",
      "    accuracy                           0.50      1440\n",
      "   macro avg       0.57      0.52      0.42      1440\n",
      "weighted avg       0.57      0.50      0.41      1440\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:18<00:00,  3.36it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.46it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "Epoch [2/30] Training Loss: 0.013444891311879795\n",
      "Epoch [2/30] Accuracy: 0.13457590935216357\n",
      "Epoch [2/30] Accuracy: 0.5138352902537154\n",
      "VALIDATION METRICS\n",
      "Epoch [2/30] Validation Loss: 0.012897780537605286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       242\n",
      "           1       0.00      0.00      0.00       123\n",
      "           2       0.10      0.93      0.18       143\n",
      "           3       0.45      0.06      0.10       647\n",
      "           4       0.00      0.00      0.00       193\n",
      "           5       0.00      0.00      0.00        45\n",
      "           6       0.00      0.00      0.00        47\n",
      "\n",
      "    accuracy                           0.12      1440\n",
      "   macro avg       0.08      0.14      0.04      1440\n",
      "weighted avg       0.21      0.12      0.06      1440\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.87      0.67       759\n",
      "           1       0.54      0.16      0.25       681\n",
      "\n",
      "    accuracy                           0.54      1440\n",
      "   macro avg       0.54      0.52      0.46      1440\n",
      "weighted avg       0.54      0.54      0.47      1440\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home2/suyash.mathur/semeval24/task3/original_paper_replication/combined_step1.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/combined_step1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=207'>208</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m===============================\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/combined_step1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=209'>210</a>\u001b[0m     torch\u001b[39m.\u001b[39msave(emotion_model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/tmp/semeval24_task3/final_models/emotion_models/emotion_model_\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/combined_step1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=210'>211</a>\u001b[0m     torch\u001b[39m.\u001b[39;49msave(cause_model\u001b[39m.\u001b[39;49mstate_dict(), \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/tmp/semeval24_task3/final_models/cause_models/cause_model_\u001b[39;49m\u001b[39m{\u001b[39;49;00mepoch\u001b[39m:\u001b[39;49;00m\u001b[39m02\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/combined_step1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=212'>213</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining complete!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/torch/serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 422\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/torch/serialization.py:290\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_like\u001b[39m.\u001b[39;49mwrite_end_of_file()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Define your model\n",
    "# model = EmotionClassifier(input_size=11237, hidden_size=5000, num_layers=2, num_heads=2, dropout=0.2, num_emotions=7)\n",
    "emotion_model = EmotionClassifier(input_size=768+6373+4096, hidden_size=3000, num_layers=3, dropout=0.6, num_emotions=7)\n",
    "cause_model = EmotionClassifier(input_size=768+6373+4096, hidden_size=3000, num_layers=2, dropout=0.6, num_emotions=2)\n",
    "emotion_model.to(\"cuda:1\")\n",
    "cause_model.to(\"cuda:1\")\n",
    "\n",
    "weights_tensor = torch.tensor(np.array(indexer.weights)).to(\"cuda:1\").float()\n",
    "emotion_criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights_tensor,\n",
    "    ignore_index=7\n",
    ")\n",
    "\n",
    "cause_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "num_epochs = 30\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "emotion_optimizer = AdamW(emotion_model.parameters(), lr=0.0001)\n",
    "emotion_lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    emotion_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "cause_optimizer = AdamW(cause_model.parameters(), lr=1e-4)\n",
    "cause_lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    cause_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define training parameters\n",
    "\n",
    "# Training loop\n",
    "for epoch in (range(num_epochs)):\n",
    "    emotion_model.train()  # Set the model to training mode\n",
    "    cause_model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    total_correct_emotions = 0\n",
    "    total_correct_causes = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):  # Assuming you have a DataLoader for your dataset\n",
    "        # Extract data from the batch\n",
    "        audio = batch['audio'].to('cuda:1')\n",
    "        video = batch['video'].to('cuda:1')\n",
    "        text = batch['text'].to('cuda:1')\n",
    "        cause_indices = batch['cause_labels'].to('cuda:1')\n",
    "        \n",
    "        audio_copy = audio.clone().detach()\n",
    "        video_copy = video.clone().detach()\n",
    "        text_copy = text.clone().detach()\n",
    "        \n",
    "        emotion_indices = batch['emotion_labels'].to('cuda:1')\n",
    "        pad_mask = batch['pad_mask'].to('cuda:1')\n",
    "\n",
    "        # Forward pass\n",
    "        emotion_logits = emotion_model(audio_copy, video_copy, text_copy)\n",
    "\n",
    "        # Reshape emotion_logits\n",
    "        emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "        # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "        emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "        # Calculate a mask to exclude padded positions from the loss\n",
    "        pad_mask = pad_mask.view(-1).float()\n",
    "\n",
    "        # Calculate the loss, excluding padded positions\n",
    "        emotion_loss = emotion_criterion(emotion_logits, emotion_indices)\n",
    "        # masked_loss = torch.sum(loss * pad_mask) / torch.sum(pad_mask)\n",
    "        masked_loss = emotion_loss# *pad_mask\n",
    "        # Backpropagation and optimization\n",
    "        \n",
    "        \n",
    "        cause_logits = cause_model(audio, video, text)\n",
    "        cause_logits = cause_logits.view(-1, cause_logits.size(-1))\n",
    "        cause_indices = cause_indices.view(-1)\n",
    "        \n",
    "        cause_loss = cause_criterion(cause_logits, cause_indices)\n",
    "        masked_loss += cause_loss\n",
    "        \n",
    "        emotion_optimizer.zero_grad()\n",
    "        cause_optimizer.zero_grad()\n",
    "        \n",
    "        masked_loss.backward()\n",
    "        \n",
    "        emotion_optimizer.step()\n",
    "        cause_optimizer.step()        \n",
    "\n",
    "        total_loss += masked_loss.item()\n",
    "        total_tokens += torch.sum(pad_mask).item()\n",
    "        \n",
    "        predicted_emotions = torch.argmax(emotion_logits, dim=1)\n",
    "        correct_predictions_emotions = ((predicted_emotions == emotion_indices) * pad_mask).sum().item()\n",
    "\n",
    "        predicted_causes = torch.argmax(cause_logits, dim=1)\n",
    "        correct_predictions_causes = ((predicted_causes == cause_indices) * pad_mask).sum().item()\n",
    "        \n",
    "        total_correct_emotions += correct_predictions_emotions\n",
    "        total_correct_causes += correct_predictions_causes\n",
    "        total_predictions += torch.sum(pad_mask).item()  # Batch size\n",
    "        \n",
    "    \n",
    "    emotion_lr_scheduler.step()\n",
    "    cause_lr_scheduler.step()\n",
    "    \n",
    "    emotion_model.eval()  # Set the model to evaluation mode\n",
    "    cause_model.eval()\n",
    "    \n",
    "    total_val_loss = 0.0\n",
    "    total_val_tokens = 0\n",
    "    total_val_correct_emotions = 0\n",
    "    total_val_correct_causes = 0\n",
    "    total_val_predictions = 0\n",
    "    true_labels_emotion = []\n",
    "    predicted_labels_emotion = []\n",
    "    true_labels_cause = []\n",
    "    predicted_labels_cause = []\n",
    "    padded_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(validation_dataloader):\n",
    "            audio = val_batch['audio'].to('cuda:1')\n",
    "            video = val_batch['video'].to('cuda:1')\n",
    "            text = val_batch['text'].to('cuda:1')\n",
    "            emotion_indices = val_batch['emotion_labels'].to('cuda:1')\n",
    "            cause_indices = val_batch['cause_labels'].to('cuda:1')\n",
    "            pad_mask = val_batch['pad_mask'].to('cuda:1')\n",
    "            \n",
    "            audio_copy = audio.clone().detach()\n",
    "            video_copy = video.clone().detach()\n",
    "            text_copy = text.clone().detach()\n",
    "\n",
    "            emotion_logits = emotion_model(audio_copy, video_copy, text_copy)\n",
    "\n",
    "            # Reshape emotion_logits\n",
    "            emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "            # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "            emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "            pad_mask = pad_mask.view(-1)   \n",
    "\n",
    "            # Calculate the loss, excluding padded positions\n",
    "            val_loss = emotion_criterion(emotion_logits, emotion_indices)\n",
    "            masked_loss = val_loss #torch.sum(val_loss * pad_mask) / torch.sum(pad_mask)\n",
    "            \n",
    "            cause_logits = cause_model(audio, video, text)\n",
    "            cause_logits = cause_logits.view(-1, cause_logits.size(-1))\n",
    "            cause_indices = cause_indices.view(-1)\n",
    "            cause_loss = cause_criterion(cause_logits, cause_indices)\n",
    "            masked_loss += cause_loss\n",
    "            \n",
    "            total_val_loss += masked_loss.item()\n",
    "            total_val_tokens += torch.sum(pad_mask).item()\n",
    "            \n",
    "            predicted_emotions_val = torch.argmax(emotion_logits, dim=1)\n",
    "            correct_predictions_val = ((predicted_emotions_val == emotion_indices) * pad_mask).sum().item()\n",
    "            total_val_correct_emotions += correct_predictions_val\n",
    "            \n",
    "            predicted_causes_val = torch.argmax(cause_logits, dim=1)\n",
    "            correct_predictions_causes_val = ((predicted_causes_val == cause_indices) * pad_mask).sum().item()\n",
    "            total_val_correct_causes += correct_predictions_causes_val\n",
    "            \n",
    "            total_val_predictions += torch.sum(pad_mask).item()\n",
    "\n",
    "            # Store true and predicted labels for F1 score calculation\n",
    "            true_labels_emotion.extend(emotion_indices.cpu().numpy())\n",
    "            predicted_labels_emotion.extend(predicted_emotions_val.cpu().numpy())\n",
    "            \n",
    "            true_labels_cause.extend(cause_indices.cpu().numpy())\n",
    "            predicted_labels_cause.extend(predicted_causes_val.cpu().numpy())\n",
    "            padded_labels.extend(pad_mask.cpu().numpy())\n",
    "\n",
    "    final_true_labels_emotion = [label for label, pad in zip(true_labels_emotion, padded_labels) if pad == 1]\n",
    "    final_predicted_labels_emotion = [label for label, pad in zip(predicted_labels_emotion, padded_labels) if pad == 1]\n",
    "    \n",
    "    final_true_labels_cause = [label for label, pad in zip(true_labels_cause, padded_labels) if pad == 1]\n",
    "    final_predicted_labels_cause = [label for label, pad in zip(predicted_labels_cause, padded_labels) if pad == 1]\n",
    "    \n",
    "    emotion_classification_rep = classification_report(final_true_labels_emotion, final_predicted_labels_emotion)\n",
    "    cause_classification_rep = classification_report(final_true_labels_cause, final_predicted_labels_cause)\n",
    "    \n",
    "    # Calculate and print the average loss for this epoch\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    avg_val_loss = total_val_loss / total_val_tokens\n",
    "    \n",
    "    print(\"===============================\")\n",
    "    print(\"Training data metrics\")\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Training Loss: {avg_loss}\")\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Accuracy: {total_correct_emotions / total_predictions}\")\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Accuracy: {total_correct_causes / total_predictions}\")\n",
    "    \n",
    "    print(\"VALIDATION METRICS\")\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Validation Loss: {avg_val_loss}\")\n",
    "    print(emotion_classification_rep)\n",
    "    print(cause_classification_rep)\n",
    "    print(\"===============================\")\n",
    "\n",
    "    torch.save(emotion_model.state_dict(), f\"/tmp/semeval24_task3/final_models/emotion_models/emotion_model_{epoch:02}.pt\")\n",
    "    torch.save(cause_model.state_dict(), f\"/tmp/semeval24_task3/final_models/cause_models/cause_model_{epoch:02}.pt\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
