{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "from encoder_paths import *\n",
    "import json\n",
    "from pprint import pprint\n",
    "TRAIN_FILE_PATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/training.json\"\n",
    "VALIDATION_FILE_PATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/testing.json\"\n",
    "with open(TRAIN_FILE_PATH) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(VALIDATION_FILE_PATH) as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "pprint(len(train_data))\n",
    "pprint(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f34ad0e6190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "import torch\n",
    "\n",
    "numpy.random.seed(69)\n",
    "random.seed(69)\n",
    "torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger', 'fear', 'neutral', 'joy', 'disgust', 'surprise', 'sadness'}\n"
     ]
    }
   ],
   "source": [
    "all_emotions = set()\n",
    "for conversation in train_data:\n",
    "    conversation = conversation[\"conversation\"]\n",
    "    for utterance in conversation:\n",
    "        all_emotions.add(utterance[\"emotion\"])\n",
    "pprint(all_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionIndexer:\n",
    "    def __init__(self):\n",
    "        self.emotion_to_index = {\n",
    "            'joy': 0,\n",
    "            'sadness': 1,\n",
    "            'anger': 2,\n",
    "            'neutral': 3,\n",
    "            'surprise': 4,\n",
    "            'disgust': 5,\n",
    "            'fear': 6,\n",
    "        }\n",
    "\n",
    "        self.index_to_emotion = {index: emotion for emotion, index in self.emotion_to_index.items()}\n",
    "\n",
    "    def emotion_to_idx(self, emotion):\n",
    "        return self.emotion_to_index.get(emotion, None)\n",
    "\n",
    "    def idx_to_emotion(self, index):\n",
    "        return self.index_to_emotion.get(index, None)\n",
    "\n",
    "# Example usage\n",
    "indexer = EmotionIndexer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        with open(audio_embeddings_path, \"rb\") as f:\n",
    "            self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        audio_embedding = audio_embedding.squeeze()\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    \n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        with open(video_embeddings_path, \"rb\") as f:\n",
    "            self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        # video_name = video_name.split(\".\")[0]\n",
    "        video_embedding = self.video_embeddings[video_name].reshape((16,-1))\n",
    "        video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, json_file, audio_encoder, video_encoder, text_encoder, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = self.load_data(json_file)\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def load_data(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.data[idx]['conversation']\n",
    "        # emotion_labels = [utterance['emotion'] for utterance in conversation]\n",
    "        audio_paths = [utterance['video_name'].replace('mp4', 'wav') for utterance in conversation]\n",
    "        video_paths = [utterance['video_name'] for utterance in conversation]\n",
    "        texts = [utterance['video_name'] for utterance in conversation]\n",
    "\n",
    "        audio_embeddings = [self.audio_encoder.lmao(audio_path) for audio_path in audio_paths]\n",
    "        video_embeddings = [self.video_encoder.lmao(video_path) for video_path in video_paths]\n",
    "        text_embeddings = [self.text_encoder.lmao(text) for text in texts]\n",
    "\n",
    "        cause_pairs = self.data[idx]['emotion-cause_pairs']\n",
    "        useful_utterances = set([int(cause_pair[1]) for cause_pair in cause_pairs])\n",
    "        cause_labels = []\n",
    "        for utterance in conversation:\n",
    "            if utterance['utterance_ID'] in useful_utterances:\n",
    "                cause_labels.append(1)\n",
    "            else:\n",
    "                cause_labels.append(0)\n",
    "        \n",
    "        # Pad or truncate conversations to the maximum sequence length\n",
    "        if len(conversation) < self.max_seq_len:\n",
    "            pad_length = self.max_seq_len - len(conversation)\n",
    "            audio_embeddings += [torch.zeros_like(audio_embeddings[0])] * pad_length\n",
    "            video_embeddings += [torch.zeros_like(video_embeddings[0])] * pad_length\n",
    "            text_embeddings += [torch.zeros_like(text_embeddings[0])] * pad_length\n",
    "            cause_labels += [-1] * pad_length\n",
    "            pad_mask = [1] * len(conversation) + [0] * pad_length\n",
    "        else:\n",
    "            audio_embeddings = audio_embeddings[:self.max_seq_len]\n",
    "            video_embeddings = video_embeddings[:self.max_seq_len]\n",
    "            text_embeddings = text_embeddings[:self.max_seq_len]\n",
    "            cause_labels = cause_labels[:self.max_seq_len]\n",
    "            pad_mask = [1] * self.max_seq_len\n",
    "\n",
    "        audio_embeddings = torch.stack(audio_embeddings)\n",
    "        video_embeddings = torch.stack(video_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        cause_labels = torch.from_numpy(np.array(cause_labels))\n",
    "        pad_mask = torch.from_numpy(np.array(pad_mask))\n",
    "        \n",
    "        return {\n",
    "            'audio': audio_embeddings,\n",
    "            'video': video_embeddings,\n",
    "            'text': text_embeddings,\n",
    "            'cause_labels': cause_labels,\n",
    "            'pad_mask': pad_mask\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "# You need to define your audio, video, and text encoders accordingly\n",
    "\n",
    "# Define your data paths\n",
    "# DATA_DIR = \"/tmp/semeval24_task3\"\n",
    "\n",
    "# AUDIO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/audio_embedding_6373.npy\"\n",
    "# VIDEO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\"\n",
    "# TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "max_seq_len = 35  # Adjust this according to your needs\n",
    "\n",
    "train_dataset = ConversationDataset(TRAIN_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "validation_dataset = ConversationDataset(VALIDATION_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=True)\n",
    "# Example of iterating through batches\n",
    "# for batch in dataloader:\n",
    "#     audio = batch['audio']  # Shape: (batch_size, max_seq_len, audio_embedding_size)\n",
    "#     video = batch['video']  # Shape: (batch_size, max_seq_len, video_embedding_size)\n",
    "#     text = batch['text']    # Shape: (batch_size, max_seq_len, text_embedding_size)\n",
    "#     cause_labels = batch['cause_labels']  # List of emotion labels for each utterance in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout, num_emotions):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "\n",
    "#         self.transformer_encoder = TransformerEncoder(\n",
    "#             TransformerEncoderLayer(hidden_size, num_heads, hidden_size, dropout),\n",
    "#             num_layers\n",
    "#         )\n",
    "        \n",
    "#         self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "#     def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "\n",
    "#         # Concatenate or combine the audio, video, and text encodings here\n",
    "#         # You can use any method like concatenation, addition, or other fusion techniques\n",
    "#         # Combine the encodings (you can customize this part)\n",
    "#         audio_encoding = audio_encoding.float()\n",
    "#         video_encoding = video_encoding.float()\n",
    "#         text_encoding = text_encoding.float().squeeze()\n",
    "#         combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "#         combined_encoding = self.first_linear(combined_encoding)\n",
    "        \n",
    "        \n",
    "#         combined_encoding = combined_encoding.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, input_size)\n",
    "        \n",
    "        \n",
    "#         transformer_output = self.transformer_encoder(combined_encoding)\n",
    "\n",
    "#         # Take the output of the Transformer encoder for the last position as the summary\n",
    "#         emotion_logits = self.linear(transformer_output.permute(1, 0, 2))\n",
    "\n",
    "#         return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_emotions, embedding_dropout=0.2):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        # Dropouts for each modality\n",
    "        self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        # Initial linear layer to process concatenated embeddings\n",
    "        self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Additional linear layer for further processing\n",
    "        self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "\n",
    "        # Final linear layer to predict emotion categories\n",
    "        self.final_linear = nn.Linear(hidden_size, num_emotions, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "        # Concatenate or combine the audio, video, and text encodings\n",
    "        audio_encoding = self.audio_dropout(audio_encoding.float())\n",
    "        video_encoding = self.video_dropout(video_encoding.float())\n",
    "        text_encoding = self.text_dropout(text_encoding.float().squeeze())\n",
    "        \n",
    "        combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "        # Pass the combined encoding through linear layers and activation\n",
    "        combined_encoding = self.first_linear(combined_encoding)\n",
    "        combined_encoding = self.relu(combined_encoding)\n",
    "        combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        combined_encoding = self.relu(combined_encoding) # Apply ReLU before final layer\n",
    "        \n",
    "        # Final linear layer to produce emotion logits\n",
    "        emotion_logits = self.final_linear(combined_encoding)\n",
    "        \n",
    "        # Apply a softmax layer to get probabilities (if necessary depending on your loss function)\n",
    "        emotion_probs = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "        return emotion_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "#         self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "#         self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "#         self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#         self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "#         # Replace Transformer with BiLSTM\n",
    "#         self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "#                               dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "#         self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "#     def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "#         # Concatenate or combine the audio, video, and text encodings\n",
    "#         audio_encoding = audio_encoding.float()\n",
    "#         video_encoding = video_encoding.float()\n",
    "#         text_encoding = text_encoding.float().squeeze()\n",
    "        \n",
    "#         audio_encoding = self.audio_dropout(audio_encoding)\n",
    "#         video_encoding = self.video_dropout(video_encoding)\n",
    "#         text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "#         combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "#         combined_encoding = self.first_linear(combined_encoding)\n",
    "#         combined_encoding = self.relu(combined_encoding)\n",
    "#         combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        \n",
    "#         # Pass through BiLSTM\n",
    "#         lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "#         # Take the output of the BiLSTM\n",
    "#         emotion_logits = self.linear(lstm_output)\n",
    "#         # Apply a softmax layer\n",
    "#         emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "#         return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/semeval24_task3/audio_embeddings/audio_embeddings_facebook_wav2vec2-large-960h.pkl\n",
      "/tmp/semeval24_task3/text_embeddings/text_embeddings_roberta_base_emotion.pkl\n",
      "/tmp/semeval24_task3/video_embeddings/final_embeddings.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 43.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 71.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/40] Training Loss: 0.004366974097890154\n",
      "Epoch [0/40] Validation Loss: 0.004199992534187105\n",
      "Epoch [0/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.82      0.69       759\n",
      "           1       0.65      0.36      0.47       681\n",
      "\n",
      "    accuracy                           0.60      1440\n",
      "   macro avg       0.62      0.59      0.58      1440\n",
      "weighted avg       0.62      0.60      0.58      1440\n",
      "\n",
      "Epoch [0/40] Accuracy: 0.5789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 66.35it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 91.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] Training Loss: 0.0042516655283360285\n",
      "Epoch [1/40] Validation Loss: 0.004101035123070081\n",
      "Epoch [1/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.73      0.67       759\n",
      "           1       0.62      0.49      0.55       681\n",
      "\n",
      "    accuracy                           0.62      1440\n",
      "   macro avg       0.62      0.61      0.61      1440\n",
      "weighted avg       0.62      0.62      0.61      1440\n",
      "\n",
      "Epoch [1/40] Accuracy: 0.6264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 53.86it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 47.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40] Training Loss: 0.004154428417571651\n",
      "Epoch [2/40] Validation Loss: 0.004058618967731794\n",
      "Epoch [2/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.72      0.66       759\n",
      "           1       0.62      0.50      0.55       681\n",
      "\n",
      "    accuracy                           0.62      1440\n",
      "   macro avg       0.62      0.61      0.61      1440\n",
      "weighted avg       0.62      0.62      0.61      1440\n",
      "\n",
      "Epoch [2/40] Accuracy: 0.6367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 52.36it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 84.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/40] Training Loss: 0.0041017073727131715\n",
      "Epoch [3/40] Validation Loss: 0.004016553651955392\n",
      "Epoch [3/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.71      0.67       759\n",
      "           1       0.62      0.53      0.57       681\n",
      "\n",
      "    accuracy                           0.63      1440\n",
      "   macro avg       0.63      0.62      0.62      1440\n",
      "weighted avg       0.63      0.63      0.62      1440\n",
      "\n",
      "Epoch [3/40] Accuracy: 0.6425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 65.39it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 68.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/40] Training Loss: 0.004068811843817883\n",
      "Epoch [4/40] Validation Loss: 0.003998864276541604\n",
      "Epoch [4/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.73      0.67       759\n",
      "           1       0.63      0.52      0.57       681\n",
      "\n",
      "    accuracy                           0.63      1440\n",
      "   macro avg       0.63      0.62      0.62      1440\n",
      "weighted avg       0.63      0.63      0.62      1440\n",
      "\n",
      "Epoch [4/40] Accuracy: 0.6470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 49.07it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 54.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/40] Training Loss: 0.0040428131630350405\n",
      "Epoch [5/40] Validation Loss: 0.00398724770380391\n",
      "Epoch [5/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.70      0.67       759\n",
      "           1       0.63      0.57      0.60       681\n",
      "\n",
      "    accuracy                           0.64      1440\n",
      "   macro avg       0.64      0.63      0.63      1440\n",
      "weighted avg       0.64      0.64      0.64      1440\n",
      "\n",
      "Epoch [5/40] Accuracy: 0.6544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 51.67it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 68.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/40] Training Loss: 0.0040200877947235295\n",
      "Epoch [6/40] Validation Loss: 0.003982146746582455\n",
      "Epoch [6/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       759\n",
      "           1       0.63      0.61      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.64      0.64      0.64      1440\n",
      "weighted avg       0.64      0.65      0.64      1440\n",
      "\n",
      "Epoch [6/40] Accuracy: 0.6577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 54.96it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 67.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/40] Training Loss: 0.004008624342061663\n",
      "Epoch [7/40] Validation Loss: 0.0039620281507571535\n",
      "Epoch [7/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.68       759\n",
      "           1       0.64      0.60      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [7/40] Accuracy: 0.6607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 47.47it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 54.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/40] Training Loss: 0.003998456667655696\n",
      "Epoch [8/40] Validation Loss: 0.0039333818687333\n",
      "Epoch [8/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       759\n",
      "           1       0.64      0.62      0.63       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [8/40] Accuracy: 0.6645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 51.32it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 64.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/40] Training Loss: 0.003985671380811435\n",
      "Epoch [9/40] Validation Loss: 0.0039461595316727955\n",
      "Epoch [9/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.73      0.69       759\n",
      "           1       0.65      0.56      0.60       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.64      0.64      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [9/40] Accuracy: 0.6642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 53.77it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 63.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/40] Training Loss: 0.003966281870940566\n",
      "Epoch [10/40] Validation Loss: 0.003931696837147077\n",
      "Epoch [10/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68       759\n",
      "           1       0.64      0.60      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [10/40] Accuracy: 0.6677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 52.10it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 57.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/40] Training Loss: 0.0039546052089553035\n",
      "Epoch [11/40] Validation Loss: 0.00390959953268369\n",
      "Epoch [11/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.71      0.68       759\n",
      "           1       0.65      0.59      0.62       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [11/40] Accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 46.49it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 62.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/40] Training Loss: 0.003964776744767357\n",
      "Epoch [12/40] Validation Loss: 0.003919091241227256\n",
      "Epoch [12/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       759\n",
      "           1       0.64      0.64      0.64       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [12/40] Accuracy: 0.6691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 53.37it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 66.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/40] Training Loss: 0.003947341395125267\n",
      "Epoch [13/40] Validation Loss: 0.0039165453364451725\n",
      "Epoch [13/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.73      0.69       759\n",
      "           1       0.65      0.57      0.61       681\n",
      "\n",
      "    accuracy                           0.65      1440\n",
      "   macro avg       0.65      0.65      0.65      1440\n",
      "weighted avg       0.65      0.65      0.65      1440\n",
      "\n",
      "Epoch [13/40] Accuracy: 0.6689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 53.79it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 72.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/40] Training Loss: 0.003930187698107105\n",
      "Epoch [14/40] Validation Loss: 0.0039060329811440573\n",
      "Epoch [14/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       759\n",
      "           1       0.65      0.62      0.63       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [14/40] Accuracy: 0.6761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 46.95it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 52.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/40] Training Loss: 0.003931892002360047\n",
      "Epoch [15/40] Validation Loss: 0.003919329949551158\n",
      "Epoch [15/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69       759\n",
      "           1       0.65      0.61      0.63       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [15/40] Accuracy: 0.6726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 51.33it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 66.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/40] Training Loss: 0.003905501171074161\n",
      "Epoch [16/40] Validation Loss: 0.0038984738290309905\n",
      "Epoch [16/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.70      0.68       759\n",
      "           1       0.65      0.62      0.63       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [16/40] Accuracy: 0.6763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 46.54it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 45.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/40] Training Loss: 0.003915724182512599\n",
      "Epoch [17/40] Validation Loss: 0.0039008898039658863\n",
      "Epoch [17/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       759\n",
      "           1       0.65      0.64      0.64       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [17/40] Accuracy: 0.6794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 49.53it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 61.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/40] Training Loss: 0.0038952475913943832\n",
      "Epoch [18/40] Validation Loss: 0.003923797110716502\n",
      "Epoch [18/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.74      0.70       759\n",
      "           1       0.66      0.57      0.61       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.65      0.65      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [18/40] Accuracy: 0.6810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 51.32it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 63.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/40] Training Loss: 0.0038922160841853115\n",
      "Epoch [19/40] Validation Loss: 0.003892702360947927\n",
      "Epoch [19/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69       759\n",
      "           1       0.66      0.61      0.63       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.66      0.66      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [19/40] Accuracy: 0.6829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 51.83it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 64.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/40] Training Loss: 0.0038761969504954243\n",
      "Epoch [20/40] Validation Loss: 0.003892007263170348\n",
      "Epoch [20/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       759\n",
      "           1       0.65      0.63      0.64       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [20/40] Accuracy: 0.6846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 45.51it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 48.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/40] Training Loss: 0.0038685870196042325\n",
      "Epoch [21/40] Validation Loss: 0.0038965602301888995\n",
      "Epoch [21/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.68       759\n",
      "           1       0.65      0.64      0.64       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [21/40] Accuracy: 0.6854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 48.51it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 63.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/40] Training Loss: 0.0038672871254516045\n",
      "Epoch [22/40] Validation Loss: 0.0038756190488735834\n",
      "Epoch [22/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       759\n",
      "           1       0.65      0.63      0.64       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.66      0.66      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [22/40] Accuracy: 0.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 47.08it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 50.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/40] Training Loss: 0.003864194555170617\n",
      "Epoch [23/40] Validation Loss: 0.0038847852912214068\n",
      "Epoch [23/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       759\n",
      "           1       0.65      0.64      0.64       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [23/40] Accuracy: 0.6899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 45.97it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 63.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/40] Training Loss: 0.0038541504849118253\n",
      "Epoch [24/40] Validation Loss: 0.0038808152079582215\n",
      "Epoch [24/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.69       759\n",
      "           1       0.66      0.60      0.63       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [24/40] Accuracy: 0.6902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 52.11it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 65.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/40] Training Loss: 0.0038440487040059205\n",
      "Epoch [25/40] Validation Loss: 0.0038847712179025016\n",
      "Epoch [25/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69       759\n",
      "           1       0.65      0.62      0.63       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [25/40] Accuracy: 0.6955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 50.87it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 60.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/40] Training Loss: 0.0038288989690775472\n",
      "Epoch [26/40] Validation Loss: 0.0038917617665396795\n",
      "Epoch [26/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       759\n",
      "           1       0.64      0.66      0.65       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [26/40] Accuracy: 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 50.01it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 62.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/40] Training Loss: 0.003825660903560126\n",
      "Epoch [27/40] Validation Loss: 0.0038787328120734954\n",
      "Epoch [27/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       759\n",
      "           1       0.65      0.65      0.65       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [27/40] Accuracy: 0.6937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 51.09it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 60.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/40] Training Loss: 0.0038173168552598203\n",
      "Epoch [28/40] Validation Loss: 0.00386580771042241\n",
      "Epoch [28/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.68       759\n",
      "           1       0.65      0.64      0.64       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [28/40] Accuracy: 0.6980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 52.45it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 53.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/40] Training Loss: 0.0038197078584636095\n",
      "Epoch [29/40] Validation Loss: 0.0038898016015688577\n",
      "Epoch [29/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.69       759\n",
      "           1       0.66      0.60      0.63       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [29/40] Accuracy: 0.6992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 46.28it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 51.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/40] Training Loss: 0.0038020668510185185\n",
      "Epoch [30/40] Validation Loss: 0.0039040498435497285\n",
      "Epoch [30/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       759\n",
      "           1       0.65      0.63      0.64       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [30/40] Accuracy: 0.7026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 46.43it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 65.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/40] Training Loss: 0.003798808064089981\n",
      "Epoch [31/40] Validation Loss: 0.0038511287420988083\n",
      "Epoch [31/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       759\n",
      "           1       0.65      0.64      0.64       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [31/40] Accuracy: 0.7016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 52.24it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 64.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/40] Training Loss: 0.0037790827880754135\n",
      "Epoch [32/40] Validation Loss: 0.00387099865410063\n",
      "Epoch [32/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       759\n",
      "           1       0.66      0.63      0.64       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [32/40] Accuracy: 0.7033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 52.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 58.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/40] Training Loss: 0.0037831539482854645\n",
      "Epoch [33/40] Validation Loss: 0.0038958398004372914\n",
      "Epoch [33/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       759\n",
      "           1       0.64      0.66      0.65       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [33/40] Accuracy: 0.7019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 46.45it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 53.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/40] Training Loss: 0.003771568374373817\n",
      "Epoch [34/40] Validation Loss: 0.003897696319553587\n",
      "Epoch [34/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       759\n",
      "           1       0.64      0.66      0.65       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [34/40] Accuracy: 0.7052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 44.83it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 64.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/40] Training Loss: 0.0037771959895619386\n",
      "Epoch [35/40] Validation Loss: 0.0038729437523418\n",
      "Epoch [35/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.69       759\n",
      "           1       0.66      0.61      0.63       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.66      0.66      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [35/40] Accuracy: 0.7093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 49.98it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 64.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/40] Training Loss: 0.003769056767075997\n",
      "Epoch [36/40] Validation Loss: 0.0038753732211059995\n",
      "Epoch [36/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.69       759\n",
      "           1       0.66      0.60      0.63       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.67      0.66      1440\n",
      "\n",
      "Epoch [36/40] Accuracy: 0.7083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 46.06it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 55.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/40] Training Loss: 0.0037645556791847255\n",
      "Epoch [37/40] Validation Loss: 0.0038686125642723506\n",
      "Epoch [37/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       759\n",
      "           1       0.65      0.67      0.66       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [37/40] Accuracy: 0.7090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 48.57it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 64.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/40] Training Loss: 0.0037569493796792324\n",
      "Epoch [38/40] Validation Loss: 0.0038608503838380177\n",
      "Epoch [38/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69       759\n",
      "           1       0.66      0.61      0.63       681\n",
      "\n",
      "    accuracy                           0.66      1440\n",
      "   macro avg       0.66      0.66      0.66      1440\n",
      "weighted avg       0.66      0.66      0.66      1440\n",
      "\n",
      "Epoch [38/40] Accuracy: 0.7157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 52.44it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 69.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/40] Training Loss: 0.003741105526418989\n",
      "Epoch [39/40] Validation Loss: 0.003873353824019432\n",
      "Epoch [39/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       759\n",
      "           1       0.66      0.63      0.64       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "Epoch [39/40] Accuracy: 0.7129\n",
      "Training complete!\n",
      "=======================================\n",
      "BEST MODEL\n",
      "Best epoch: 31\n",
      "Best validation loss: 0.0038511287420988083\n",
      "Best classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       759\n",
      "           1       0.65      0.64      0.64       681\n",
      "\n",
      "    accuracy                           0.67      1440\n",
      "   macro avg       0.67      0.67      0.67      1440\n",
      "weighted avg       0.67      0.67      0.67      1440\n",
      "\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Define your model\n",
    "model = EmotionClassifier(input_size=768*2+1024, hidden_size=2000, num_emotions=2)\n",
    "model.to(\"cuda:1\")\n",
    "\n",
    "num_epochs = 40\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = AdamW(model.parameters(), lr=0.5*1e-5)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Define training parameters\n",
    "\n",
    "# Training loop\n",
    "best_model_file = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "best_classification_report = None\n",
    "\n",
    "print(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "print(TEXT_EMBEDDINGS_FILEPATH)\n",
    "print(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "for epoch in (range(num_epochs)):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):  # Assuming you have a DataLoader for your dataset\n",
    "        # Extract data from the batch\n",
    "        audio = batch['audio'].to('cuda:1')\n",
    "        video = batch['video'].to('cuda:1')\n",
    "        text = batch['text'].to('cuda:1')\n",
    "        emotion_indices = batch['cause_labels'].to('cuda:1')\n",
    "        pad_mask = batch['pad_mask'].to('cuda:1')\n",
    "        \n",
    "\n",
    "        # Forward pass\n",
    "        emotion_logits = model(audio, video, text)\n",
    "\n",
    "        # Reshape emotion_logits\n",
    "        emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "        # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "        emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "        # Calculate a mask to exclude padded positions from the loss\n",
    "        pad_mask = pad_mask.view(-1)     \n",
    "\n",
    "        # Calculate the loss, excluding padded positions\n",
    "        loss = criterion(emotion_logits, emotion_indices)\n",
    "        # masked_loss = torch.sum(loss * pad_mask) / torch.sum(pad_mask)\n",
    "        masked_loss = loss\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        masked_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += masked_loss.item()\n",
    "        total_tokens += torch.sum(pad_mask).item()\n",
    "        \n",
    "        predicted_emotions = torch.argmax(emotion_logits, dim=1)\n",
    "        correct_predictions = ((predicted_emotions == emotion_indices) * pad_mask).sum().item()\n",
    "\n",
    "        total_correct += correct_predictions\n",
    "        total_predictions += torch.sum(pad_mask).item()  # Batch size\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    total_val_tokens = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_predictions = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    padded_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(validation_dataloader):\n",
    "            audio = val_batch['audio'].to('cuda:1')\n",
    "            video = val_batch['video'].to('cuda:1')\n",
    "            text = val_batch['text'].to('cuda:1')\n",
    "            emotion_indices = val_batch['cause_labels'].to('cuda:1')\n",
    "            pad_mask = val_batch['pad_mask'].to('cuda:1')\n",
    "\n",
    "            emotion_logits = model(audio, video, text)\n",
    "\n",
    "            # Reshape emotion_logits\n",
    "            emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "            # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "            emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "            pad_mask = pad_mask.view(-1)   \n",
    "\n",
    "            # Calculate the loss, excluding padded positions\n",
    "            val_loss = criterion(emotion_logits, emotion_indices)\n",
    "            masked_loss = torch.sum(val_loss * pad_mask) / torch.sum(pad_mask)\n",
    "            \n",
    "            total_val_loss += masked_loss.item()\n",
    "            total_val_tokens += torch.sum(pad_mask).item()\n",
    "            \n",
    "            predicted_emotions_val = torch.argmax(emotion_logits, dim=1)\n",
    "            correct_predictions_val = ((predicted_emotions_val == emotion_indices) * pad_mask).sum().item()\n",
    "            total_val_correct += correct_predictions_val\n",
    "            total_val_predictions += torch.sum(pad_mask).item()\n",
    "\n",
    "            # Store true and predicted labels for F1 score calculation\n",
    "            true_labels.extend(emotion_indices.cpu().numpy())\n",
    "            predicted_labels.extend(predicted_emotions_val.cpu().numpy())\n",
    "            padded_labels.extend(pad_mask.cpu().numpy())\n",
    "\n",
    "    final_true_labels = [label for label, pad in zip(true_labels, padded_labels) if pad == 1]\n",
    "    final_predicted_labels = [label for label, pad in zip(predicted_labels, padded_labels) if pad == 1]\n",
    "    classification_rep = classification_report(final_true_labels, final_predicted_labels)\n",
    "\n",
    "    # Calculate and print the average loss for this epoch\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    avg_val_loss = total_val_loss / total_val_tokens\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Training Loss: {avg_loss}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Validation Loss: {avg_val_loss}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Classification Report:\\n{classification_rep}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Accuracy: {total_correct / total_predictions:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        best_classification_report = classification_rep\n",
    "        best_model_file = f\"/tmp/semeval24_task3/baseline_models/cause_models/best_cause_model.pt\"\n",
    "        torch.save(model.state_dict(), best_model_file)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"/tmp/semeval24_task3/baseline_models/cause_models/cause_model_{epoch:02}.pt\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(\"=======================================\")\n",
    "print(\"BEST MODEL\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Best validation loss: {best_val_loss}\")\n",
    "print(f\"Best classification report:\\n{best_classification_report}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
