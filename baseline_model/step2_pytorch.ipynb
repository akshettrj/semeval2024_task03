{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba951e4-a7d5-4f21-8fb8-7918de7bd983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "from encoder_paths import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2a0059-825e-4bc6-affd-6b1eb34e4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# import random\n",
    "# import torch\n",
    "\n",
    "# numpy.random.seed(69)\n",
    "# random.seed(69)\n",
    "# torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2806718a-4a8a-473c-9ea8-99d1d03768b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "BASE_DIR = \"/tmp/akshett.jindal\"\n",
    "HUGGINGFACE_CACHE_DIR = os.path.join(BASE_DIR, \".huggingface_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b524940c-cccb-46c7-b285-a5210aa69bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# DATA_DIR = \"/tmp/semeval24_task3\"\n",
    "\n",
    "# TRAIN_DATA_FILEPATH = os.path.join(DATA_DIR, \"final_clean_data\", \"train\", \"Subtask_2.json\")\n",
    "# VAL_DATA_FILEPATH = os.path.join(DATA_DIR, \"final_clean_data\", \"val\", \"Subtask_2.json\")\n",
    "\n",
    "TRAIN_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/training.json\"\n",
    "VAL_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/testing.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86071165-de8e-4a02-91c2-66165d6d65c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        with open(audio_embeddings_path, \"rb\") as f:\n",
    "            self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        audio_embedding = audio_embedding.squeeze()\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    \n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        with open(video_embeddings_path, \"rb\") as f:\n",
    "            self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        # video_name = video_name.split(\".\")[0]\n",
    "        video_embedding = self.video_embeddings[video_name].reshape((16,-1))\n",
    "        video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544e6927-b52c-4edf-b7e4-bd9285d35a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmotionCauseDataset(Dataset):\n",
    "    def __init__(self, file_path, audio_encoder, video_encoder, text_encoder, neg_to_pos_ratio):\n",
    "        with open(file_path) as f:\n",
    "            self.file_data = json.load(f)\n",
    "\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.data = []\n",
    "        self.POSITIVE_SAMPLE_COUNT = 0\n",
    "        self.NEGATIVE_SAMPLE_COUNT = 0\n",
    "\n",
    "        for conversation in self.file_data:\n",
    "            positive_samples = []\n",
    "            negative_samples = []\n",
    "\n",
    "            utterances = {\n",
    "                utterance[\"utterance_ID\"]: utterance\n",
    "                for utterance in conversation[\"conversation\"]\n",
    "            }\n",
    "            utterance_ids = set(utterances.keys())\n",
    "\n",
    "            causes = {\n",
    "                utterance_id: []\n",
    "                for utterance_id in utterance_ids\n",
    "            }\n",
    "\n",
    "            for emo_cause_pairs in conversation[\"emotion-cause_pairs\"]:\n",
    "                emotion_utterance_num, emotion = emo_cause_pairs[0].split(\"_\")\n",
    "                emotion_utterance_num = int(emotion_utterance_num)\n",
    "                cause_utterance_num = int(emo_cause_pairs[1])\n",
    "\n",
    "                if emotion != \"neutral\":\n",
    "                    causes[emotion_utterance_num].append(cause_utterance_num)\n",
    "\n",
    "            for emo_utterance_id in utterance_ids:\n",
    "                emo_utterance = utterances[emo_utterance_id]\n",
    "\n",
    "                if utterances[emo_utterance_id][\"emotion\"] == \"neutral\":\n",
    "                    continue\n",
    "\n",
    "                for cause_utterance_id in utterance_ids:\n",
    "                    cause_utterance = utterances[cause_utterance_id]\n",
    "\n",
    "                    is_cause = cause_utterance_id in causes[emo_utterance_id]\n",
    "\n",
    "                    data_point = {\n",
    "                        \"original_utterance\": {\n",
    "                            \"id\": emo_utterance_id,\n",
    "                            \"text\": emo_utterance[\"text\"],\n",
    "                            \"video_name\": emo_utterance[\"video_name\"],\n",
    "                        },\n",
    "                        \"cause_utterance\": {\n",
    "                            \"id\": cause_utterance_id,\n",
    "                            \"text\": cause_utterance[\"text\"],\n",
    "                            \"video_name\": cause_utterance[\"video_name\"],\n",
    "                        },\n",
    "                        \"is_cause\": is_cause,\n",
    "                    }\n",
    "\n",
    "                    if is_cause:\n",
    "                        positive_samples.append(data_point)\n",
    "                        self.POSITIVE_SAMPLE_COUNT += 1\n",
    "                    else:\n",
    "                        negative_samples.append(data_point)\n",
    "                        self.NEGATIVE_SAMPLE_COUNT += 1\n",
    "\n",
    "            random.shuffle(negative_samples)\n",
    "\n",
    "            self.data.extend(positive_samples)\n",
    "            if neg_to_pos_ratio is not None:\n",
    "                self.data.extend(negative_samples[:min(neg_to_pos_ratio * len(positive_samples), len(negative_samples))])\n",
    "            else:\n",
    "                self.data.extend(negative_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "\n",
    "        orig_utt = data[\"original_utterance\"]\n",
    "        cause_utt = data[\"cause_utterance\"]\n",
    "\n",
    "        orig_id = orig_utt[\"id\"]\n",
    "        orig_text = orig_utt[\"text\"]\n",
    "        orig_video = orig_utt[\"video_name\"]\n",
    "        orig_audio = orig_utt[\"video_name\"].replace(\".mp4\", \".wav\")\n",
    "\n",
    "        cause_id = cause_utt[\"id\"]\n",
    "        cause_text = cause_utt[\"text\"]\n",
    "        cause_video = cause_utt[\"video_name\"]\n",
    "        cause_audio = cause_utt[\"video_name\"].replace(\".mp4\", \".wav\")\n",
    "\n",
    "        is_cause = data[\"is_cause\"]\n",
    "\n",
    "        return {\n",
    "            \"distance\": abs(orig_id - cause_id),\n",
    "            \"original_audio\": self.audio_encoder.lmao(orig_audio).float(),\n",
    "            \"original_video\": self.video_encoder.lmao(orig_video).float(),\n",
    "            \"original_text\": self.text_encoder.lmao(orig_video).squeeze().float(),\n",
    "            \"cause_audio\": self.audio_encoder.lmao(cause_audio).float(),\n",
    "            \"cause_video\": self.video_encoder.lmao(cause_video).float(),\n",
    "            \"cause_text\": self.text_encoder.lmao(cause_video).squeeze().float(),\n",
    "            \"is_cause\": 1.0 if is_cause else 0.0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246a8dd9-1de3-4c4b-b1b2-3089cfd6a653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49835, 11047)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# AUDIO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/audio_embedding_6373.npy\"\n",
    "# VIDEO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\"\n",
    "# TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "\n",
    "trn_dataset = EmotionCauseDataset(\n",
    "    TRAIN_DATA_FILEPATH,\n",
    "    audio_encoder,\n",
    "    video_encoder,\n",
    "    text_encoder,\n",
    "    neg_to_pos_ratio=5\n",
    ")\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = EmotionCauseDataset(\n",
    "    VAL_DATA_FILEPATH,\n",
    "    audio_encoder,\n",
    "    video_encoder,\n",
    "    text_encoder,\n",
    "    neg_to_pos_ratio=None,\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "len(trn_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae58a04-a8b4-444c-a76d-e7a3ac6fe11e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_positional_embeddings(dimension, count):\n",
    "    embeddings = [list(np.zeros(dimension))]\n",
    "    embeddings.extend([\n",
    "        list(np.random.normal(loc=0.0, scale=0.1, size=dimension)) for _ in range(count)\n",
    "    ])\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5243cb81-825d-4146-8db0-46ed822dd34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionCauseDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        utterance_embedding_size,\n",
    "        device,\n",
    "        hidden_dimension=4096,\n",
    "        positional_embeddings_dimension=200,\n",
    "        dropout=0.2,\n",
    "        *args, **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "        positional_embeddings = generate_positional_embeddings(positional_embeddings_dimension, 200)\n",
    "        self.positional_embeddings = torch.from_numpy(positional_embeddings).to(device).float()\n",
    "        \n",
    "        self.non_neutral_dropout = nn.Dropout(dropout)\n",
    "        self.candidate_cause_dropout = nn.Dropout(dropout)\n",
    "        self.distance_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(utterance_embedding_size*2 + positional_embeddings_dimension, hidden_dimension)\n",
    "        self.linear1_activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dimension, 1)\n",
    "\n",
    "    def forward(self, non_neutral_utterances, candidate_cause_utterances, distances):\n",
    "        positional_embedding = self.positional_embeddings[distances]\n",
    "        \n",
    "        non_neutral_utterances = self.non_neutral_dropout(non_neutral_utterances)\n",
    "        candidate_cause_utterances = self.candidate_cause_dropout(candidate_cause_utterances)\n",
    "        positional_embedding = self.distance_dropout(positional_embedding)\n",
    "\n",
    "        embeddings = torch.concat((non_neutral_utterances, candidate_cause_utterances, positional_embedding), axis=1)\n",
    "\n",
    "        return self.linear2(\n",
    "            self.linear1_activation(\n",
    "                self.linear1(embeddings)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f41eb7cc-c5c2-4709-bb9f-3e0ba28a8616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "def save_model(epoch_num):\n",
    "    torch.save(model.state_dict(), f\"/tmp/semeval24_task3/baseline_models/pairing_models/paring_model_{epoch_num:02}.pt\")\n",
    "    numpy.save(\n",
    "        f\"/tmp/semeval24_task3/baseline_models/pairing_models/pairing_model_pos_embeds_{epoch_num:02}.npy\",\n",
    "        model.positional_embeddings.cpu().numpy(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63eaef23-10f5-4068-b853-7f2d9a393201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/semeval24_task3/audio_embeddings/audio_embeddings_facebook_wav2vec2-large-960h.pkl\n",
      "/tmp/semeval24_task3/video_embeddings/final_embeddings.pkl\n",
      "/tmp/semeval24_task3/text_embeddings/text_embeddings_roberta_base_emotion.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▎         | 1/40 [00:15<09:47, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.71      0.82     10056\n",
      "         1.0       0.22      0.84      0.35       991\n",
      "\n",
      "    accuracy                           0.72     11047\n",
      "   macro avg       0.60      0.77      0.58     11047\n",
      "weighted avg       0.91      0.72      0.78     11047\n",
      "\n",
      "Epoch [01/40] Train Loss: 0.021651680268703545\n",
      "Epoch [01/40] Validation Loss: 0.015877737575916106\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [02/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.68      0.80     10056\n",
      "         1.0       0.22      0.90      0.35       991\n",
      "\n",
      "    accuracy                           0.70     11047\n",
      "   macro avg       0.60      0.79      0.58     11047\n",
      "weighted avg       0.92      0.70      0.76     11047\n",
      "\n",
      "Epoch [02/40] Train Loss: 0.018089095583156665\n",
      "Epoch [02/40] Validation Loss: 0.014999469486214452\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   8%|▊         | 3/40 [00:40<08:08, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [03/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.73      0.84     10056\n",
      "         1.0       0.25      0.91      0.39       991\n",
      "\n",
      "    accuracy                           0.74     11047\n",
      "   macro avg       0.62      0.82      0.61     11047\n",
      "weighted avg       0.92      0.74      0.80     11047\n",
      "\n",
      "Epoch [03/40] Train Loss: 0.016311299078519494\n",
      "Epoch [03/40] Validation Loss: 0.01376458428342945\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 4/40 [00:51<07:15, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [04/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.78      0.87     10056\n",
      "         1.0       0.28      0.89      0.43       991\n",
      "\n",
      "    accuracy                           0.79     11047\n",
      "   macro avg       0.63      0.83      0.65     11047\n",
      "weighted avg       0.92      0.79      0.83     11047\n",
      "\n",
      "Epoch [04/40] Train Loss: 0.015288858616885557\n",
      "Epoch [04/40] Validation Loss: 0.012685060016938721\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  12%|█▎        | 5/40 [01:01<06:39, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [05/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.78      0.87     10056\n",
      "         1.0       0.29      0.89      0.43       991\n",
      "\n",
      "    accuracy                           0.79     11047\n",
      "   macro avg       0.64      0.84      0.65     11047\n",
      "weighted avg       0.92      0.79      0.83     11047\n",
      "\n",
      "Epoch [05/40] Train Loss: 0.014646341082452266\n",
      "Epoch [05/40] Validation Loss: 0.01247019187363717\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  15%|█▌        | 6/40 [01:11<06:13, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [06/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.78      0.87     10056\n",
      "         1.0       0.28      0.88      0.43       991\n",
      "\n",
      "    accuracy                           0.79     11047\n",
      "   macro avg       0.63      0.83      0.65     11047\n",
      "weighted avg       0.92      0.79      0.83     11047\n",
      "\n",
      "Epoch [06/40] Train Loss: 0.014108756681197947\n",
      "Epoch [06/40] Validation Loss: 0.012277895417753568\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  18%|█▊        | 7/40 [01:21<05:56, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [07/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.79      0.88     10056\n",
      "         1.0       0.29      0.89      0.44       991\n",
      "\n",
      "    accuracy                           0.80     11047\n",
      "   macro avg       0.64      0.84      0.66     11047\n",
      "weighted avg       0.92      0.80      0.84     11047\n",
      "\n",
      "Epoch [07/40] Train Loss: 0.01369272637314633\n",
      "Epoch [07/40] Validation Loss: 0.012027130482783964\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [08/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.83      0.90     10056\n",
      "         1.0       0.34      0.85      0.48       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.84      0.69     11047\n",
      "weighted avg       0.92      0.84      0.86     11047\n",
      "\n",
      "Epoch [08/40] Train Loss: 0.013382758799351786\n",
      "Epoch [08/40] Validation Loss: 0.011020403686284175\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 8/40 [01:32<05:38, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [09/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.83      0.90     10056\n",
      "         1.0       0.33      0.85      0.48       991\n",
      "\n",
      "    accuracy                           0.83     11047\n",
      "   macro avg       0.66      0.84      0.69     11047\n",
      "weighted avg       0.92      0.83      0.86     11047\n",
      "\n",
      "Epoch [09/40] Train Loss: 0.013059278399625961\n",
      "Epoch [09/40] Validation Loss: 0.010949552919571079\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 10/40 [01:52<05:10, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.88     10056\n",
      "         1.0       0.30      0.88      0.45       991\n",
      "\n",
      "    accuracy                           0.81     11047\n",
      "   macro avg       0.64      0.84      0.67     11047\n",
      "weighted avg       0.92      0.81      0.84     11047\n",
      "\n",
      "Epoch [10/40] Train Loss: 0.01275242424943678\n",
      "Epoch [10/40] Validation Loss: 0.011565114667501803\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  28%|██▊       | 11/40 [02:00<04:42,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.36      0.84      0.50       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.85      0.71     11047\n",
      "weighted avg       0.93      0.85      0.87     11047\n",
      "\n",
      "Epoch [11/40] Train Loss: 0.012545865441388812\n",
      "Epoch [11/40] Validation Loss: 0.010575315716605242\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 12/40 [02:09<04:21,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.81      0.89     10056\n",
      "         1.0       0.31      0.87      0.46       991\n",
      "\n",
      "    accuracy                           0.82     11047\n",
      "   macro avg       0.65      0.84      0.68     11047\n",
      "weighted avg       0.92      0.82      0.85     11047\n",
      "\n",
      "Epoch [12/40] Train Loss: 0.012295713358306214\n",
      "Epoch [12/40] Validation Loss: 0.011298057640373548\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  32%|███▎      | 13/40 [02:17<04:05,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.90     10056\n",
      "         1.0       0.34      0.85      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.84      0.70     11047\n",
      "weighted avg       0.93      0.84      0.87     11047\n",
      "\n",
      "Epoch [13/40] Train Loss: 0.012139150444900125\n",
      "Epoch [13/40] Validation Loss: 0.010604929536890535\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.83      0.90     10056\n",
      "         1.0       0.33      0.86      0.47       991\n",
      "\n",
      "    accuracy                           0.83     11047\n",
      "   macro avg       0.66      0.84      0.69     11047\n",
      "weighted avg       0.92      0.83      0.86     11047\n",
      "\n",
      "Epoch [14/40] Train Loss: 0.011912886149498902\n",
      "Epoch [14/40] Validation Loss: 0.01095061965719033\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  38%|███▊      | 15/40 [02:34<03:40,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.91     10056\n",
      "         1.0       0.34      0.85      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.84      0.70     11047\n",
      "weighted avg       0.93      0.84      0.87     11047\n",
      "\n",
      "Epoch [15/40] Train Loss: 0.011768796210837044\n",
      "Epoch [15/40] Validation Loss: 0.010594772453051028\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 16/40 [02:43<03:29,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.91     10056\n",
      "         1.0       0.34      0.85      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.85      0.70     11047\n",
      "weighted avg       0.93      0.84      0.87     11047\n",
      "\n",
      "Epoch [16/40] Train Loss: 0.01150064641292969\n",
      "Epoch [16/40] Validation Loss: 0.010647147172699305\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.82      0.90     10056\n",
      "         1.0       0.33      0.86      0.47       991\n",
      "\n",
      "    accuracy                           0.83     11047\n",
      "   macro avg       0.65      0.84      0.68     11047\n",
      "weighted avg       0.92      0.83      0.86     11047\n",
      "\n",
      "Epoch [17/40] Train Loss: 0.011410596947168736\n",
      "Epoch [17/40] Validation Loss: 0.010886248196499313\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  45%|████▌     | 18/40 [03:00<03:10,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.82      0.89     10056\n",
      "         1.0       0.32      0.86      0.47       991\n",
      "\n",
      "    accuracy                           0.82     11047\n",
      "   macro avg       0.65      0.84      0.68     11047\n",
      "weighted avg       0.92      0.82      0.86     11047\n",
      "\n",
      "Epoch [18/40] Train Loss: 0.011266318549847604\n",
      "Epoch [18/40] Validation Loss: 0.011048182742504901\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  48%|████▊     | 19/40 [03:08<02:59,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.90     10056\n",
      "         1.0       0.34      0.86      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.85      0.70     11047\n",
      "weighted avg       0.93      0.84      0.87     11047\n",
      "\n",
      "Epoch [19/40] Train Loss: 0.011150978081671968\n",
      "Epoch [19/40] Validation Loss: 0.010601210717323607\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 20/40 [03:17<02:50,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.82      0.90     10056\n",
      "         1.0       0.33      0.86      0.47       991\n",
      "\n",
      "    accuracy                           0.83     11047\n",
      "   macro avg       0.66      0.84      0.69     11047\n",
      "weighted avg       0.93      0.83      0.86     11047\n",
      "\n",
      "Epoch [20/40] Train Loss: 0.010927748677040909\n",
      "Epoch [20/40] Validation Loss: 0.010786898023076103\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.35      0.84      0.50       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.84      0.70     11047\n",
      "weighted avg       0.93      0.85      0.87     11047\n",
      "\n",
      "Epoch [21/40] Train Loss: 0.01080382408705325\n",
      "Epoch [21/40] Validation Loss: 0.010369905621561454\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  55%|█████▌    | 22/40 [03:34<02:34,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.92     10056\n",
      "         1.0       0.37      0.82      0.51       991\n",
      "\n",
      "    accuracy                           0.86     11047\n",
      "   macro avg       0.67      0.84      0.71     11047\n",
      "weighted avg       0.93      0.86      0.88     11047\n",
      "\n",
      "Epoch [22/40] Train Loss: 0.010732314066788186\n",
      "Epoch [22/40] Validation Loss: 0.010156467143503201\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.82      0.89     10056\n",
      "         1.0       0.32      0.86      0.47       991\n",
      "\n",
      "    accuracy                           0.82     11047\n",
      "   macro avg       0.65      0.84      0.68     11047\n",
      "weighted avg       0.92      0.82      0.86     11047\n",
      "\n",
      "Epoch [23/40] Train Loss: 0.010488654451610386\n",
      "Epoch [23/40] Validation Loss: 0.010989646931669095\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 24/40 [03:51<02:17,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.90     10056\n",
      "         1.0       0.34      0.85      0.48       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.84      0.69     11047\n",
      "weighted avg       0.92      0.84      0.87     11047\n",
      "\n",
      "Epoch [24/40] Train Loss: 0.01038616621104312\n",
      "Epoch [24/40] Validation Loss: 0.010676818901219055\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  62%|██████▎   | 25/40 [04:00<02:08,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.35      0.84      0.49       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.84      0.70     11047\n",
      "weighted avg       0.92      0.85      0.87     11047\n",
      "\n",
      "Epoch [25/40] Train Loss: 0.010223922577883371\n",
      "Epoch [25/40] Validation Loss: 0.010396726255346524\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  65%|██████▌   | 26/40 [04:08<01:59,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.35      0.85      0.50       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.85      0.70     11047\n",
      "weighted avg       0.93      0.85      0.87     11047\n",
      "\n",
      "Epoch [26/40] Train Loss: 0.010054511941350833\n",
      "Epoch [26/40] Validation Loss: 0.010361690217720688\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  68%|██████▊   | 27/40 [04:16<01:49,  8.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.83      0.90     10056\n",
      "         1.0       0.33      0.85      0.47       991\n",
      "\n",
      "    accuracy                           0.83     11047\n",
      "   macro avg       0.66      0.84      0.69     11047\n",
      "weighted avg       0.92      0.83      0.86     11047\n",
      "\n",
      "Epoch [27/40] Train Loss: 0.009988775869656687\n",
      "Epoch [27/40] Validation Loss: 0.010891963089819264\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.91     10056\n",
      "         1.0       0.34      0.84      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.84      0.70     11047\n",
      "weighted avg       0.92      0.84      0.87     11047\n",
      "\n",
      "Epoch [28/40] Train Loss: 0.009813417787080043\n",
      "Epoch [28/40] Validation Loss: 0.010510081529109292\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  72%|███████▎  | 29/40 [04:33<01:32,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.83      0.90     10056\n",
      "         1.0       0.34      0.85      0.48       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.84      0.69     11047\n",
      "weighted avg       0.92      0.84      0.86     11047\n",
      "\n",
      "Epoch [29/40] Train Loss: 0.009678010756482178\n",
      "Epoch [29/40] Validation Loss: 0.010665588102386506\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 30/40 [04:41<01:23,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.36      0.83      0.50       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.84      0.71     11047\n",
      "weighted avg       0.92      0.85      0.88     11047\n",
      "\n",
      "Epoch [30/40] Train Loss: 0.009609234639393385\n",
      "Epoch [30/40] Validation Loss: 0.010319046400781035\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  78%|███████▊  | 31/40 [04:50<01:15,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.87      0.92     10056\n",
      "         1.0       0.38      0.81      0.52       991\n",
      "\n",
      "    accuracy                           0.87     11047\n",
      "   macro avg       0.68      0.84      0.72     11047\n",
      "weighted avg       0.93      0.87      0.89     11047\n",
      "\n",
      "Epoch [31/40] Train Loss: 0.00943158412550486\n",
      "Epoch [31/40] Validation Loss: 0.010147693365384495\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 32/40 [04:58<01:06,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.85      0.91     10056\n",
      "         1.0       0.35      0.84      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.67      0.84      0.70     11047\n",
      "weighted avg       0.92      0.84      0.87     11047\n",
      "\n",
      "Epoch [32/40] Train Loss: 0.009320971730641996\n",
      "Epoch [32/40] Validation Loss: 0.010548864263472561\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  82%|████████▎ | 33/40 [05:06<00:58,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.92     10056\n",
      "         1.0       0.36      0.82      0.50       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.84      0.71     11047\n",
      "weighted avg       0.92      0.85      0.88     11047\n",
      "\n",
      "Epoch [33/40] Train Loss: 0.009101213555808008\n",
      "Epoch [33/40] Validation Loss: 0.010482201630247509\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  85%|████████▌ | 34/40 [05:15<00:50,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.92     10056\n",
      "         1.0       0.37      0.82      0.51       991\n",
      "\n",
      "    accuracy                           0.86     11047\n",
      "   macro avg       0.67      0.84      0.71     11047\n",
      "weighted avg       0.92      0.86      0.88     11047\n",
      "\n",
      "Epoch [34/40] Train Loss: 0.00902168791365001\n",
      "Epoch [34/40] Validation Loss: 0.010328140715786683\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.87      0.92     10056\n",
      "         1.0       0.38      0.81      0.52       991\n",
      "\n",
      "    accuracy                           0.87     11047\n",
      "   macro avg       0.68      0.84      0.72     11047\n",
      "weighted avg       0.93      0.87      0.89     11047\n",
      "\n",
      "Epoch [35/40] Train Loss: 0.008888100817437215\n",
      "Epoch [35/40] Validation Loss: 0.010299613095778851\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 36/40 [05:32<00:33,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.91     10056\n",
      "         1.0       0.36      0.82      0.50       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.84      0.71     11047\n",
      "weighted avg       0.92      0.85      0.88     11047\n",
      "\n",
      "Epoch [36/40] Train Loss: 0.00885281052704886\n",
      "Epoch [36/40] Validation Loss: 0.010447432948132841\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  92%|█████████▎| 37/40 [05:40<00:24,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.91     10056\n",
      "         1.0       0.35      0.84      0.49       991\n",
      "\n",
      "    accuracy                           0.84     11047\n",
      "   macro avg       0.66      0.84      0.70     11047\n",
      "weighted avg       0.92      0.84      0.87     11047\n",
      "\n",
      "Epoch [37/40] Train Loss: 0.008753942798322338\n",
      "Epoch [37/40] Validation Loss: 0.010692247411760556\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.88      0.93     10056\n",
      "         1.0       0.40      0.79      0.53       991\n",
      "\n",
      "    accuracy                           0.87     11047\n",
      "   macro avg       0.69      0.84      0.73     11047\n",
      "weighted avg       0.93      0.87      0.89     11047\n",
      "\n",
      "Epoch [38/40] Train Loss: 0.008579514221041923\n",
      "Epoch [38/40] Validation Loss: 0.01025525563294203\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  98%|█████████▊| 39/40 [05:57<00:08,  8.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.91     10056\n",
      "         1.0       0.36      0.83      0.51       991\n",
      "\n",
      "    accuracy                           0.85     11047\n",
      "   macro avg       0.67      0.84      0.71     11047\n",
      "weighted avg       0.93      0.85      0.88     11047\n",
      "\n",
      "Epoch [39/40] Train Loss: 0.008460671467149801\n",
      "Epoch [39/40] Validation Loss: 0.01048637245084592\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 40/40 [06:05<00:00,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/40] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.92     10056\n",
      "         1.0       0.37      0.82      0.51       991\n",
      "\n",
      "    accuracy                           0.86     11047\n",
      "   macro avg       0.67      0.84      0.71     11047\n",
      "weighted avg       0.92      0.86      0.88     11047\n",
      "\n",
      "Epoch [40/40] Train Loss: 0.008363544177621206\n",
      "Epoch [40/40] Validation Loss: 0.010511460680921634\n",
      "------------------------------------------------------------\n",
      "===================BEST MODEL===================\n",
      "Best Model Epoch: 30\n",
      "Best Model Validation Loss: 0.010147693365384495\n",
      "Best Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.87      0.92     10056\n",
      "         1.0       0.38      0.81      0.52       991\n",
      "\n",
      "    accuracy                           0.87     11047\n",
      "   macro avg       0.68      0.84      0.72     11047\n",
      "weighted avg       0.93      0.87      0.89     11047\n",
      "\n",
      "================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "NUM_EPOCHS = 40\n",
    "AUDIO_EMBEDDING_SIZE = 768\n",
    "VIDEO_EMBEDDING_SIZE = 768\n",
    "TEXT_EMBEDDING_SIZE = 1024\n",
    "TOTAL_EMBEDDING_SIZE = (\n",
    "    AUDIO_EMBEDDING_SIZE + VIDEO_EMBEDDING_SIZE + TEXT_EMBEDDING_SIZE\n",
    ")\n",
    "# TOTAL_EMBEDDING_SIZE = 11237\n",
    "\n",
    "model = EmotionCauseDetector(\n",
    "    TOTAL_EMBEDDING_SIZE,\n",
    "    device,\n",
    "    hidden_dimension=2000,\n",
    ")\n",
    "_ = model.to(device)\n",
    "\n",
    "weight_ratio = trn_dataset.NEGATIVE_SAMPLE_COUNT / trn_dataset.POSITIVE_SAMPLE_COUNT\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weight_ratio).to(device))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "\n",
    "total_steps = len(trn_dataloader) * NUM_EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "best_model_file = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "best_classification_report = None\n",
    "\n",
    "print(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "print(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "print(TEXT_EMBEDDINGS_FILEPATH)\n",
    "epoch_iter = tqdm(range(NUM_EPOCHS), desc=\"Epoch\", position=0)\n",
    "for epoch in epoch_iter:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(trn_dataloader, desc=\"Train Data Batch\", position=1, leave=False):\n",
    "        distances = batch[\"distance\"].to(device)\n",
    "\n",
    "        orig_audios = batch[\"original_audio\"].to(device)\n",
    "        orig_videos = batch[\"original_video\"].to(device)\n",
    "        orig_texts = batch[\"original_text\"].to(device)\n",
    "\n",
    "        cause_audios = batch[\"cause_audio\"].to(device)\n",
    "        cause_videos = batch[\"cause_video\"].to(device)\n",
    "        cause_texts = batch[\"cause_text\"].to(device)\n",
    "\n",
    "        is_cause = batch[\"is_cause\"].to(device)\n",
    "\n",
    "        orig_embedding = torch.cat((orig_audios, orig_videos, orig_texts), axis=1).float()\n",
    "        cause_embedding = torch.cat((cause_audios, cause_videos, cause_texts), axis=1).float()\n",
    "\n",
    "        output_logits = model(orig_embedding, cause_embedding, distances).squeeze()\n",
    "\n",
    "        loss = criterion(output_logits, is_cause)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    total_val_predictions = 0\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(val_dataloader, desc=\"Val Data Batch\", position=1, leave=False):\n",
    "            distances = val_batch[\"distance\"].to(device)\n",
    "\n",
    "            orig_audios = val_batch[\"original_audio\"].to(device)\n",
    "            orig_videos = val_batch[\"original_video\"].to(device)\n",
    "            orig_texts = val_batch[\"original_text\"].to(device)\n",
    "\n",
    "            cause_audios = val_batch[\"cause_audio\"].to(device)\n",
    "            cause_videos = val_batch[\"cause_video\"].to(device)\n",
    "            cause_texts = val_batch[\"cause_text\"].to(device)\n",
    "\n",
    "            is_cause = val_batch[\"is_cause\"].to(device)\n",
    "\n",
    "            orig_embedding = torch.cat((orig_audios, orig_videos, orig_texts), axis=1).float()\n",
    "            cause_embedding = torch.cat((cause_audios, cause_videos, cause_texts), axis=1).float()\n",
    "\n",
    "            output_logits = model(orig_embedding, cause_embedding, distances).squeeze()\n",
    "\n",
    "            val_loss = criterion(output_logits, is_cause)\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            predicted_is_cause = (output_logits >= 0.5).float()\n",
    "\n",
    "            correct_predictions_val = (is_cause == predicted_is_cause).sum().item()\n",
    "\n",
    "            total_val_correct += correct_predictions_val\n",
    "            total_val_predictions += (predicted_is_cause == 1.0).sum().item()\n",
    "\n",
    "            true_labels.extend(is_cause.cpu().numpy())\n",
    "            predicted_labels.extend(predicted_is_cause.cpu().numpy())\n",
    "\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "    avg_loss = total_loss / len(trn_dataset)\n",
    "    avg_val_loss = total_val_loss / len(val_dataset)\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        best_classification_report = report\n",
    "        # best_model_file = save_model(epoch)\n",
    "        torch.save(model.state_dict(), f\"/tmp/semeval24_task3/baseline_models/pairing_models/paring_model_best_model.pt\")\n",
    "        numpy.save(\n",
    "            f\"/tmp/semeval24_task3/baseline_models/pairing_models/pairing_model_pos_embeds_best_model.npy\",\n",
    "            model.positional_embeddings.cpu().numpy(),\n",
    "        )\n",
    "\n",
    "    print(f\"Epoch [{epoch+1:02}/{NUM_EPOCHS}] Classification Report:\\n{report}\")\n",
    "    print(f\"Epoch [{epoch+1:02}/{NUM_EPOCHS}] Train Loss: {avg_loss}\")\n",
    "    print(f\"Epoch [{epoch+1:02}/{NUM_EPOCHS}] Validation Loss: {avg_val_loss}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    save_model(epoch)\n",
    "\n",
    "print(\"===================BEST MODEL===================\")\n",
    "print(f\"Best Model Epoch: {best_epoch}\")\n",
    "print(f\"Best Model Validation Loss: {best_val_loss}\")\n",
    "print(f\"Best Model Classification Report:\\n{best_classification_report}\")\n",
    "print(\"================================================\")\n",
    "\n",
    "# Current best, Epoch 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28226244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
