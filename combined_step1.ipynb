{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8ed30e-65a3-4e55-bc32-2694490898f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "TRAIN_FILE_PATH = \"/home2/suyash.mathur/final_clean_data/train/Subtask_2.json\"\n",
    "VALIDATION_FILE_PATH = \"/home2/suyash.mathur/final_clean_data/val/Subtask_2.json\"\n",
    "with open(TRAIN_FILE_PATH) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(VALIDATION_FILE_PATH) as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "pprint(len(train_data))\n",
    "pprint(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2075, 1008, 1489, 5258, 1670, 376, 340]\n",
      "[0.00048192771084337347, 0.000992063492063492, 0.000671591672263264, 0.0001901863826550019, 0.0005988023952095808, 0.0026595744680851063, 0.0029411764705882353]\n"
     ]
    }
   ],
   "source": [
    "class EmotionIndexer:\n",
    "    def __init__(self):\n",
    "        self.emotion_to_index = {\n",
    "            'joy': 0,\n",
    "            'sadness': 1,\n",
    "            'anger': 2,\n",
    "            'neutral': 3,\n",
    "            'surprise': 4,\n",
    "            'disgust': 5,\n",
    "            'fear': 6,\n",
    "            'pad': 7,\n",
    "        }\n",
    "        self.emotion_freq = [0]*7\n",
    "        self.weights = None\n",
    "\n",
    "        self.index_to_emotion = {index: emotion for emotion, index in self.emotion_to_index.items()}\n",
    "\n",
    "    def emotion_to_idx(self, emotion):\n",
    "        return self.emotion_to_index.get(emotion, None)\n",
    "\n",
    "    def idx_to_emotion(self, index):\n",
    "        return self.index_to_emotion.get(index, None)\n",
    "    \n",
    "    def compute_weights(self, data):\n",
    "        for conversation in data:\n",
    "            conversation = conversation['conversation']\n",
    "            for utterance in conversation:\n",
    "                emotion = utterance['emotion']\n",
    "                self.emotion_freq[self.emotion_to_index[emotion]] += 1\n",
    "        print(self.emotion_freq)\n",
    "        self.weights = [1/freq for freq in self.emotion_freq]\n",
    "\n",
    "# Example usage\n",
    "indexer = EmotionIndexer()\n",
    "indexer.compute_weights(train_data)\n",
    "print(indexer.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        with open(audio_embeddings_path, \"rb\") as f:\n",
    "            self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    \n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        with open(video_embeddings_path, \"rb\") as f:\n",
    "            self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        # video_name = video_name.split(\".\")[0]\n",
    "        video_embedding = self.video_embeddings[video_name].reshape((16,-1))\n",
    "        video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, json_file, audio_encoder, video_encoder, text_encoder, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = self.load_data(json_file)\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def load_data(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.data[idx]['conversation']\n",
    "        emotion_labels = [utterance['emotion'] for utterance in conversation]\n",
    "        audio_paths = [utterance['video_name'].replace('mp4', 'wav') for utterance in conversation]\n",
    "        video_paths = [utterance['video_name'] for utterance in conversation]\n",
    "        texts = [utterance['video_name'] for utterance in conversation]\n",
    "\n",
    "        audio_embeddings = [self.audio_encoder.lmao(audio_path) for audio_path in audio_paths]\n",
    "        video_embeddings = [self.video_encoder.lmao(video_path) for video_path in video_paths]\n",
    "        text_embeddings = [self.text_encoder.lmao(text) for text in texts]\n",
    "\n",
    "        cause_pairs = self.data[idx]['emotion-cause_pairs']\n",
    "        useful_utterances = set([int(cause_pair[1]) for cause_pair in cause_pairs])\n",
    "        cause_labels = []\n",
    "        for utterance in conversation:\n",
    "            if utterance['utterance_ID'] in useful_utterances:\n",
    "                cause_labels.append(1)\n",
    "            else:\n",
    "                cause_labels.append(0)\n",
    "        \n",
    "        \n",
    "        # Pad or truncate conversations to the maximum sequence length\n",
    "        if len(conversation) < self.max_seq_len and False:\n",
    "            pad_length = self.max_seq_len - len(conversation)\n",
    "            audio_embeddings += [torch.zeros_like(audio_embeddings[0])] * pad_length\n",
    "            video_embeddings += [torch.zeros_like(video_embeddings[0])] * pad_length\n",
    "            text_embeddings += [torch.zeros_like(text_embeddings[0])] * pad_length\n",
    "            cause_labels += [-1] * pad_length\n",
    "            emotion_labels += ['pad'] * pad_length\n",
    "            pad_mask = [1] * len(conversation) + [0] * pad_length\n",
    "        else:\n",
    "            audio_embeddings = audio_embeddings[:self.max_seq_len]\n",
    "            video_embeddings = video_embeddings[:self.max_seq_len]\n",
    "            text_embeddings = text_embeddings[:self.max_seq_len]\n",
    "            emotion_labels = emotion_labels[:self.max_seq_len]\n",
    "            cause_labels = cause_labels[:self.max_seq_len]\n",
    "            pad_mask = [1] * self.max_seq_len\n",
    "\n",
    "        emotion_indices = [indexer.emotion_to_idx(emotion) for emotion in emotion_labels]\n",
    "        \n",
    "        audio_embeddings = torch.stack(audio_embeddings)\n",
    "        video_embeddings = torch.stack(video_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        emotion_indices = torch.from_numpy(np.array(emotion_indices))\n",
    "        pad_mask = torch.from_numpy(np.array(pad_mask))\n",
    "        cause_labels = torch.from_numpy(np.array(cause_labels))\n",
    "        \n",
    "        return {\n",
    "            'audio': audio_embeddings,\n",
    "            'video': video_embeddings,\n",
    "            'text': text_embeddings,\n",
    "            'emotion_labels': emotion_indices,\n",
    "            'pad_mask': pad_mask,\n",
    "            'cause_labels': cause_labels,\n",
    "        }\n",
    "# Example usage\n",
    "# You need to define your audio, video, and text encoders accordingly\n",
    "\n",
    "# Define your data paths\n",
    "DATA_DIR = \"/tmp/semeval24_task3\"\n",
    "\n",
    "AUDIO_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"audio_embeddings\", \"audio_embeddings.pkl\")\n",
    "VIDEO_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"video_embeddings\", \"train\", \"video_embeddings.pkl\")\n",
    "TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "max_seq_len = 40  # Adjust this according to your needs\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = ConversationDataset(TRAIN_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "validation_dataset = ConversationDataset(VALIDATION_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Example of iterating through batches\n",
    "# for batch in dataloader:\n",
    "#     audio = batch['audio']  # Shape: (batch_size, max_seq_len, audio_embedding_size)\n",
    "#     video = batch['video']  # Shape: (batch_size, max_seq_len, video_embedding_size)\n",
    "#     text = batch['text']    # Shape: (batch_size, max_seq_len, text_embedding_size)\n",
    "#     emotions = batch['emotion_labels']  # List of emotion labels for each utterance in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BiLSTM_basic(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim=768, hidden_dim=300, output_size=13):\n",
    "        super(BiLSTM_basic, self).__init__()\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        # if embeddings is None:\n",
    "        #     self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # else:\n",
    "        # self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "        # 2. LSTM Layer\n",
    "        #embedding dimension must be equal to bert embeddings\n",
    "        #use of 'batch_first=true'?\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=1, batch_first=False)\n",
    "        \n",
    "        # 3. Optional dropout layer\n",
    "        self.dropout_layer = nn.Dropout(p=0.3)\n",
    "\n",
    "        # 4. Dense Layer ?? \n",
    "        self.hidden2tag = nn.Linear(2*hidden_dim, output_size)\n",
    "\n",
    "        self.relu=nn.ReLU\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    \n",
    "    def generate_emissions(self, batch_text):\n",
    "        hidden_layer = self.init_hidden(len(batch_text))\n",
    "\n",
    "        embeddings = enco(batch_text)\n",
    "\n",
    "        # x_packed = pack_padded_sequence(embeddings, batch_first = True)\n",
    "        \n",
    "        # packed_seqs = pack_padded_sequence(embeddings, batch_length)\n",
    "        print(embeddings.shape)\n",
    "        lstm_output, _ = self.lstm(embeddings, hidden_layer)\n",
    "        print(lstm_output.shape)\n",
    "        # lstm_output, _ = pad_packed_sequence(lstm_output)\n",
    "\n",
    "        # self.relu(lstm_output)\n",
    "        # lstm_output, op_lengths = pad_packed_sequence(lstm_output, batch_first = True)\n",
    "\n",
    "        lstm_output = self.dropout_layer(lstm_output)\n",
    "        print(lstm_output.shape)\n",
    "\n",
    "        emissions = self.hidden2tag(lstm_output)\n",
    "        # emissions = torch.squeeze(emissions)\n",
    "        # emissions = emissions.unsqueeze(0)\n",
    "\n",
    "        return emissions\n",
    "        \n",
    "    def loss(self, batch_text, batch_label):\n",
    "        # print(len(batch_text))\n",
    "\n",
    "        # hidden_layer = self.init_hidden(len(batch_text))\n",
    "\n",
    "        # embeddings = enco(batch_text)\n",
    "\n",
    "        # # x_packed = pack_padded_sequence(embeddings, batch_first = True)\n",
    "        \n",
    "        # # packed_seqs = pack_padded_sequence(embeddings, batch_length)\n",
    "        # lstm_output, _ = self.lstm(embeddings, hidden_layer)\n",
    "        # print(lstm_output.shape)\n",
    "        # # lstm_output, _ = pad_packed_sequence(lstm_output)\n",
    "\n",
    "        # # self.relu(lstm_output)\n",
    "        # # lstm_output, op_lengths = pad_packed_sequence(lstm_output, batch_first = True)\n",
    "\n",
    "        # lstm_output = self.dropout_layer(lstm_output)\n",
    "        # print(lstm_output.shape)\n",
    "\n",
    "        emissions = self.generate_emissions(batch_text)\n",
    "        batch_label = batch_label.unsqueeze(1)\n",
    "        # print(logits.shape)\n",
    "        loss = -self.crf_model(emissions, batch_label)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, batch_text):\n",
    "        emissions = self.generate_emissions(batch_text)\n",
    "        # print(logits.shape)\n",
    "        label = self.crf_model.viterbi_decode(emissions)\n",
    "        return label\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.randn(2, 1, self.hidden_dim).to(device), torch.randn(2, 1, self.hidden_dim).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from TorchCRF import CRF\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "        # Replace Transformer with BiLSTM\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "                              dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "        self.crf_model = CRF(num_emotions)\n",
    "        \n",
    "\n",
    "    def generate_emissions(self, audio_encoding, video_encoding, text_encoding):\n",
    "        # Concatenate or combine the audio, video, and text encodings\n",
    "        audio_encoding = audio_encoding.float()\n",
    "        video_encoding = video_encoding.float()\n",
    "        text_encoding = text_encoding.float()\n",
    "        \n",
    "        audio_encoding = self.audio_dropout(audio_encoding)\n",
    "        video_encoding = self.video_dropout(video_encoding)\n",
    "        text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "        combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "        combined_encoding = self.first_linear(combined_encoding)\n",
    "        combined_encoding = self.relu(combined_encoding)\n",
    "        combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "        # Take the output of the BiLSTM\n",
    "        emotion_logits = self.linear(lstm_output)\n",
    "        # Apply a softmax layer\n",
    "        # emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "        return emotion_logits\n",
    "\n",
    "    def loss(self, audio_encoding, video_encoding, text_encoding, emotion_labels, padding):\n",
    "\n",
    "        emissions = self.generate_emissions(audio_encoding, video_encoding, text_encoding)\n",
    "        emotion_labels = emotion_labels.unsqueeze(1)\n",
    "        x, y, _ = emissions.shape\n",
    "        padding = torch.ones((x, y), dtype=torch.bool).to('cuda')\n",
    "        emotion_labels = emotion_labels.squeeze(1)\n",
    "        loss = -self.crf_model(emissions, emotion_labels, padding)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, audio_encoding, video_encoding, text_encoding):\n",
    "        emissions = self.generate_emissions(audio_encoding, video_encoding, text_encoding)\n",
    "        x, y, _ = emissions.shape\n",
    "        padding = torch.ones((x, y), dtype=torch.bool).to('cuda')\n",
    "        label = self.crf_model.viterbi_decode(emissions, padding)\n",
    "        return label\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim).to('cuda'), torch.randn(2, 1, self.hidden_dim).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:56<00:00, 10.60it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 36.85it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.35      0.41       226\n",
      "           1       0.35      0.21      0.26       139\n",
      "           2       0.17      0.08      0.11       126\n",
      "           3       0.59      0.86      0.70       671\n",
      "           4       0.21      0.15      0.17       170\n",
      "           5       0.00      0.00      0.00        38\n",
      "           6       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.51      1403\n",
      "   macro avg       0.26      0.23      0.24      1403\n",
      "weighted avg       0.44      0.51      0.46      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72       795\n",
      "           1       0.63      0.58      0.60       608\n",
      "\n",
      "    accuracy                           0.67      1403\n",
      "   macro avg       0.66      0.66      0.66      1403\n",
      "weighted avg       0.67      0.67      0.67      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:52<00:00, 11.02it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 37.06it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.27      0.37       226\n",
      "           1       0.45      0.21      0.29       139\n",
      "           2       0.27      0.24      0.25       126\n",
      "           3       0.61      0.88      0.72       671\n",
      "           4       0.54      0.52      0.53       170\n",
      "           5       0.00      0.00      0.00        38\n",
      "           6       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.57      1403\n",
      "   macro avg       0.36      0.30      0.31      1403\n",
      "weighted avg       0.53      0.57      0.52      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.89      0.74       795\n",
      "           1       0.71      0.34      0.46       608\n",
      "\n",
      "    accuracy                           0.65      1403\n",
      "   macro avg       0.67      0.62      0.60      1403\n",
      "weighted avg       0.67      0.65      0.62      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:56<00:00, 10.61it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 36.71it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.31      0.42       226\n",
      "           1       0.36      0.27      0.31       139\n",
      "           2       0.38      0.19      0.25       126\n",
      "           3       0.60      0.91      0.73       671\n",
      "           4       0.63      0.42      0.51       170\n",
      "           5       0.00      0.00      0.00        38\n",
      "           6       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.58      1403\n",
      "   macro avg       0.37      0.30      0.32      1403\n",
      "weighted avg       0.54      0.58      0.53      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.74       795\n",
      "           1       0.67      0.57      0.62       608\n",
      "\n",
      "    accuracy                           0.69      1403\n",
      "   macro avg       0.69      0.68      0.68      1403\n",
      "weighted avg       0.69      0.69      0.69      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:52<00:00, 10.98it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 36.90it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.40      0.46       226\n",
      "           1       0.53      0.24      0.33       139\n",
      "           2       0.28      0.38      0.33       126\n",
      "           3       0.67      0.78      0.72       671\n",
      "           4       0.45      0.59      0.51       170\n",
      "           5       0.00      0.00      0.00        38\n",
      "           6       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.57      1403\n",
      "   macro avg       0.35      0.34      0.34      1403\n",
      "weighted avg       0.54      0.57      0.54      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73       795\n",
      "           1       0.65      0.65      0.65       608\n",
      "\n",
      "    accuracy                           0.69      1403\n",
      "   macro avg       0.69      0.69      0.69      1403\n",
      "weighted avg       0.69      0.69      0.69      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:52<00:00, 10.99it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 37.04it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.40      0.45       226\n",
      "           1       0.41      0.33      0.37       139\n",
      "           2       0.34      0.33      0.33       126\n",
      "           3       0.64      0.83      0.72       671\n",
      "           4       0.65      0.48      0.55       170\n",
      "           5       0.00      0.00      0.00        38\n",
      "           6       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.58      1403\n",
      "   macro avg       0.37      0.34      0.35      1403\n",
      "weighted avg       0.54      0.58      0.55      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.89      0.75       795\n",
      "           1       0.71      0.34      0.46       608\n",
      "\n",
      "    accuracy                           0.65      1403\n",
      "   macro avg       0.67      0.62      0.60      1403\n",
      "weighted avg       0.67      0.65      0.62      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:56<00:00, 10.63it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 36.75it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.44      0.49       226\n",
      "           1       0.42      0.34      0.37       139\n",
      "           2       0.32      0.39      0.35       126\n",
      "           3       0.66      0.85      0.74       671\n",
      "           4       0.70      0.43      0.53       170\n",
      "           5       0.00      0.00      0.00        38\n",
      "           6       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.60      1403\n",
      "   macro avg       0.38      0.35      0.36      1403\n",
      "weighted avg       0.56      0.60      0.57      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.61      0.67       795\n",
      "           1       0.59      0.74      0.66       608\n",
      "\n",
      "    accuracy                           0.67      1403\n",
      "   macro avg       0.67      0.67      0.66      1403\n",
      "weighted avg       0.68      0.67      0.67      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:56<00:00, 10.63it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 35.61it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.39      0.44       226\n",
      "           1       0.47      0.26      0.33       139\n",
      "           2       0.24      0.53      0.33       126\n",
      "           3       0.68      0.73      0.71       671\n",
      "           4       0.55      0.51      0.53       170\n",
      "           5       0.00      0.00      0.00        38\n",
      "           6       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.55      1403\n",
      "   macro avg       0.35      0.35      0.33      1403\n",
      "weighted avg       0.54      0.55      0.54      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.69      0.70       795\n",
      "           1       0.61      0.65      0.63       608\n",
      "\n",
      "    accuracy                           0.67      1403\n",
      "   macro avg       0.66      0.67      0.67      1403\n",
      "weighted avg       0.67      0.67      0.67      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:53<00:00, 10.85it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 36.72it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.46      0.46       226\n",
      "           1       0.44      0.31      0.36       139\n",
      "           2       0.35      0.36      0.36       126\n",
      "           3       0.68      0.79      0.73       671\n",
      "           4       0.55      0.55      0.55       170\n",
      "           5       0.00      0.00      0.00        38\n",
      "           6       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.58      1403\n",
      "   macro avg       0.36      0.35      0.35      1403\n",
      "weighted avg       0.54      0.58      0.56      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.71       795\n",
      "           1       0.62      0.57      0.59       608\n",
      "\n",
      "    accuracy                           0.66      1403\n",
      "   macro avg       0.65      0.65      0.65      1403\n",
      "weighted avg       0.66      0.66      0.66      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:55<00:00, 10.67it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 35.93it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.57      0.49       226\n",
      "           1       0.42      0.37      0.39       139\n",
      "           2       0.38      0.34      0.36       126\n",
      "           3       0.68      0.74      0.71       671\n",
      "           4       0.63      0.49      0.55       170\n",
      "           5       0.00      0.00      0.00        38\n",
      "           6       0.25      0.06      0.10        33\n",
      "\n",
      "    accuracy                           0.58      1403\n",
      "   macro avg       0.40      0.37      0.37      1403\n",
      "weighted avg       0.55      0.58      0.56      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.71       795\n",
      "           1       0.62      0.46      0.53       608\n",
      "\n",
      "    accuracy                           0.64      1403\n",
      "   macro avg       0.64      0.62      0.62      1403\n",
      "weighted avg       0.64      0.64      0.63      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [02:05<00:00,  9.82it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 36.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.42      0.48       226\n",
      "           1       0.36      0.36      0.36       139\n",
      "           2       0.42      0.25      0.31       126\n",
      "           3       0.65      0.82      0.72       671\n",
      "           4       0.57      0.48      0.52       170\n",
      "           5       0.00      0.00      0.00        38\n",
      "           6       0.13      0.12      0.12        33\n",
      "\n",
      "    accuracy                           0.58      1403\n",
      "   macro avg       0.39      0.35      0.36      1403\n",
      "weighted avg       0.55      0.58      0.55      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       795\n",
      "           1       0.60      0.57      0.58       608\n",
      "\n",
      "    accuracy                           0.65      1403\n",
      "   macro avg       0.64      0.64      0.64      1403\n",
      "weighted avg       0.64      0.65      0.64      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:52<00:00, 11.02it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 37.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.52      0.44       226\n",
      "           1       0.49      0.19      0.28       139\n",
      "           2       0.28      0.41      0.34       126\n",
      "           3       0.70      0.67      0.68       671\n",
      "           4       0.50      0.61      0.55       170\n",
      "           5       0.11      0.03      0.04        38\n",
      "           6       0.22      0.06      0.10        33\n",
      "\n",
      "    accuracy                           0.54      1403\n",
      "   macro avg       0.39      0.36      0.35      1403\n",
      "weighted avg       0.54      0.54      0.53      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       795\n",
      "           1       0.59      0.58      0.58       608\n",
      "\n",
      "    accuracy                           0.64      1403\n",
      "   macro avg       0.64      0.63      0.64      1403\n",
      "weighted avg       0.64      0.64      0.64      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:52<00:00, 11.02it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 37.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.50      0.46       226\n",
      "           1       0.50      0.22      0.31       139\n",
      "           2       0.31      0.41      0.36       126\n",
      "           3       0.68      0.74      0.71       671\n",
      "           4       0.56      0.57      0.57       170\n",
      "           5       0.40      0.05      0.09        38\n",
      "           6       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.56      1403\n",
      "   macro avg       0.41      0.36      0.36      1403\n",
      "weighted avg       0.55      0.56      0.55      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.68       795\n",
      "           1       0.58      0.64      0.61       608\n",
      "\n",
      "    accuracy                           0.65      1403\n",
      "   macro avg       0.64      0.65      0.64      1403\n",
      "weighted avg       0.65      0.65      0.65      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:54<00:00, 10.76it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 36.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.42      0.45       226\n",
      "           1       0.37      0.36      0.36       139\n",
      "           2       0.39      0.37      0.38       126\n",
      "           3       0.65      0.79      0.71       671\n",
      "           4       0.62      0.46      0.53       170\n",
      "           5       0.23      0.08      0.12        38\n",
      "           6       0.50      0.09      0.15        33\n",
      "\n",
      "    accuracy                           0.57      1403\n",
      "   macro avg       0.47      0.37      0.39      1403\n",
      "weighted avg       0.55      0.57      0.55      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.69       795\n",
      "           1       0.60      0.58      0.59       608\n",
      "\n",
      "    accuracy                           0.65      1403\n",
      "   macro avg       0.64      0.64      0.64      1403\n",
      "weighted avg       0.65      0.65      0.65      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [01:56<00:00, 10.58it/s]\n",
      "100%|██████████| 138/138 [00:03<00:00, 36.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training data metrics\n",
      "VALIDATION METRICS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.50      0.47       226\n",
      "           1       0.38      0.29      0.33       139\n",
      "           2       0.25      0.47      0.33       126\n",
      "           3       0.72      0.63      0.67       671\n",
      "           4       0.55      0.45      0.50       170\n",
      "           5       0.10      0.11      0.10        38\n",
      "           6       0.08      0.12      0.10        33\n",
      "\n",
      "    accuracy                           0.51      1403\n",
      "   macro avg       0.36      0.37      0.36      1403\n",
      "weighted avg       0.55      0.51      0.52      1403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       795\n",
      "           1       0.58      0.57      0.58       608\n",
      "\n",
      "    accuracy                           0.64      1403\n",
      "   macro avg       0.63      0.63      0.63      1403\n",
      "weighted avg       0.64      0.64      0.64      1403\n",
      "\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 567/1236 [00:53<01:02, 10.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home2/suyash.mathur/semeval24/task3/last_resort/combined_step1.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-ada/home2/suyash.mathur/semeval24/task3/last_resort/combined_step1.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m total_correct_causes \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-ada/home2/suyash.mathur/semeval24/task3/last_resort/combined_step1.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m total_predictions \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-ada/home2/suyash.mathur/semeval24/task3/last_resort/combined_step1.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(train_dataloader):  \u001b[39m# Assuming you have a DataLoader for your dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-ada/home2/suyash.mathur/semeval24/task3/last_resort/combined_step1.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m# Extract data from the batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-ada/home2/suyash.mathur/semeval24/task3/last_resort/combined_step1.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     audio \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-ada/home2/suyash.mathur/semeval24/task3/last_resort/combined_step1.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     video \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/std.py:1188\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1186\u001b[0m dt \u001b[39m=\u001b[39m cur_t \u001b[39m-\u001b[39m last_print_t\n\u001b[1;32m   1187\u001b[0m \u001b[39mif\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m mininterval \u001b[39mand\u001b[39;00m cur_t \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_start_t:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(n \u001b[39m-\u001b[39;49m last_print_n)\n\u001b[1;32m   1189\u001b[0m     last_print_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_n\n\u001b[1;32m   1190\u001b[0m     last_print_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_t\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/std.py:1239\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ema_dn(dn)\n\u001b[1;32m   1238\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ema_dt(dt)\n\u001b[0;32m-> 1239\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrefresh(lock_args\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlock_args)\n\u001b[1;32m   1240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdynamic_miniters:\n\u001b[1;32m   1241\u001b[0m     \u001b[39m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[39m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m     \u001b[39m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     \u001b[39m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m     \u001b[39m# at least 5 more iterations.\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxinterval \u001b[39mand\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxinterval:\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/std.py:1344\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39macquire()\n\u001b[0;32m-> 1344\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisplay()\n\u001b[1;32m   1345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nolock:\n\u001b[1;32m   1346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/std.py:1492\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[1;32m   1491\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1492\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__str__\u001b[39;49m() \u001b[39mif\u001b[39;49;00m msg \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m msg)\n\u001b[1;32m   1493\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[1;32m   1494\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(\u001b[39m-\u001b[39mpos)\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/std.py:347\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_status\u001b[39m(s):\n\u001b[1;32m    346\u001b[0m     len_s \u001b[39m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 347\u001b[0m     fp_write(\u001b[39m'\u001b[39;49m\u001b[39m\\r\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m s \u001b[39m+\u001b[39;49m (\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mmax\u001b[39;49m(last_len[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39;49m len_s, \u001b[39m0\u001b[39;49m)))\n\u001b[1;32m    348\u001b[0m     last_len[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m len_s\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/std.py:341\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfp_write\u001b[39m(s):\n\u001b[1;32m    340\u001b[0m     fp\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(s))\n\u001b[0;32m--> 341\u001b[0m     fp_flush()\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/utils.py:127\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    129\u001b[0m         \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merrno \u001b[39m!=\u001b[39m \u001b[39m5\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/ipykernel/iostream.py:564\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(evt\u001b[39m.\u001b[39mset)\n\u001b[1;32m    563\u001b[0m     \u001b[39m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt\u001b[39m.\u001b[39;49mwait(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_timeout):\n\u001b[1;32m    565\u001b[0m         \u001b[39m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    566\u001b[0m         \u001b[39m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIOStream.flush timed out\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39m__stderr__)\n\u001b[1;32m    568\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Define your model\n",
    "# model = EmotionClassifier(input_size=11237, hidden_size=5000, num_layers=2, num_heads=2, dropout=0.2, num_emotions=7)\n",
    "emotion_model = EmotionClassifier(input_size=768*3, hidden_size=2000, num_layers=3, dropout=0.6, num_emotions=7)\n",
    "cause_model = EmotionClassifier(input_size=768*3, hidden_size=2000, num_layers=2, dropout=0.6, num_emotions=2)\n",
    "emotion_model.to(\"cuda\")\n",
    "cause_model.to(\"cuda\")\n",
    "\n",
    "weights_tensor = torch.tensor(np.array(indexer.weights)).to(\"cuda\").float()\n",
    "emotion_criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights_tensor,\n",
    "    ignore_index=7\n",
    ")\n",
    "\n",
    "cause_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "num_epochs = 30\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "emotion_optimizer = AdamW(emotion_model.parameters(), lr=0.0001)\n",
    "emotion_lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    emotion_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "cause_optimizer = AdamW(cause_model.parameters(), lr=1e-4)\n",
    "cause_lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    cause_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "# Define training parameters\n",
    "\n",
    "# Training loop\n",
    "for epoch in (range(num_epochs)):\n",
    "    emotion_model.train()  # Set the model to training mode\n",
    "    cause_model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    total_correct_emotions = 0\n",
    "    total_correct_causes = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):  # Assuming you have a DataLoader for your dataset\n",
    "        # Extract data from the batch\n",
    "        audio = batch['audio'].to('cuda')\n",
    "        video = batch['video'].to('cuda')\n",
    "        text = batch['text'].to('cuda')\n",
    "        cause_indices = batch['cause_labels'].to('cuda')\n",
    "        \n",
    "        audio_copy = audio.clone().detach()\n",
    "        video_copy = video.clone().detach()\n",
    "        text_copy = text.clone().detach()\n",
    "        \n",
    "        emotion_indices = batch['emotion_labels'].to('cuda')\n",
    "        pad_mask = batch['pad_mask'].to('cuda')\n",
    "\n",
    "        # Forward pass\n",
    "        # emotion_logits = emotion_model(audio_copy, video_copy, text_copy)\n",
    "\n",
    "        # Reshape emotion_logits\n",
    "        # emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "        # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "        # emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "        # Calculate a mask to exclude padded positions from the loss\n",
    "        pad_mask = pad_mask.view(-1).float()\n",
    "\n",
    "        # Calculate the loss, excluding padded positions\n",
    "        emotion_loss = emotion_model.loss(audio_copy, video_copy, text_copy, emotion_indices, pad_mask)\n",
    "        # masked_loss = torch.sum(loss * pad_mask) / torch.sum(pad_mask)\n",
    "        masked_loss = emotion_loss# *pad_mask\n",
    "        # Backpropagation and optimization\n",
    "        \n",
    "        \n",
    "        # cause_logits = cause_model(audio, video, text)\n",
    "        # cause_logits = cause_logits.view(-1, cause_logits.size(-1))\n",
    "        # cause_indices = cause_indices.view(-1)\n",
    "        \n",
    "        cause_loss = cause_model.loss(audio, video, text, cause_indices, pad_mask)\n",
    "        masked_loss += cause_loss\n",
    "        \n",
    "        emotion_optimizer.zero_grad()\n",
    "        cause_optimizer.zero_grad()\n",
    "        \n",
    "        masked_loss.backward()\n",
    "        \n",
    "        emotion_optimizer.step()\n",
    "        cause_optimizer.step()        \n",
    "\n",
    "        total_loss += masked_loss.item()\n",
    "        total_tokens += torch.sum(pad_mask).item()\n",
    "        \n",
    "        predicted_emotions = emotion_model.predict(audio, video, text)\n",
    "        correct_predictions_emotions = ((predicted_emotions == emotion_indices) * pad_mask).sum().item()\n",
    "\n",
    "        predicted_causes = cause_model.predict(audio, video, text)\n",
    "        correct_predictions_causes = ((predicted_causes == cause_indices) * pad_mask).sum().item()\n",
    "        \n",
    "        total_correct_emotions += correct_predictions_emotions\n",
    "        total_correct_causes += correct_predictions_causes\n",
    "        total_predictions += torch.sum(pad_mask).item()  # Batch size\n",
    "        \n",
    "    \n",
    "    emotion_lr_scheduler.step()\n",
    "    cause_lr_scheduler.step()\n",
    "    \n",
    "    emotion_model.eval()  # Set the model to evaluation mode\n",
    "    cause_model.eval()\n",
    "    \n",
    "    total_val_loss = 0.0\n",
    "    total_val_tokens = 0\n",
    "    total_val_correct_emotions = 0\n",
    "    total_val_correct_causes = 0\n",
    "    total_val_predictions = 0\n",
    "    true_labels_emotion = []\n",
    "    predicted_labels_emotion = []\n",
    "    true_labels_cause = []\n",
    "    predicted_labels_cause = []\n",
    "    padded_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(validation_dataloader):\n",
    "            audio = val_batch['audio'].to('cuda')\n",
    "            video = val_batch['video'].to('cuda')\n",
    "            text = val_batch['text'].to('cuda')\n",
    "            emotion_indices = val_batch['emotion_labels'].to('cuda')\n",
    "            cause_indices = val_batch['cause_labels'].to('cuda')\n",
    "            pad_mask = val_batch['pad_mask'].to('cuda')\n",
    "            \n",
    "            audio_copy = audio.clone().detach()\n",
    "            video_copy = video.clone().detach()\n",
    "            text_copy = text.clone().detach()\n",
    "\n",
    "            # emotion_logits = emotion_model(audio_copy, video_copy, text_copy)\n",
    "\n",
    "            # Reshape emotion_logits\n",
    "            # emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "            # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "            # emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "            pad_mask = pad_mask.view(-1)   \n",
    "\n",
    "            # Calculate the loss, excluding padded positions\n",
    "            val_loss = emotion_model.loss(audio_copy, video_copy, text_copy, emotion_indices, pad_mask)\n",
    "            masked_loss = val_loss #torch.sum(val_loss * pad_mask) / torch.sum(pad_mask)\n",
    "            \n",
    "            # cause_logits = cause_model(audio, video, text)\n",
    "            # cause_logits = cause_logits.view(-1, cause_logits.size(-1))\n",
    "            # cause_indices = cause_indices.view(-1)\n",
    "            cause_loss = cause_model.loss(audio, video, text, cause_indices, pad_mask)\n",
    "            masked_loss += cause_loss\n",
    "            \n",
    "            total_val_loss += masked_loss.item()\n",
    "            total_val_tokens += torch.sum(pad_mask).item()\n",
    "            \n",
    "            predicted_emotions_val = emotion_model.predict(audio, video, text)\n",
    "            correct_predictions_val = ((predicted_emotions_val == emotion_indices) * pad_mask).sum().item()\n",
    "            total_val_correct_emotions += correct_predictions_val\n",
    "            \n",
    "            predicted_causes_val = cause_model.predict(audio, video, text)\n",
    "            correct_predictions_causes_val = ((predicted_causes_val == cause_indices) * pad_mask).sum().item()\n",
    "            total_val_correct_causes += correct_predictions_causes_val\n",
    "            \n",
    "            total_val_predictions += torch.sum(pad_mask).item()\n",
    "\n",
    "            # Store true and predicted labels for F1 score calculation\n",
    "            emotion_indices = emotion_indices.cpu().squeeze(0).numpy()\n",
    "            true_labels_emotion.extend(emotion_indices)\n",
    "            predicted_labels_emotion.extend(predicted_emotions_val[0])\n",
    "            \n",
    "            \n",
    "            true_labels_cause.extend(cause_indices.cpu().squeeze(0).numpy())\n",
    "            predicted_labels_cause.extend(predicted_causes_val[0])\n",
    "            padded_labels.extend(pad_mask.cpu().numpy())\n",
    "    # print(\"AAAAAAAAAAAAAAAAAAAAAAAAaa\")\n",
    "    # print(len(final_true_labels_cause))\n",
    "    # print(len(final_predicted_labels_cause))\n",
    "    # print(\"AAAAAAAAAAAAAAAAAAAAAAAAaa\")\n",
    "    final_true_labels_emotion = [label for label, pad in zip(true_labels_emotion, padded_labels)]\n",
    "    final_predicted_labels_emotion = [label for label, pad in zip(predicted_labels_emotion, padded_labels)]\n",
    "    \n",
    "    final_true_labels_cause = [label for label, pad in zip(true_labels_cause, padded_labels)]\n",
    "    final_predicted_labels_cause = [label for label, pad in zip(predicted_labels_cause, padded_labels)]\n",
    "    \n",
    "    emotion_classification_rep = classification_report(final_true_labels_emotion, final_predicted_labels_emotion)\n",
    "    cause_classification_rep = classification_report(final_true_labels_cause, final_predicted_labels_cause)\n",
    "    \n",
    "    # Calculate and print the average loss for this epoch\n",
    "    # avg_loss = total_loss / total_tokens\n",
    "    # avg_val_loss = total_val_loss / total_val_tokens\n",
    "    \n",
    "    print(\"===============================\")\n",
    "    print(\"Training data metrics\")\n",
    "    # print(f\"Epoch [{epoch + 1}/{num_epochs}] Training Loss: {avg_loss}\")\n",
    "    # print(f\"Epoch [{epoch + 1}/{num_epochs}] Accuracy: {total_correct_emotions / total_predictions}\")\n",
    "    # print(f\"Epoch [{epoch + 1}/{num_epochs}] Accuracy: {total_correct_causes / total_predictions}\")\n",
    "    \n",
    "    print(\"VALIDATION METRICS\")\n",
    "    # print(f\"Epoch [{epoch + 1}/{num_epochs}] Validation Loss: {avg_val_loss}\")\n",
    "    print(emotion_classification_rep)\n",
    "    print(cause_classification_rep)\n",
    "    print(\"===============================\")\n",
    "\n",
    "    torch.save(emotion_model.state_dict(), f\"/tmp/semeval24_task3/final_models/emotion_models/emotion_model_{epoch:02}.pt\")\n",
    "    torch.save(cause_model.state_dict(), f\"/tmp/semeval24_task3/final_models/cause_models/cause_model_{epoch:02}.pt\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
