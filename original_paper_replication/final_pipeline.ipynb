{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c616ee6-0273-43a9-baad-a07c3bac3672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "DATA_DIR = \"/tmp/semeval24_task3/\"\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "TRAIN_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/training.json\"\n",
    "# TEST_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/testing.json\"\n",
    "TEST_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Evaluation_Data/Subtask_2_test.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ce159fc-758c-4bb2-860a-ea15885d1a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(TEST_DATA_FILEPATH) as f:\n",
    "    test_data = json.load(f)\n",
    "with open(TRAIN_DATA_FILEPATH) as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15715037-fba8-4cee-939c-9af7c8f25346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "VID_ID_MAPPING = np.load(\"/home2/suyash.mathur/semeval24/task3/MECPE/data/video_id_mapping.npy\", allow_pickle=True).item()\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        self.audio_embeddings = np.load(audio_embeddings_path)\n",
    "        # with open(audio_embeddings_path, \"rb\") as f:\n",
    "            # self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        audio_name = VID_ID_MAPPING[audio_name]\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    \n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        self.video_embeddings = np.load(video_embeddings_path)\n",
    "        # with open(video_embeddings_path, \"rb\") as f:\n",
    "        #     self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        video_name = video_name.split(\".\")[0]\n",
    "        video_name = VID_ID_MAPPING[video_name]\n",
    "        video_embedding = self.video_embeddings[video_name]\n",
    "        # video_embedding = video_embedding.reshape((16,-1))\n",
    "        # video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "734b7058-fa30-471e-8c71-9b88af2fd1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2059, 1024, 1472, 5282, 1647, 369, 326]\n"
     ]
    }
   ],
   "source": [
    "class EmotionIndexer:\n",
    "    def __init__(self):\n",
    "        self.emotion_to_index = {\n",
    "            'joy': 0,\n",
    "            'sadness': 1,\n",
    "            'anger': 2,\n",
    "            'neutral': 3,\n",
    "            'surprise': 4,\n",
    "            'disgust': 5,\n",
    "            'fear': 6,\n",
    "            'pad': 7,\n",
    "        }\n",
    "        self.emotion_freq = [0]*7\n",
    "        self.weights = None\n",
    "\n",
    "        self.index_to_emotion = {index: emotion for emotion, index in self.emotion_to_index.items()}\n",
    "\n",
    "    def emotion_to_idx(self, emotion):\n",
    "        return self.emotion_to_index.get(emotion, None)\n",
    "\n",
    "    def idx_to_emotion(self, index):\n",
    "        return self.index_to_emotion.get(index, None)\n",
    "\n",
    "    def compute_weights(self, data):\n",
    "        for conversation in data:\n",
    "            conversation = conversation['conversation']\n",
    "            for utterance in conversation:\n",
    "                emotion = utterance['emotion']\n",
    "                self.emotion_freq[self.emotion_to_index[emotion]] += 1\n",
    "        print(self.emotion_freq)\n",
    "        self.weights = [1/freq for freq in self.emotion_freq]\n",
    "\n",
    "# Example usage\n",
    "indexer = EmotionIndexer()\n",
    "indexer.compute_weights(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60972bfd-56a1-46cd-9587-2a06b4357891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1961a5a6-f3ce-4f37-9f18-f454ba29aa37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dia1375utt1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m# Example of iterating through batches\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m test_dataloader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m     audio \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Shape: (batch_size, max_seq_len, audio_embedding_size)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m     video \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Shape: (batch_size, max_seq_len, video_embedding_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/mindeye/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m video_paths \u001b[39m=\u001b[39m [utterance[\u001b[39m'\u001b[39m\u001b[39mvideo_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m conversation]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m texts \u001b[39m=\u001b[39m [utterance[\u001b[39m'\u001b[39m\u001b[39mvideo_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m conversation]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m audio_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_encoder\u001b[39m.\u001b[39mlmao(audio_path) \u001b[39mfor\u001b[39;00m audio_path \u001b[39min\u001b[39;00m audio_paths]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m video_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_encoder\u001b[39m.\u001b[39mlmao(video_path) \u001b[39mfor\u001b[39;00m video_path \u001b[39min\u001b[39;00m video_paths]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m text_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_encoder\u001b[39m.\u001b[39mlmao(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n",
      "\u001b[1;32m/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m video_paths \u001b[39m=\u001b[39m [utterance[\u001b[39m'\u001b[39m\u001b[39mvideo_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m conversation]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m texts \u001b[39m=\u001b[39m [utterance[\u001b[39m'\u001b[39m\u001b[39mvideo_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m conversation]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m audio_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_encoder\u001b[39m.\u001b[39;49mlmao(audio_path) \u001b[39mfor\u001b[39;00m audio_path \u001b[39min\u001b[39;00m audio_paths]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m video_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_encoder\u001b[39m.\u001b[39mlmao(video_path) \u001b[39mfor\u001b[39;00m video_path \u001b[39min\u001b[39;00m video_paths]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m text_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_encoder\u001b[39m.\u001b[39mlmao(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n",
      "\u001b[1;32m/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlmao\u001b[39m(\u001b[39mself\u001b[39m, audio_name):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     audio_name \u001b[39m=\u001b[39m audio_name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     audio_name \u001b[39m=\u001b[39m VID_ID_MAPPING[audio_name]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     audio_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_embeddings[audio_name]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mfrom_numpy(audio_embedding)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dia1375utt1'"
     ]
    }
   ],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, json_file, audio_encoder, video_encoder, text_encoder, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = self.load_data(json_file)\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def load_data(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.data[idx]['conversation']\n",
    "        emotion_labels = []\n",
    "        audio_paths = [utterance['video_name'].replace('mp4', 'wav') for utterance in conversation]\n",
    "        video_paths = [utterance['video_name'] for utterance in conversation]\n",
    "        texts = [utterance['video_name'] for utterance in conversation]\n",
    "\n",
    "        audio_embeddings = [self.audio_encoder.lmao(audio_path) for audio_path in audio_paths]\n",
    "        video_embeddings = [self.video_encoder.lmao(video_path) for video_path in video_paths]\n",
    "        text_embeddings = [self.text_encoder.lmao(text) for text in texts]\n",
    "        \n",
    "        cause_pairs = []\n",
    "        useful_utterances = set([int(cause_pair[1]) for cause_pair in cause_pairs])\n",
    "        cause_labels = []\n",
    "        for utterance in conversation:\n",
    "            if utterance['utterance_ID'] in useful_utterances:\n",
    "                cause_labels.append(1)\n",
    "            else:\n",
    "                cause_labels.append(0)\n",
    "\n",
    "        # Pad or truncate conversations to the maximum sequence length\n",
    "        if len(conversation) < self.max_seq_len:\n",
    "            pad_length = self.max_seq_len - len(conversation)\n",
    "            audio_embeddings += [torch.zeros_like(audio_embeddings[0])] * pad_length\n",
    "            video_embeddings += [torch.zeros_like(video_embeddings[0])] * pad_length\n",
    "            text_embeddings += [torch.zeros_like(text_embeddings[0])] * pad_length\n",
    "            emotion_labels += ['pad'] * pad_length\n",
    "            cause_labels += [-1] * pad_length\n",
    "            pad_mask = [1] * len(conversation) + [0] * pad_length\n",
    "        else:\n",
    "            audio_embeddings = audio_embeddings[:self.max_seq_len]\n",
    "            video_embeddings = video_embeddings[:self.max_seq_len]\n",
    "            text_embeddings = text_embeddings[:self.max_seq_len]\n",
    "            emotion_labels = emotion_labels[:self.max_seq_len]\n",
    "            cause_labels = cause_labels[:self.max_seq_len]\n",
    "            pad_mask = [1] * self.max_seq_len\n",
    "\n",
    "        emotion_indices = [indexer.emotion_to_idx(emotion) for emotion in emotion_labels]\n",
    "        \n",
    "        audio_embeddings = torch.stack(audio_embeddings)\n",
    "        video_embeddings = torch.stack(video_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        emotion_indices = torch.from_numpy(np.array(emotion_indices))\n",
    "        pad_mask = torch.from_numpy(np.array(pad_mask))\n",
    "        cause_labels = torch.from_numpy(np.array(cause_labels))\n",
    "        \n",
    "        return {\n",
    "            'audio': audio_embeddings,\n",
    "            'video': video_embeddings,\n",
    "            'text': text_embeddings,\n",
    "            # 'conversation_id': \n",
    "        }\n",
    "# Example usage\n",
    "# You need to define your audio, video, and text encoders accordingly\n",
    "\n",
    "# Define your data paths\n",
    "AUDIO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/audio_embedding_6373.npy\"\n",
    "VIDEO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\"\n",
    "TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "max_seq_len = 35  # Adjust this according to your needs\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "test_dataset = ConversationDataset(TEST_DATA_FILEPATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Example of iterating through batches\n",
    "for batch in test_dataloader:\n",
    "    audio = batch['audio']  # Shape: (batch_size, max_seq_len, audio_embedding_size)\n",
    "    video = batch['video']  # Shape: (batch_size, max_seq_len, video_embedding_size)\n",
    "    text = batch['text']    # Shape: (batch_size, max_seq_len, text_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionClassifierOG(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "        super(EmotionClassifierOG, self).__init__()\n",
    "        \n",
    "        # self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "        # self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "        # self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "        # self.relu = nn.ReLU()\n",
    "        \n",
    "        # self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "        # Replace Transformer with BiLSTM\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "                              dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "        # Concatenate or combine the audio, video, and text encodings\n",
    "        audio_encoding = audio_encoding.float()\n",
    "        video_encoding = video_encoding.float()\n",
    "        text_encoding = text_encoding.float()\n",
    "        \n",
    "        # audio_encoding = self.audio_dropout(audio_encoding)\n",
    "        # video_encoding = self.video_dropout(video_encoding)\n",
    "        # text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "        combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "        combined_encoding = self.first_linear(combined_encoding)\n",
    "        # combined_encoding = self.relu(combined_encoding)\n",
    "        # combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "        # Take the output of the BiLSTM\n",
    "        emotion_logits = self.linear(lstm_output)\n",
    "        # Apply a softmax layer\n",
    "        emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "        return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76c585d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "        # Replace Transformer with BiLSTM\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "                              dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "        # Concatenate or combine the audio, video, and text encodings\n",
    "        audio_encoding = audio_encoding.float()\n",
    "        video_encoding = video_encoding.float()\n",
    "        text_encoding = text_encoding.float()\n",
    "        \n",
    "        audio_encoding = self.audio_dropout(audio_encoding)\n",
    "        video_encoding = self.video_dropout(video_encoding)\n",
    "        text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "        combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "        combined_encoding = self.first_linear(combined_encoding)\n",
    "        combined_encoding = self.relu(combined_encoding)\n",
    "        combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "        # Take the output of the BiLSTM\n",
    "        emotion_logits = self.linear(lstm_output)\n",
    "        # Apply a softmax layer\n",
    "        emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "        return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5f9fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_positional_embeddings(dimension, count):\n",
    "    embeddings = [list(np.zeros(dimension))]\n",
    "    embeddings.extend([\n",
    "        list(np.random.normal(loc=0.0, scale=0.1, size=dimension)) for _ in range(count)\n",
    "    ])\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8327df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionCauseDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        utterance_embedding_size,\n",
    "        device,\n",
    "        hidden_dimension=4096,\n",
    "        positional_embeddings_dimension=200,\n",
    "        dropout=0.2,\n",
    "        *args, **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "        positional_embeddings = generate_positional_embeddings(positional_embeddings_dimension, 200)\n",
    "        self.positional_embeddings = torch.from_numpy(positional_embeddings).to(device).float()\n",
    "        \n",
    "        self.non_neutral_dropout = nn.Dropout(dropout)\n",
    "        self.candidate_cause_dropout = nn.Dropout(dropout)\n",
    "        self.distance_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(utterance_embedding_size*2 + positional_embeddings_dimension, hidden_dimension)\n",
    "        self.linear1_activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dimension, 1)\n",
    "\n",
    "    def forward(self, non_neutral_utterances, candidate_cause_utterances, distances):\n",
    "        positional_embedding = self.positional_embeddings[distances].float()\n",
    "        \n",
    "        non_neutral_utterances = self.non_neutral_dropout(non_neutral_utterances)\n",
    "        candidate_cause_utterances = self.candidate_cause_dropout(candidate_cause_utterances)\n",
    "        positional_embedding = self.distance_dropout(positional_embedding)\n",
    "\n",
    "        embeddings = torch.concat((non_neutral_utterances, candidate_cause_utterances, positional_embedding), axis=1).float()\n",
    "\n",
    "        return self.linear2(\n",
    "            self.linear1_activation(\n",
    "                self.linear1(embeddings)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2922c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/665 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dia1375utt1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m video_paths \u001b[39m=\u001b[39m [utterance[\u001b[39m'\u001b[39m\u001b[39mvideo_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m conversation[\u001b[39m'\u001b[39m\u001b[39mconversation\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m texts \u001b[39m=\u001b[39m [utterance[\u001b[39m'\u001b[39m\u001b[39mvideo_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m conversation[\u001b[39m'\u001b[39m\u001b[39mconversation\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m audio_embeddings \u001b[39m=\u001b[39m [audio_encoder\u001b[39m.\u001b[39mlmao(audio_path) \u001b[39mfor\u001b[39;00m audio_path \u001b[39min\u001b[39;00m audio_paths]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m video_embeddings \u001b[39m=\u001b[39m [video_encoder\u001b[39m.\u001b[39mlmao(video_path) \u001b[39mfor\u001b[39;00m video_path \u001b[39min\u001b[39;00m video_paths]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m text_embeddings \u001b[39m=\u001b[39m [text_encoder\u001b[39m.\u001b[39mlmao(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n",
      "\u001b[1;32m/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m video_paths \u001b[39m=\u001b[39m [utterance[\u001b[39m'\u001b[39m\u001b[39mvideo_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m conversation[\u001b[39m'\u001b[39m\u001b[39mconversation\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m texts \u001b[39m=\u001b[39m [utterance[\u001b[39m'\u001b[39m\u001b[39mvideo_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m conversation[\u001b[39m'\u001b[39m\u001b[39mconversation\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m audio_embeddings \u001b[39m=\u001b[39m [audio_encoder\u001b[39m.\u001b[39;49mlmao(audio_path) \u001b[39mfor\u001b[39;00m audio_path \u001b[39min\u001b[39;00m audio_paths]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m video_embeddings \u001b[39m=\u001b[39m [video_encoder\u001b[39m.\u001b[39mlmao(video_path) \u001b[39mfor\u001b[39;00m video_path \u001b[39min\u001b[39;00m video_paths]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m text_embeddings \u001b[39m=\u001b[39m [text_encoder\u001b[39m.\u001b[39mlmao(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n",
      "\u001b[1;32m/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlmao\u001b[39m(\u001b[39mself\u001b[39m, audio_name):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     audio_name \u001b[39m=\u001b[39m audio_name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     audio_name \u001b[39m=\u001b[39m VID_ID_MAPPING[audio_name]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     audio_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_embeddings[audio_name]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsuyash-gnode/home2/suyash.mathur/semeval24/task3/original_paper_replication/final_pipeline.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mfrom_numpy(audio_embedding)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dia1375utt1'"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "emotion_classifier = EmotionClassifier(input_size=768+4096+6373, hidden_size=2000, num_layers=3, dropout=0.6, num_emotions=7)\n",
    "emotion_classifier.load_state_dict(torch.load('/tmp/semeval24_task3/final_models/emotion_models/best_model.pt'))\n",
    "\n",
    "cause_classifier = EmotionClassifier(input_size=768+4096+6373, hidden_size=2000, num_layers=2, dropout=0.6, num_emotions=2)\n",
    "cause_classifier.load_state_dict(torch.load('/tmp/semeval24_task3/final_models/cause_models/best_cause_model.pt'))\n",
    "\n",
    "emotion_cause_detector = EmotionCauseDetector(utterance_embedding_size=768+4096+6373, device='cuda:1', hidden_dimension=2000)\n",
    "emotion_cause_detector.load_state_dict(torch.load('/tmp/semeval24_task3/final_models/pairing_models/paring_model_best_model.pt'))\n",
    "\n",
    "emotion_classifier.to('cuda:1')\n",
    "cause_classifier.to('cuda:1')\n",
    "emotion_cause_detector.to('cuda:1')\n",
    "positional_embeddings = np.load('/tmp/semeval24_task3/final_models/pairing_models/pairing_model_pos_embeds_best_model.npy')\n",
    "emotion_cause_detector.positional_embeddings = torch.from_numpy(positional_embeddings).to('cuda:1').float()\n",
    "\n",
    "emotion_cause_pairs = defaultdict(list)\n",
    "\n",
    "for conversation_idx, conversation in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    conversation_id = conversation['conversation_ID']\n",
    "    \n",
    "    audio_paths = [utterance['video_name'].replace('mp4', 'wav') for utterance in conversation['conversation']]\n",
    "    video_paths = [utterance['video_name'] for utterance in conversation['conversation']]\n",
    "    texts = [utterance['video_name'] for utterance in conversation['conversation']]\n",
    "    \n",
    "    audio_embeddings = [audio_encoder.lmao(audio_path) for audio_path in audio_paths]\n",
    "    video_embeddings = [video_encoder.lmao(video_path) for video_path in video_paths]\n",
    "    text_embeddings = [text_encoder.lmao(text) for text in texts]\n",
    "\n",
    "    audio_embeddings = torch.stack(audio_embeddings)\n",
    "    video_embeddings = torch.stack(video_embeddings)\n",
    "    text_embeddings = torch.stack(text_embeddings)\n",
    "    \n",
    "    audio_embeddings = audio_embeddings.unsqueeze(0).to('cuda:1')\n",
    "    video_embeddings = video_embeddings.unsqueeze(0).to('cuda:1')\n",
    "    text_embeddings = text_embeddings.unsqueeze(0).to('cuda:1')\n",
    "    \n",
    "    emotion_logits = emotion_classifier(audio_embeddings, video_embeddings, text_embeddings)\n",
    "    emotion_logits = emotion_logits.squeeze(0)\n",
    "    predicted_emotions = torch.argmax(emotion_logits, dim=1)\n",
    "    \n",
    "    candidate_utterances = [(idx, emotion.cpu().item()) for idx, emotion in enumerate(predicted_emotions) if emotion != 3]\n",
    "    \n",
    "    cause_logits = cause_classifier(audio_embeddings, video_embeddings, text_embeddings)\n",
    "    cause_logits = cause_logits.squeeze(0)\n",
    "    predicted_causes = torch.argmax(cause_logits, dim=1)\n",
    "    \n",
    "    candidate_causes = [idx for idx, cause in enumerate(predicted_causes) if cause == 1]\n",
    "    \n",
    "    for candidate_utterance in candidate_utterances:\n",
    "        for candidate_cause in candidate_causes:\n",
    "            utterance_embedding = torch.cat((audio_embeddings[0][candidate_utterance[0]], video_embeddings[0][candidate_utterance[0]], text_embeddings[0][candidate_utterance[0]]), dim=0)\n",
    "            cause_embedding = torch.cat((audio_embeddings[0][candidate_cause], video_embeddings[0][candidate_cause], text_embeddings[0][candidate_cause]), dim=0)\n",
    "            distance = abs(candidate_utterance[0] - candidate_cause)\n",
    "            \n",
    "            utterance_embedding = utterance_embedding.unsqueeze(0).to('cuda:1')\n",
    "            cause_embedding = cause_embedding.unsqueeze(0).to('cuda:1')\n",
    "            distance = torch.tensor([distance]).to('cuda:1')\n",
    "            prediction = emotion_cause_detector(utterance_embedding, cause_embedding, distance)\n",
    "            prediction = torch.sigmoid(prediction)\n",
    "            prediction = prediction.cpu().item()\n",
    "            if prediction >= 0.5:\n",
    "                emotion_cause_pairs[conversation_id].append((f\"{candidate_utterance[0]+1}_{indexer.idx_to_emotion(candidate_utterance[1])}\", f\"{candidate_cause+1}\"))\n",
    "    test_data[conversation_idx]['emotion-cause_pairs'] = emotion_cause_pairs[conversation_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8b8924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Validation_prediction.json', 'w') as f:\n",
    "    json.dump(test_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7ad89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: Subtask_2_pred.json (deflated 91%)\n"
     ]
    }
   ],
   "source": [
    "!zip msurfer_submission.zip Subtask_2_pred.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/suyash.mathur/semeval24/task3/MECPE\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
