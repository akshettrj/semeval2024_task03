{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "TRAIN_FILE_PATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/training.json\"\n",
    "VALIDATION_FILE_PATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/testing.json\"\n",
    "with open(TRAIN_FILE_PATH) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(VALIDATION_FILE_PATH) as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "pprint(len(train_data))\n",
    "pprint(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f87e26221b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "import torch\n",
    "\n",
    "numpy.random.seed(69)\n",
    "random.seed(69)\n",
    "torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disgust', 'neutral', 'anger', 'sadness', 'surprise', 'joy', 'fear'}\n"
     ]
    }
   ],
   "source": [
    "all_emotions = set()\n",
    "for conversation in train_data:\n",
    "    conversation = conversation[\"conversation\"]\n",
    "    for utterance in conversation:\n",
    "        all_emotions.add(utterance[\"emotion\"])\n",
    "pprint(all_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionIndexer:\n",
    "    def __init__(self):\n",
    "        self.emotion_to_index = {\n",
    "            'joy': 0,\n",
    "            'sadness': 1,\n",
    "            'anger': 2,\n",
    "            'neutral': 3,\n",
    "            'surprise': 4,\n",
    "            'disgust': 5,\n",
    "            'fear': 6,\n",
    "        }\n",
    "\n",
    "        self.index_to_emotion = {index: emotion for emotion, index in self.emotion_to_index.items()}\n",
    "\n",
    "    def emotion_to_idx(self, emotion):\n",
    "        return self.emotion_to_index.get(emotion, None)\n",
    "\n",
    "    def idx_to_emotion(self, index):\n",
    "        return self.index_to_emotion.get(index, None)\n",
    "\n",
    "# Example usage\n",
    "indexer = EmotionIndexer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "VID_ID_MAPPING = np.load(\"/home2/suyash.mathur/semeval24/task3/MECPE/data/video_id_mapping.npy\", allow_pickle=True).item()\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        self.audio_embeddings = np.load(audio_embeddings_path)\n",
    "        # with open(audio_embeddings_path, \"rb\") as f:\n",
    "            # self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        audio_name = VID_ID_MAPPING[audio_name]\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    \n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        self.video_embeddings = np.load(video_embeddings_path)\n",
    "        # with open(video_embeddings_path, \"rb\") as f:\n",
    "        #     self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        video_name = video_name.split(\".\")[0]\n",
    "        video_name = VID_ID_MAPPING[video_name]\n",
    "        video_embedding = self.video_embeddings[video_name]\n",
    "        # video_embedding = video_embedding.reshape((16,-1))\n",
    "        # video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, json_file, audio_encoder, video_encoder, text_encoder, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = self.load_data(json_file)\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def load_data(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.data[idx]['conversation']\n",
    "        # emotion_labels = [utterance['emotion'] for utterance in conversation]\n",
    "        audio_paths = [utterance['video_name'].replace('mp4', 'wav') for utterance in conversation]\n",
    "        video_paths = [utterance['video_name'] for utterance in conversation]\n",
    "        texts = [utterance['video_name'] for utterance in conversation]\n",
    "\n",
    "        audio_embeddings = [self.audio_encoder.lmao(audio_path) for audio_path in audio_paths]\n",
    "        video_embeddings = [self.video_encoder.lmao(video_path) for video_path in video_paths]\n",
    "        text_embeddings = [self.text_encoder.lmao(text) for text in texts]\n",
    "\n",
    "        cause_pairs = self.data[idx]['emotion-cause_pairs']\n",
    "        useful_utterances = set([int(cause_pair[1]) for cause_pair in cause_pairs])\n",
    "        cause_labels = []\n",
    "        for utterance in conversation:\n",
    "            if utterance['utterance_ID'] in useful_utterances:\n",
    "                cause_labels.append(1)\n",
    "            else:\n",
    "                cause_labels.append(0)\n",
    "        \n",
    "        # Pad or truncate conversations to the maximum sequence length\n",
    "        if len(conversation) < self.max_seq_len:\n",
    "            pad_length = self.max_seq_len - len(conversation)\n",
    "            audio_embeddings += [torch.zeros_like(audio_embeddings[0])] * pad_length\n",
    "            video_embeddings += [torch.zeros_like(video_embeddings[0])] * pad_length\n",
    "            text_embeddings += [torch.zeros_like(text_embeddings[0])] * pad_length\n",
    "            cause_labels += [-1] * pad_length\n",
    "            pad_mask = [1] * len(conversation) + [0] * pad_length\n",
    "        else:\n",
    "            audio_embeddings = audio_embeddings[:self.max_seq_len]\n",
    "            video_embeddings = video_embeddings[:self.max_seq_len]\n",
    "            text_embeddings = text_embeddings[:self.max_seq_len]\n",
    "            cause_labels = cause_labels[:self.max_seq_len]\n",
    "            pad_mask = [1] * self.max_seq_len\n",
    "\n",
    "        audio_embeddings = torch.stack(audio_embeddings)\n",
    "        video_embeddings = torch.stack(video_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        cause_labels = torch.from_numpy(np.array(cause_labels))\n",
    "        pad_mask = torch.from_numpy(np.array(pad_mask))\n",
    "        \n",
    "        return {\n",
    "            'audio': audio_embeddings,\n",
    "            'video': video_embeddings,\n",
    "            'text': text_embeddings,\n",
    "            'cause_labels': cause_labels,\n",
    "            'pad_mask': pad_mask\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "# You need to define your audio, video, and text encoders accordingly\n",
    "\n",
    "# Define your data paths\n",
    "DATA_DIR = \"/tmp/semeval24_task3\"\n",
    "\n",
    "AUDIO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/audio_embedding_6373.npy\"\n",
    "VIDEO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\"\n",
    "TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "max_seq_len = 35  # Adjust this according to your needs\n",
    "\n",
    "train_dataset = ConversationDataset(TRAIN_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "validation_dataset = ConversationDataset(VALIDATION_FILE_PATH, audio_encoder, video_encoder, text_encoder, max_seq_len)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=True)\n",
    "# Example of iterating through batches\n",
    "# for batch in dataloader:\n",
    "#     audio = batch['audio']  # Shape: (batch_size, max_seq_len, audio_embedding_size)\n",
    "#     video = batch['video']  # Shape: (batch_size, max_seq_len, video_embedding_size)\n",
    "#     text = batch['text']    # Shape: (batch_size, max_seq_len, text_embedding_size)\n",
    "#     cause_labels = batch['cause_labels']  # List of emotion labels for each utterance in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# class EmotionClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout, num_emotions):\n",
    "#         super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "#         self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "\n",
    "#         self.transformer_encoder = TransformerEncoder(\n",
    "#             TransformerEncoderLayer(hidden_size, num_heads, hidden_size, dropout),\n",
    "#             num_layers\n",
    "#         )\n",
    "        \n",
    "#         self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "#     def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "\n",
    "#         # Concatenate or combine the audio, video, and text encodings here\n",
    "#         # You can use any method like concatenation, addition, or other fusion techniques\n",
    "#         # Combine the encodings (you can customize this part)\n",
    "#         audio_encoding = audio_encoding.float()\n",
    "#         video_encoding = video_encoding.float()\n",
    "#         text_encoding = text_encoding.float().squeeze()\n",
    "#         combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "#         combined_encoding = self.first_linear(combined_encoding)\n",
    "        \n",
    "        \n",
    "#         combined_encoding = combined_encoding.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, input_size)\n",
    "        \n",
    "        \n",
    "#         transformer_output = self.transformer_encoder(combined_encoding)\n",
    "\n",
    "#         # Take the output of the Transformer encoder for the last position as the summary\n",
    "#         emotion_logits = self.linear(transformer_output.permute(1, 0, 2))\n",
    "\n",
    "#         return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, num_emotions, embedding_dropout=0.2):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        self.audio_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.video_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.text_dropout = nn.Dropout(embedding_dropout)\n",
    "\n",
    "        self.first_linear = nn.Linear(input_size, hidden_size, dtype=torch.float32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.second_linear_layer = nn.Linear(hidden_size, hidden_size, dtype=torch.float32)\n",
    "        # Replace Transformer with BiLSTM\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "                              dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, audio_encoding, video_encoding, text_encoding):\n",
    "        # Concatenate or combine the audio, video, and text encodings\n",
    "        audio_encoding = audio_encoding.float()\n",
    "        video_encoding = video_encoding.float()\n",
    "        text_encoding = text_encoding.float().squeeze()\n",
    "        \n",
    "        audio_encoding = self.audio_dropout(audio_encoding)\n",
    "        video_encoding = self.video_dropout(video_encoding)\n",
    "        text_encoding = self.text_dropout(text_encoding)\n",
    "        \n",
    "        combined_encoding = torch.cat((audio_encoding, video_encoding, text_encoding), dim=2)\n",
    "        \n",
    "        combined_encoding = self.first_linear(combined_encoding)\n",
    "        combined_encoding = self.relu(combined_encoding)\n",
    "        combined_encoding = self.second_linear_layer(combined_encoding)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_output, _ = self.bilstm(combined_encoding)\n",
    "\n",
    "        # Take the output of the BiLSTM\n",
    "        emotion_logits = self.linear(lstm_output)\n",
    "        # Apply a softmax layer\n",
    "        emotion_logits = torch.softmax(emotion_logits, dim=2)\n",
    "\n",
    "        return emotion_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.11it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25.47it/s]\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20] Training Loss: 0.0044337513656469\n",
      "Epoch [0/20] Validation Loss: 0.00431380093925529\n",
      "Epoch [0/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       759\n",
      "           1       0.00      0.00      0.00       681\n",
      "\n",
      "    accuracy                           0.53      1440\n",
      "   macro avg       0.26      0.50      0.35      1440\n",
      "weighted avg       0.28      0.53      0.36      1440\n",
      "\n",
      "Epoch [0/20] Accuracy: 0.5185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.58it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Training Loss: 0.004426448061720903\n",
      "Epoch [1/20] Validation Loss: 0.004308859548634953\n",
      "Epoch [1/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       759\n",
      "           1       0.77      0.01      0.03       681\n",
      "\n",
      "    accuracy                           0.53      1440\n",
      "   macro avg       0.65      0.51      0.36      1440\n",
      "weighted avg       0.64      0.53      0.38      1440\n",
      "\n",
      "Epoch [1/20] Accuracy: 0.5196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.33it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Training Loss: 0.004419325989390332\n",
      "Epoch [2/20] Validation Loss: 0.004298492148518562\n",
      "Epoch [2/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.89      0.67       759\n",
      "           1       0.51      0.13      0.20       681\n",
      "\n",
      "    accuracy                           0.53      1440\n",
      "   macro avg       0.52      0.51      0.43      1440\n",
      "weighted avg       0.52      0.53      0.45      1440\n",
      "\n",
      "Epoch [2/20] Accuracy: 0.5305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.28it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Training Loss: 0.004414604302158946\n",
      "Epoch [3/20] Validation Loss: 0.004297765054636532\n",
      "Epoch [3/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.60      0.58       759\n",
      "           1       0.51      0.46      0.49       681\n",
      "\n",
      "    accuracy                           0.53      1440\n",
      "   macro avg       0.53      0.53      0.53      1440\n",
      "weighted avg       0.53      0.53      0.53      1440\n",
      "\n",
      "Epoch [3/20] Accuracy: 0.5269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.10it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Training Loss: 0.004412601959999975\n",
      "Epoch [4/20] Validation Loss: 0.0042985058079163235\n",
      "Epoch [4/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.86      0.66       759\n",
      "           1       0.51      0.17      0.25       681\n",
      "\n",
      "    accuracy                           0.53      1440\n",
      "   macro avg       0.52      0.51      0.46      1440\n",
      "weighted avg       0.52      0.53      0.47      1440\n",
      "\n",
      "Epoch [4/20] Accuracy: 0.5326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.29it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 24.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Training Loss: 0.004406435392360811\n",
      "Epoch [5/20] Validation Loss: 0.004291289962000317\n",
      "Epoch [5/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.77      0.63       759\n",
      "           1       0.50      0.26      0.34       681\n",
      "\n",
      "    accuracy                           0.53      1440\n",
      "   macro avg       0.52      0.52      0.49      1440\n",
      "weighted avg       0.52      0.53      0.50      1440\n",
      "\n",
      "Epoch [5/20] Accuracy: 0.5362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.25it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Training Loss: 0.0044086536796175795\n",
      "Epoch [6/20] Validation Loss: 0.00429106532699532\n",
      "Epoch [6/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.84      0.65       759\n",
      "           1       0.51      0.19      0.27       681\n",
      "\n",
      "    accuracy                           0.53      1440\n",
      "   macro avg       0.52      0.51      0.46      1440\n",
      "weighted avg       0.52      0.53      0.47      1440\n",
      "\n",
      "Epoch [6/20] Accuracy: 0.5319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.25it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Training Loss: 0.004407961514011981\n",
      "Epoch [7/20] Validation Loss: 0.004280894663598802\n",
      "Epoch [7/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.82      0.65       759\n",
      "           1       0.52      0.22      0.31       681\n",
      "\n",
      "    accuracy                           0.54      1440\n",
      "   macro avg       0.53      0.52      0.48      1440\n",
      "weighted avg       0.53      0.54      0.49      1440\n",
      "\n",
      "Epoch [7/20] Accuracy: 0.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.28it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Training Loss: 0.004407614520915731\n",
      "Epoch [8/20] Validation Loss: 0.004278525213400523\n",
      "Epoch [8/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.72      0.62       759\n",
      "           1       0.52      0.33      0.41       681\n",
      "\n",
      "    accuracy                           0.54      1440\n",
      "   macro avg       0.53      0.53      0.52      1440\n",
      "weighted avg       0.53      0.54      0.52      1440\n",
      "\n",
      "Epoch [8/20] Accuracy: 0.5321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.45it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Training Loss: 0.004400316913664874\n",
      "Epoch [9/20] Validation Loss: 0.0042796472708384195\n",
      "Epoch [9/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.76      0.64       759\n",
      "           1       0.54      0.31      0.39       681\n",
      "\n",
      "    accuracy                           0.55      1440\n",
      "   macro avg       0.54      0.54      0.52      1440\n",
      "weighted avg       0.55      0.55      0.52      1440\n",
      "\n",
      "Epoch [9/20] Accuracy: 0.5396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.52it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Training Loss: 0.004405648198970698\n",
      "Epoch [10/20] Validation Loss: 0.004277430888679292\n",
      "Epoch [10/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.63       759\n",
      "           1       0.52      0.30      0.38       681\n",
      "\n",
      "    accuracy                           0.54      1440\n",
      "   macro avg       0.54      0.53      0.51      1440\n",
      "weighted avg       0.54      0.54      0.51      1440\n",
      "\n",
      "Epoch [10/20] Accuracy: 0.5351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.22it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Training Loss: 0.004397279504230529\n",
      "Epoch [11/20] Validation Loss: 0.0042754204322894415\n",
      "Epoch [11/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.71      0.62       759\n",
      "           1       0.53      0.37      0.44       681\n",
      "\n",
      "    accuracy                           0.55      1440\n",
      "   macro avg       0.55      0.54      0.53      1440\n",
      "weighted avg       0.55      0.55      0.54      1440\n",
      "\n",
      "Epoch [11/20] Accuracy: 0.5393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 13.82it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 24.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Training Loss: 0.004402815037852722\n",
      "Epoch [12/20] Validation Loss: 0.004283147801955541\n",
      "Epoch [12/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.87      0.66       759\n",
      "           1       0.51      0.14      0.22       681\n",
      "\n",
      "    accuracy                           0.53      1440\n",
      "   macro avg       0.52      0.51      0.44      1440\n",
      "weighted avg       0.52      0.53      0.45      1440\n",
      "\n",
      "Epoch [12/20] Accuracy: 0.5369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.11it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 24.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Training Loss: 0.004401442838993475\n",
      "Epoch [13/20] Validation Loss: 0.004272626837094625\n",
      "Epoch [13/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.69      0.62       759\n",
      "           1       0.53      0.39      0.45       681\n",
      "\n",
      "    accuracy                           0.55      1440\n",
      "   macro avg       0.54      0.54      0.53      1440\n",
      "weighted avg       0.54      0.55      0.54      1440\n",
      "\n",
      "Epoch [13/20] Accuracy: 0.5352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.10it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Training Loss: 0.004402764183770789\n",
      "Epoch [14/20] Validation Loss: 0.004277354809972975\n",
      "Epoch [14/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.77      0.64       759\n",
      "           1       0.54      0.31      0.39       681\n",
      "\n",
      "    accuracy                           0.55      1440\n",
      "   macro avg       0.55      0.54      0.52      1440\n",
      "weighted avg       0.55      0.55      0.52      1440\n",
      "\n",
      "Epoch [14/20] Accuracy: 0.5359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.43it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Training Loss: 0.004397477155366411\n",
      "Epoch [15/20] Validation Loss: 0.004277699730462498\n",
      "Epoch [15/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.77      0.64       759\n",
      "           1       0.53      0.30      0.38       681\n",
      "\n",
      "    accuracy                           0.55      1440\n",
      "   macro avg       0.54      0.53      0.51      1440\n",
      "weighted avg       0.54      0.55      0.52      1440\n",
      "\n",
      "Epoch [15/20] Accuracy: 0.5324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.31it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Training Loss: 0.004402274695491251\n",
      "Epoch [16/20] Validation Loss: 0.004279490229156282\n",
      "Epoch [16/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.61      0.58       759\n",
      "           1       0.52      0.46      0.49       681\n",
      "\n",
      "    accuracy                           0.54      1440\n",
      "   macro avg       0.54      0.54      0.54      1440\n",
      "weighted avg       0.54      0.54      0.54      1440\n",
      "\n",
      "Epoch [16/20] Accuracy: 0.5398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.35it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 28.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Training Loss: 0.004389790040399947\n",
      "Epoch [17/20] Validation Loss: 0.004274863004684448\n",
      "Epoch [17/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.72      0.62       759\n",
      "           1       0.52      0.35      0.42       681\n",
      "\n",
      "    accuracy                           0.54      1440\n",
      "   macro avg       0.54      0.53      0.52      1440\n",
      "weighted avg       0.54      0.54      0.52      1440\n",
      "\n",
      "Epoch [17/20] Accuracy: 0.5471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.72it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Training Loss: 0.0043908991155116196\n",
      "Epoch [18/20] Validation Loss: 0.004279294858376185\n",
      "Epoch [18/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.67      0.61       759\n",
      "           1       0.52      0.41      0.46       681\n",
      "\n",
      "    accuracy                           0.55      1440\n",
      "   macro avg       0.54      0.54      0.53      1440\n",
      "weighted avg       0.54      0.55      0.54      1440\n",
      "\n",
      "Epoch [18/20] Accuracy: 0.5434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:05<00:00, 14.17it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 26.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Training Loss: 0.004391755711436223\n",
      "Epoch [19/20] Validation Loss: 0.00427369914121098\n",
      "Epoch [19/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57       759\n",
      "           1       0.52      0.52      0.52       681\n",
      "\n",
      "    accuracy                           0.55      1440\n",
      "   macro avg       0.55      0.55      0.55      1440\n",
      "weighted avg       0.55      0.55      0.55      1440\n",
      "\n",
      "Epoch [19/20] Accuracy: 0.5450\n",
      "Training complete!\n",
      "=======================================\n",
      "BEST MODEL\n",
      "Best epoch: 13\n",
      "Best validation loss: 0.004272626837094625\n",
      "Best classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.69      0.62       759\n",
      "           1       0.53      0.39      0.45       681\n",
      "\n",
      "    accuracy                           0.55      1440\n",
      "   macro avg       0.54      0.54      0.53      1440\n",
      "weighted avg       0.54      0.55      0.54      1440\n",
      "\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Define your model\n",
    "model = EmotionClassifier(input_size=768+6373+4096, hidden_size=2000, num_layers=2, dropout=0.6, num_emotions=2)\n",
    "model.to(\"cuda:1\")\n",
    "\n",
    "num_epochs = 20\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = AdamW(model.parameters(), lr=0.5*1e-5)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Define training parameters\n",
    "\n",
    "# Training loop\n",
    "best_model_file = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "best_classification_report = None\n",
    "\n",
    "for epoch in (range(num_epochs)):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):  # Assuming you have a DataLoader for your dataset\n",
    "        # Extract data from the batch\n",
    "        audio = batch['audio'].to('cuda:1')\n",
    "        video = batch['video'].to('cuda:1')\n",
    "        text = batch['text'].to('cuda:1')\n",
    "        emotion_indices = batch['cause_labels'].to('cuda:1')\n",
    "        pad_mask = batch['pad_mask'].to('cuda:1')\n",
    "\n",
    "        # Forward pass\n",
    "        emotion_logits = model(audio, video, text)\n",
    "\n",
    "        # Reshape emotion_logits\n",
    "        emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "        # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "        emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "        # Calculate a mask to exclude padded positions from the loss\n",
    "        pad_mask = pad_mask.view(-1)     \n",
    "\n",
    "        # Calculate the loss, excluding padded positions\n",
    "        loss = criterion(emotion_logits, emotion_indices)\n",
    "        # masked_loss = torch.sum(loss * pad_mask) / torch.sum(pad_mask)\n",
    "        masked_loss = loss\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        masked_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += masked_loss.item()\n",
    "        total_tokens += torch.sum(pad_mask).item()\n",
    "        \n",
    "        predicted_emotions = torch.argmax(emotion_logits, dim=1)\n",
    "        correct_predictions = ((predicted_emotions == emotion_indices) * pad_mask).sum().item()\n",
    "\n",
    "        total_correct += correct_predictions\n",
    "        total_predictions += torch.sum(pad_mask).item()  # Batch size\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    total_val_tokens = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_predictions = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    padded_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(validation_dataloader):\n",
    "            audio = val_batch['audio'].to('cuda:1')\n",
    "            video = val_batch['video'].to('cuda:1')\n",
    "            text = val_batch['text'].to('cuda:1')\n",
    "            emotion_indices = val_batch['cause_labels'].to('cuda:1')\n",
    "            pad_mask = val_batch['pad_mask'].to('cuda:1')\n",
    "\n",
    "            emotion_logits = model(audio, video, text)\n",
    "\n",
    "            # Reshape emotion_logits\n",
    "            emotion_logits = emotion_logits.view(-1, emotion_logits.size(-1))\n",
    "\n",
    "            # Flatten emotion_indices (assuming it's a 2D tensor with shape [batch_size, max_sequence_length])\n",
    "            emotion_indices = emotion_indices.view(-1)\n",
    "\n",
    "            pad_mask = pad_mask.view(-1)   \n",
    "\n",
    "            # Calculate the loss, excluding padded positions\n",
    "            val_loss = criterion(emotion_logits, emotion_indices)\n",
    "            masked_loss = torch.sum(val_loss * pad_mask) / torch.sum(pad_mask)\n",
    "            \n",
    "            total_val_loss += masked_loss.item()\n",
    "            total_val_tokens += torch.sum(pad_mask).item()\n",
    "            \n",
    "            predicted_emotions_val = torch.argmax(emotion_logits, dim=1)\n",
    "            correct_predictions_val = ((predicted_emotions_val == emotion_indices) * pad_mask).sum().item()\n",
    "            total_val_correct += correct_predictions_val\n",
    "            total_val_predictions += torch.sum(pad_mask).item()\n",
    "\n",
    "            # Store true and predicted labels for F1 score calculation\n",
    "            true_labels.extend(emotion_indices.cpu().numpy())\n",
    "            predicted_labels.extend(predicted_emotions_val.cpu().numpy())\n",
    "            padded_labels.extend(pad_mask.cpu().numpy())\n",
    "\n",
    "    final_true_labels = [label for label, pad in zip(true_labels, padded_labels) if pad == 1]\n",
    "    final_predicted_labels = [label for label, pad in zip(predicted_labels, padded_labels) if pad == 1]\n",
    "    classification_rep = classification_report(final_true_labels, final_predicted_labels)\n",
    "\n",
    "    # Calculate and print the average loss for this epoch\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    avg_val_loss = total_val_loss / total_val_tokens\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Training Loss: {avg_loss}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Validation Loss: {avg_val_loss}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Classification Report:\\n{classification_rep}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Accuracy: {total_correct / total_predictions:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        best_classification_report = classification_rep\n",
    "        best_model_file = f\"/tmp/semeval24_task3/final_models/cause_models/best_cause_model.pt\"\n",
    "        torch.save(model.state_dict(), best_model_file)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"/tmp/semeval24_task3/final_models/cause_models/cause_model_{epoch:02}.pt\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(\"=======================================\")\n",
    "print(\"BEST MODEL\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Best validation loss: {best_val_loss}\")\n",
    "print(f\"Best classification report:\\n{best_classification_report}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
