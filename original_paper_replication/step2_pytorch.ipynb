{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba951e4-a7d5-4f21-8fb8-7918de7bd983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/suyash.mathur/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2a0059-825e-4bc6-affd-6b1eb34e4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# import random\n",
    "# import torch\n",
    "\n",
    "# numpy.random.seed(69)\n",
    "# random.seed(69)\n",
    "# torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2806718a-4a8a-473c-9ea8-99d1d03768b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "BASE_DIR = \"/tmp/akshett.jindal\"\n",
    "HUGGINGFACE_CACHE_DIR = os.path.join(BASE_DIR, \".huggingface_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b524940c-cccb-46c7-b285-a5210aa69bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "DATA_DIR = \"/tmp/semeval24_task3\"\n",
    "\n",
    "# TRAIN_DATA_FILEPATH = os.path.join(DATA_DIR, \"final_clean_data\", \"train\", \"Subtask_2.json\")\n",
    "# VAL_DATA_FILEPATH = os.path.join(DATA_DIR, \"final_clean_data\", \"val\", \"Subtask_2.json\")\n",
    "\n",
    "TRAIN_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/training.json\"\n",
    "VAL_DATA_FILEPATH = \"/tmp/semeval24_task3/SemEval-2024_Task3/official_data/Training_data/text/testing.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13407c11-6aa6-4157-b8fd-d03bbfec8410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "VID_ID_MAPPING = np.load(\"/home2/suyash.mathur/semeval24/task3/MECPE/data/video_id_mapping.npy\", allow_pickle=True).item()\n",
    "\n",
    "class YourAudioEncoder():\n",
    "    def __init__(self, audio_embeddings_path):\n",
    "        self.audio_embeddings = np.load(audio_embeddings_path)\n",
    "        # with open(audio_embeddings_path, \"rb\") as f:\n",
    "            # self.audio_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, audio_name):\n",
    "        audio_name = audio_name.split(\".\")[0]\n",
    "        audio_name = VID_ID_MAPPING[audio_name]\n",
    "        audio_embedding = self.audio_embeddings[audio_name]\n",
    "        return torch.from_numpy(audio_embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b04ac2-3917-4c1d-a371-750cb3836a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class YourVideoEncoder():\n",
    "    def __init__(self, video_embeddings_path):\n",
    "        self.video_embeddings = np.load(video_embeddings_path)\n",
    "        # with open(video_embeddings_path, \"rb\") as f:\n",
    "        #     self.video_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        video_name = video_name.split(\".\")[0]\n",
    "        video_name = VID_ID_MAPPING[video_name]\n",
    "        video_embedding = self.video_embeddings[video_name]\n",
    "        # video_embedding = video_embedding.reshape((16,-1))\n",
    "        # video_embedding = np.mean(video_embedding, axis=0)\n",
    "        return torch.from_numpy(video_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86071165-de8e-4a02-91c2-66165d6d65c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "class YourTextEncoder():\n",
    "    def __init__(self, text_embeddings_path):\n",
    "        with open(text_embeddings_path, \"rb\") as f:\n",
    "            self.text_embeddings = pickle.load(f)\n",
    "\n",
    "    def lmao(self, video_name):\n",
    "        text_embedding = self.text_embeddings[video_name]\n",
    "        return torch.from_numpy(text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544e6927-b52c-4edf-b7e4-bd9285d35a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmotionCauseDataset(Dataset):\n",
    "    def __init__(self, file_path, audio_encoder, video_encoder, text_encoder, neg_to_pos_ratio):\n",
    "        with open(file_path) as f:\n",
    "            self.file_data = json.load(f)\n",
    "\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.video_encoder = video_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.data = []\n",
    "        self.POSITIVE_SAMPLE_COUNT = 0\n",
    "        self.NEGATIVE_SAMPLE_COUNT = 0\n",
    "\n",
    "        for conversation in self.file_data:\n",
    "            positive_samples = []\n",
    "            negative_samples = []\n",
    "\n",
    "            utterances = {\n",
    "                utterance[\"utterance_ID\"]: utterance\n",
    "                for utterance in conversation[\"conversation\"]\n",
    "            }\n",
    "            utterance_ids = set(utterances.keys())\n",
    "\n",
    "            causes = {\n",
    "                utterance_id: []\n",
    "                for utterance_id in utterance_ids\n",
    "            }\n",
    "\n",
    "            for emo_cause_pairs in conversation[\"emotion-cause_pairs\"]:\n",
    "                emotion_utterance_num, emotion = emo_cause_pairs[0].split(\"_\")\n",
    "                emotion_utterance_num = int(emotion_utterance_num)\n",
    "                cause_utterance_num = int(emo_cause_pairs[1])\n",
    "\n",
    "                if emotion != \"neutral\":\n",
    "                    causes[emotion_utterance_num].append(cause_utterance_num)\n",
    "\n",
    "            for emo_utterance_id in utterance_ids:\n",
    "                emo_utterance = utterances[emo_utterance_id]\n",
    "\n",
    "                if utterances[emo_utterance_id][\"emotion\"] == \"neutral\":\n",
    "                    continue\n",
    "\n",
    "                for cause_utterance_id in utterance_ids:\n",
    "                    cause_utterance = utterances[cause_utterance_id]\n",
    "\n",
    "                    is_cause = cause_utterance_id in causes[emo_utterance_id]\n",
    "\n",
    "                    data_point = {\n",
    "                        \"original_utterance\": {\n",
    "                            \"id\": emo_utterance_id,\n",
    "                            \"text\": emo_utterance[\"text\"],\n",
    "                            \"video_name\": emo_utterance[\"video_name\"],\n",
    "                        },\n",
    "                        \"cause_utterance\": {\n",
    "                            \"id\": cause_utterance_id,\n",
    "                            \"text\": cause_utterance[\"text\"],\n",
    "                            \"video_name\": cause_utterance[\"video_name\"],\n",
    "                        },\n",
    "                        \"is_cause\": is_cause,\n",
    "                    }\n",
    "\n",
    "                    if is_cause:\n",
    "                        positive_samples.append(data_point)\n",
    "                        self.POSITIVE_SAMPLE_COUNT += 1\n",
    "                    else:\n",
    "                        negative_samples.append(data_point)\n",
    "                        self.NEGATIVE_SAMPLE_COUNT += 1\n",
    "\n",
    "            random.shuffle(negative_samples)\n",
    "\n",
    "            self.data.extend(positive_samples)\n",
    "            if neg_to_pos_ratio is not None:\n",
    "                self.data.extend(negative_samples[:min(neg_to_pos_ratio * len(positive_samples), len(negative_samples))])\n",
    "            else:\n",
    "                self.data.extend(negative_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "\n",
    "        orig_utt = data[\"original_utterance\"]\n",
    "        cause_utt = data[\"cause_utterance\"]\n",
    "\n",
    "        orig_id = orig_utt[\"id\"]\n",
    "        orig_text = orig_utt[\"text\"]\n",
    "        orig_video = orig_utt[\"video_name\"]\n",
    "        orig_audio = orig_utt[\"video_name\"].replace(\".mp4\", \".wav\")\n",
    "\n",
    "        cause_id = cause_utt[\"id\"]\n",
    "        cause_text = cause_utt[\"text\"]\n",
    "        cause_video = cause_utt[\"video_name\"]\n",
    "        cause_audio = cause_utt[\"video_name\"].replace(\".mp4\", \".wav\")\n",
    "\n",
    "        is_cause = data[\"is_cause\"]\n",
    "\n",
    "        return {\n",
    "            \"distance\": abs(orig_id - cause_id),\n",
    "            \"original_audio\": self.audio_encoder.lmao(orig_audio).float(),\n",
    "            \"original_video\": self.video_encoder.lmao(orig_video).float(),\n",
    "            \"original_text\": self.text_encoder.lmao(orig_video).squeeze().float(),\n",
    "            \"cause_audio\": self.audio_encoder.lmao(cause_audio).float(),\n",
    "            \"cause_video\": self.video_encoder.lmao(cause_video).float(),\n",
    "            \"cause_text\": self.text_encoder.lmao(cause_video).squeeze().float(),\n",
    "            \"is_cause\": 1.0 if is_cause else 0.0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246a8dd9-1de3-4c4b-b1b2-3089cfd6a653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49835, 11047)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "AUDIO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/audio_embedding_6373.npy\"\n",
    "VIDEO_EMBEDDINGS_FILEPATH = \"/tmp/semeval24_task3/og_paper_embeddings/video_embedding_4096.npy\"\n",
    "TEXT_EMBEDDINGS_FILEPATH = os.path.join(DATA_DIR, \"text_embeddings\", \"text_embeddings_bert_base.pkl\")\n",
    "\n",
    "audio_encoder = YourAudioEncoder(AUDIO_EMBEDDINGS_FILEPATH)\n",
    "video_encoder = YourVideoEncoder(VIDEO_EMBEDDINGS_FILEPATH)\n",
    "text_encoder = YourTextEncoder(TEXT_EMBEDDINGS_FILEPATH)\n",
    "\n",
    "trn_dataset = EmotionCauseDataset(\n",
    "    TRAIN_DATA_FILEPATH,\n",
    "    audio_encoder,\n",
    "    video_encoder,\n",
    "    text_encoder,\n",
    "    neg_to_pos_ratio=5\n",
    ")\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = EmotionCauseDataset(\n",
    "    VAL_DATA_FILEPATH,\n",
    "    audio_encoder,\n",
    "    video_encoder,\n",
    "    text_encoder,\n",
    "    neg_to_pos_ratio=None,\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "len(trn_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae58a04-a8b4-444c-a76d-e7a3ac6fe11e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_positional_embeddings(dimension, count):\n",
    "    embeddings = [list(np.zeros(dimension))]\n",
    "    embeddings.extend([\n",
    "        list(np.random.normal(loc=0.0, scale=0.1, size=dimension)) for _ in range(count)\n",
    "    ])\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5243cb81-825d-4146-8db0-46ed822dd34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionCauseDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        utterance_embedding_size,\n",
    "        device,\n",
    "        hidden_dimension=4096,\n",
    "        positional_embeddings_dimension=200,\n",
    "        dropout=0.2,\n",
    "        *args, **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "        positional_embeddings = generate_positional_embeddings(positional_embeddings_dimension, 200)\n",
    "        self.positional_embeddings = torch.from_numpy(positional_embeddings).to(device).float()\n",
    "        \n",
    "        self.non_neutral_dropout = nn.Dropout(dropout)\n",
    "        self.candidate_cause_dropout = nn.Dropout(dropout)\n",
    "        self.distance_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(utterance_embedding_size*2 + positional_embeddings_dimension, hidden_dimension)\n",
    "        self.linear1_activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dimension, 1)\n",
    "\n",
    "    def forward(self, non_neutral_utterances, candidate_cause_utterances, distances):\n",
    "        positional_embedding = self.positional_embeddings[distances]\n",
    "        \n",
    "        non_neutral_utterances = self.non_neutral_dropout(non_neutral_utterances)\n",
    "        candidate_cause_utterances = self.candidate_cause_dropout(candidate_cause_utterances)\n",
    "        positional_embedding = self.distance_dropout(positional_embedding)\n",
    "\n",
    "        embeddings = torch.concat((non_neutral_utterances, candidate_cause_utterances, positional_embedding), axis=1)\n",
    "\n",
    "        return self.linear2(\n",
    "            self.linear1_activation(\n",
    "                self.linear1(embeddings)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f41eb7cc-c5c2-4709-bb9f-3e0ba28a8616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "def save_model(epoch_num):\n",
    "    torch.save(model.state_dict(), f\"/tmp/semeval24_task3/final_models/pairing_models/paring_model_{epoch_num:02}.pt\")\n",
    "    numpy.save(\n",
    "        f\"/tmp/semeval24_task3/final_models/pairing_models/pairing_model_pos_embeds_{epoch_num:02}.npy\",\n",
    "        model.positional_embeddings.cpu().numpy(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63eaef23-10f5-4068-b853-7f2d9a393201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.98      0.94     10056\n",
      "         1.0       0.13      0.03      0.05       991\n",
      "\n",
      "    accuracy                           0.89     11047\n",
      "   macro avg       0.52      0.51      0.50     11047\n",
      "weighted avg       0.84      0.89      0.86     11047\n",
      "\n",
      "Epoch [01/20] Train Loss: 16042088.849810366\n",
      "Epoch [01/20] Validation Loss: 8971154.376946125\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|▌         | 1/20 [00:16<05:14, 16.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [02/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.55      0.70     10056\n",
      "         1.0       0.14      0.75      0.24       991\n",
      "\n",
      "    accuracy                           0.57     11047\n",
      "   macro avg       0.55      0.65      0.47     11047\n",
      "weighted avg       0.88      0.57      0.66     11047\n",
      "\n",
      "Epoch [02/20] Train Loss: 11341637.559529731\n",
      "Epoch [02/20] Validation Loss: 3343965.1583822863\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 2/20 [00:34<05:08, 17.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [03/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.02      0.03     10056\n",
      "         1.0       0.09      0.99      0.17       991\n",
      "\n",
      "    accuracy                           0.10     11047\n",
      "   macro avg       0.51      0.50      0.10     11047\n",
      "weighted avg       0.86      0.10      0.05     11047\n",
      "\n",
      "Epoch [03/20] Train Loss: 8925704.810745174\n",
      "Epoch [03/20] Validation Loss: 9843022.17737845\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  15%|█▌        | 3/20 [00:51<04:50, 17.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [04/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.80      0.85     10056\n",
      "         1.0       0.07      0.14      0.09       991\n",
      "\n",
      "    accuracy                           0.74     11047\n",
      "   macro avg       0.48      0.47      0.47     11047\n",
      "weighted avg       0.83      0.74      0.78     11047\n",
      "\n",
      "Epoch [04/20] Train Loss: 8581250.568777325\n",
      "Epoch [04/20] Validation Loss: 5959823.025720282\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 4/20 [01:07<04:30, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [05/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.16      0.27     10056\n",
      "         1.0       0.10      0.91      0.17       991\n",
      "\n",
      "    accuracy                           0.23     11047\n",
      "   macro avg       0.52      0.53      0.22     11047\n",
      "weighted avg       0.87      0.23      0.26     11047\n",
      "\n",
      "Epoch [05/20] Train Loss: 8021735.268583822\n",
      "Epoch [05/20] Validation Loss: 10445583.392639909\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 5/20 [01:24<04:13, 16.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [06/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.83      0.89     10056\n",
      "         1.0       0.25      0.59      0.35       991\n",
      "\n",
      "    accuracy                           0.81     11047\n",
      "   macro avg       0.60      0.71      0.62     11047\n",
      "weighted avg       0.89      0.81      0.84     11047\n",
      "\n",
      "Epoch [06/20] Train Loss: 9899819.614297159\n",
      "Epoch [06/20] Validation Loss: 5080159.909369062\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 6/20 [01:41<03:55, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [07/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.42      0.58     10056\n",
      "         1.0       0.12      0.83      0.21       991\n",
      "\n",
      "    accuracy                           0.45     11047\n",
      "   macro avg       0.54      0.62      0.40     11047\n",
      "weighted avg       0.89      0.45      0.55     11047\n",
      "\n",
      "Epoch [07/20] Train Loss: 8407077.200749332\n",
      "Epoch [07/20] Validation Loss: 3268423.5122498414\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  35%|███▌      | 7/20 [01:58<03:41, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [08/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.15      0.26     10056\n",
      "         1.0       0.10      0.95      0.18       991\n",
      "\n",
      "    accuracy                           0.22     11047\n",
      "   macro avg       0.53      0.55      0.22     11047\n",
      "weighted avg       0.89      0.22      0.25     11047\n",
      "\n",
      "Epoch [08/20] Train Loss: 12463775.026729602\n",
      "Epoch [08/20] Validation Loss: 5108105.152766351\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 8/20 [02:15<03:24, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [09/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.10      0.18     10056\n",
      "         1.0       0.09      0.96      0.17       991\n",
      "\n",
      "    accuracy                           0.17     11047\n",
      "   macro avg       0.53      0.53      0.17     11047\n",
      "weighted avg       0.88      0.17      0.18     11047\n",
      "\n",
      "Epoch [09/20] Train Loss: 8420524.69103026\n",
      "Epoch [09/20] Validation Loss: 5412287.8599892305\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  45%|████▌     | 9/20 [02:32<03:05, 16.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.92      0.94     10056\n",
      "         1.0       0.40      0.53      0.46       991\n",
      "\n",
      "    accuracy                           0.89     11047\n",
      "   macro avg       0.68      0.73      0.70     11047\n",
      "weighted avg       0.90      0.89      0.89     11047\n",
      "\n",
      "Epoch [10/20] Train Loss: 9697328.21714467\n",
      "Epoch [10/20] Validation Loss: 4393678.182388423\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 10/20 [02:49<02:48, 16.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.56      0.70     10056\n",
      "         1.0       0.13      0.66      0.22       991\n",
      "\n",
      "    accuracy                           0.57     11047\n",
      "   macro avg       0.54      0.61      0.46     11047\n",
      "weighted avg       0.87      0.57      0.66     11047\n",
      "\n",
      "Epoch [11/20] Train Loss: 10418204.687576372\n",
      "Epoch [11/20] Validation Loss: 6730420.380647115\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  55%|█████▌    | 11/20 [03:05<02:31, 16.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.61      0.74     10056\n",
      "         1.0       0.14      0.65      0.23       991\n",
      "\n",
      "    accuracy                           0.62     11047\n",
      "   macro avg       0.54      0.63      0.49     11047\n",
      "weighted avg       0.87      0.62      0.70     11047\n",
      "\n",
      "Epoch [12/20] Train Loss: 10370160.058729129\n",
      "Epoch [12/20] Validation Loss: 3371914.908136915\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 12/20 [03:22<02:13, 16.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.08      0.15     10056\n",
      "         1.0       0.09      0.98      0.17       991\n",
      "\n",
      "    accuracy                           0.16     11047\n",
      "   macro avg       0.53      0.53      0.16     11047\n",
      "weighted avg       0.89      0.16      0.15     11047\n",
      "\n",
      "Epoch [13/20] Train Loss: 8579385.486864695\n",
      "Epoch [13/20] Validation Loss: 8933342.988853788\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  65%|██████▌   | 13/20 [03:38<01:56, 16.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.06      0.12     10056\n",
      "         1.0       0.09      0.96      0.17       991\n",
      "\n",
      "    accuracy                           0.14     11047\n",
      "   macro avg       0.52      0.51      0.14     11047\n",
      "weighted avg       0.87      0.14      0.12     11047\n",
      "\n",
      "Epoch [14/20] Train Loss: 10527625.077013155\n",
      "Epoch [14/20] Validation Loss: 8349732.074300371\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 14/20 [03:55<01:39, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.62      0.75     10056\n",
      "         1.0       0.16      0.71      0.25       991\n",
      "\n",
      "    accuracy                           0.63     11047\n",
      "   macro avg       0.56      0.66      0.50     11047\n",
      "weighted avg       0.88      0.63      0.71     11047\n",
      "\n",
      "Epoch [15/20] Train Loss: 8977132.908064973\n",
      "Epoch [15/20] Validation Loss: 5951977.333735759\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 15/20 [04:12<01:23, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.73      0.80     10056\n",
      "         1.0       0.05      0.13      0.07       991\n",
      "\n",
      "    accuracy                           0.68     11047\n",
      "   macro avg       0.47      0.43      0.44     11047\n",
      "weighted avg       0.82      0.68      0.74     11047\n",
      "\n",
      "Epoch [16/20] Train Loss: 7383186.311136257\n",
      "Epoch [16/20] Validation Loss: 5044503.875582063\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 16/20 [04:29<01:06, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.96      0.95     10056\n",
      "         1.0       0.49      0.40      0.44       991\n",
      "\n",
      "    accuracy                           0.91     11047\n",
      "   macro avg       0.71      0.68      0.70     11047\n",
      "weighted avg       0.90      0.91      0.90     11047\n",
      "\n",
      "Epoch [17/20] Train Loss: 8313866.437490529\n",
      "Epoch [17/20] Validation Loss: 7907599.801409071\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  85%|████████▌ | 17/20 [04:45<00:50, 16.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.48      0.63     10056\n",
      "         1.0       0.12      0.74      0.21       991\n",
      "\n",
      "    accuracy                           0.50     11047\n",
      "   macro avg       0.54      0.61      0.42     11047\n",
      "weighted avg       0.87      0.50      0.60     11047\n",
      "\n",
      "Epoch [18/20] Train Loss: 8428199.515980573\n",
      "Epoch [18/20] Validation Loss: 42952887.30262809\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 18/20 [05:02<00:33, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.02      0.03     10056\n",
      "         1.0       0.09      1.00      0.17       991\n",
      "\n",
      "    accuracy                           0.10     11047\n",
      "   macro avg       0.54      0.51      0.10     11047\n",
      "weighted avg       0.90      0.10      0.04     11047\n",
      "\n",
      "Epoch [19/20] Train Loss: 9416122.739075698\n",
      "Epoch [19/20] Validation Loss: 11224924.780358236\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  95%|█████████▌| 19/20 [05:19<00:16, 16.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.29      0.44     10056\n",
      "         1.0       0.11      0.88      0.19       991\n",
      "\n",
      "    accuracy                           0.34     11047\n",
      "   macro avg       0.53      0.58      0.32     11047\n",
      "weighted avg       0.88      0.34      0.42     11047\n",
      "\n",
      "Epoch [20/20] Train Loss: 8738941.533780562\n",
      "Epoch [20/20] Validation Loss: 4306338.11160406\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [05:36<00:00, 16.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================BEST MODEL===================\n",
      "Best Model Epoch: 6\n",
      "Best Model Validation Loss: 3268423.5122498414\n",
      "Best Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.42      0.58     10056\n",
      "         1.0       0.12      0.83      0.21       991\n",
      "\n",
      "    accuracy                           0.45     11047\n",
      "   macro avg       0.54      0.62      0.40     11047\n",
      "weighted avg       0.89      0.45      0.55     11047\n",
      "\n",
      "================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "NUM_EPOCHS = 20\n",
    "AUDIO_EMBEDDING_SIZE = 6373\n",
    "VIDEO_EMBEDDING_SIZE = 4096\n",
    "TEXT_EMBEDDING_SIZE = 768\n",
    "TOTAL_EMBEDDING_SIZE = (\n",
    "    AUDIO_EMBEDDING_SIZE + VIDEO_EMBEDDING_SIZE + TEXT_EMBEDDING_SIZE\n",
    ")\n",
    "# TOTAL_EMBEDDING_SIZE = 11237\n",
    "\n",
    "model = EmotionCauseDetector(\n",
    "    TOTAL_EMBEDDING_SIZE,\n",
    "    device,\n",
    "    hidden_dimension=2000,\n",
    ")\n",
    "_ = model.to(device)\n",
    "\n",
    "weight_ratio = trn_dataset.NEGATIVE_SAMPLE_COUNT / trn_dataset.POSITIVE_SAMPLE_COUNT\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weight_ratio).to(device))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "\n",
    "total_steps = len(trn_dataloader) * NUM_EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "best_model_file = None\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "best_classification_report = None\n",
    "\n",
    "\n",
    "epoch_iter = tqdm(range(NUM_EPOCHS), desc=\"Epoch\", position=0)\n",
    "for epoch in epoch_iter:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(trn_dataloader, desc=\"Train Data Batch\", position=1, leave=False):\n",
    "        distances = batch[\"distance\"].to(device)\n",
    "\n",
    "        orig_audios = batch[\"original_audio\"].to(device)\n",
    "        orig_videos = batch[\"original_video\"].to(device)\n",
    "        orig_texts = batch[\"original_text\"].to(device)\n",
    "\n",
    "        cause_audios = batch[\"cause_audio\"].to(device)\n",
    "        cause_videos = batch[\"cause_video\"].to(device)\n",
    "        cause_texts = batch[\"cause_text\"].to(device)\n",
    "\n",
    "        is_cause = batch[\"is_cause\"].to(device)\n",
    "\n",
    "        orig_embedding = torch.cat((orig_audios, orig_videos, orig_texts), axis=1).float()\n",
    "        cause_embedding = torch.cat((cause_audios, cause_videos, cause_texts), axis=1).float()\n",
    "\n",
    "        output_logits = model(orig_embedding, cause_embedding, distances).squeeze()\n",
    "\n",
    "        loss = criterion(output_logits, is_cause)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    total_val_predictions = 0\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(val_dataloader, desc=\"Val Data Batch\", position=1, leave=False):\n",
    "            distances = val_batch[\"distance\"].to(device)\n",
    "\n",
    "            orig_audios = val_batch[\"original_audio\"].to(device)\n",
    "            orig_videos = val_batch[\"original_video\"].to(device)\n",
    "            orig_texts = val_batch[\"original_text\"].to(device)\n",
    "\n",
    "            cause_audios = val_batch[\"cause_audio\"].to(device)\n",
    "            cause_videos = val_batch[\"cause_video\"].to(device)\n",
    "            cause_texts = val_batch[\"cause_text\"].to(device)\n",
    "\n",
    "            is_cause = val_batch[\"is_cause\"].to(device)\n",
    "\n",
    "            orig_embedding = torch.cat((orig_audios, orig_videos, orig_texts), axis=1).float()\n",
    "            cause_embedding = torch.cat((cause_audios, cause_videos, cause_texts), axis=1).float()\n",
    "\n",
    "            output_logits = model(orig_embedding, cause_embedding, distances).squeeze()\n",
    "\n",
    "            val_loss = criterion(output_logits, is_cause)\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            predicted_is_cause = (output_logits >= 0.5).float()\n",
    "\n",
    "            correct_predictions_val = (is_cause == predicted_is_cause).sum().item()\n",
    "\n",
    "            total_val_correct += correct_predictions_val\n",
    "            total_val_predictions += (predicted_is_cause == 1.0).sum().item()\n",
    "\n",
    "            true_labels.extend(is_cause.cpu().numpy())\n",
    "            predicted_labels.extend(predicted_is_cause.cpu().numpy())\n",
    "\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "    avg_loss = total_loss / len(trn_dataset)\n",
    "    avg_val_loss = total_val_loss / len(val_dataset)\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        best_classification_report = report\n",
    "        # best_model_file = save_model(epoch)\n",
    "        torch.save(model.state_dict(), f\"/tmp/semeval24_task3/final_models/pairing_models/paring_model_best_model.pt\")\n",
    "        numpy.save(\n",
    "            f\"/tmp/semeval24_task3/final_models/pairing_models/pairing_model_pos_embeds_best_model.npy\",\n",
    "            model.positional_embeddings.cpu().numpy(),\n",
    "        )\n",
    "\n",
    "    print(f\"Epoch [{epoch+1:02}/{NUM_EPOCHS}] Classification Report:\\n{report}\")\n",
    "    print(f\"Epoch [{epoch+1:02}/{NUM_EPOCHS}] Train Loss: {avg_loss}\")\n",
    "    print(f\"Epoch [{epoch+1:02}/{NUM_EPOCHS}] Validation Loss: {avg_val_loss}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    save_model(epoch)\n",
    "\n",
    "print(\"===================BEST MODEL===================\")\n",
    "print(f\"Best Model Epoch: {best_epoch}\")\n",
    "print(f\"Best Model Validation Loss: {best_val_loss}\")\n",
    "print(f\"Best Model Classification Report:\\n{best_classification_report}\")\n",
    "print(\"================================================\")\n",
    "\n",
    "# Current best, Epoch 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28226244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
